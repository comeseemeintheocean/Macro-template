---
title: "Exchange Rate Forecasting Using Bayesian VARs Model"
author: "Qingqing Pang"

execute:
  echo: false
  
bibliography: references.bib
---


> **Abstract.** This research report explores how Bayesian VARs model predict AUD/USD exchange rate.
> **Keywords.** Bayesian Vars, Exchange rate, Forecasting, Minnesota Prior, Laplace distribution

# Objective and Motivation

The objective of this research is to use the Bayesian Vector Autoregressions (VARs) method to forecast the exchange rate of the US dollar exchange rate against the Australian dollar.

The ability to accurately forecast the foreign exchange rate is crucial for Australia’s global trade and investment. Given the prominence of the US dollar as the world's primary reserve currency, the monetary policies of the US Federal Reserve have a worldwide effect on the world economy, its significant influence on the Australia currency market should be important for domestic investors and policy maker as it can directly impact the AUD/USD exchange rate. Besides, America as one of the major trading partners for Australia, can impact bilateral trade flows and eventually affect the value of the AUD. Apart from the external from foreign countries, the domestic economic indicators can also be one of the determinants of the AUD/USD exchange rate.

The research is aimed to address the question for example, how the AUD/USD exchange rate will be in 3 months or even longer 1 year?

# Data and Variables

To better forecast the change in the exchange rate, the 13 variables are selected as follows which contain both domestic and US economic indicators that affect the exchange rate in different ways.

Real GDP and interest rates have a significant effect on the exchange rate, a higher realGDP and interest rate in Australia may increase the demand for AUD, which will lead to an appreciation of AUD and a rise in the AUD/USD exchange rate. A higher CPI indicates a lower purchasing power relative to foreign currency which may lead to a depreciation of the domestic currency. The unemployment rate can in some way represent business activity and a country with a high unemployment rate will lower the attractiveness for foreign investors and weaken the domestic currency competitiveness in the currency market. The balance of trade, which is the difference between exports and imports, also can influence the demand for its currency, a trade surplus in AUD may increase the demand for AUD dollar. 

  -   $erate_{t}$: AUD/USD exchange rate

* AUS economic indicators
  +   $crate\_au_{t}$: The Cash Rate Target, Australia
  +   $rgdp\_au_{t}$: The Real Gross Domestic Product, Australia
  +   $cpi\_au_{t}$: The Consumer Price Index, Australia
  +   $unemr\_au_{t}$: The Unemployment rate, Australia
  +   $impor\_au_{t}$: The Imports of Goods and Services, Australia
  +   $expor\_au_{t}$: The Exports of Goods and Services, Australia
* US economic indicators
  +   $crate\_us_{t}$: The Federal Funds Effective Rate, United States
  +   $rgdp\_us_{t}$: The Real Gross Domestic Product, United States
  +   $cpi\_us_{t}$: The Consumer Price Index, United States
  +   $unemr\_us_{t}$: The Unemployment rate, United States
  +   $impor\_us_{t}$: The Imports of Goods and Services, United States
  +   $expor\_us_{t}$: The Exports of Goods and Services, United States
  
### Data Cleaning
For the data cleaning part, most data for AUS is from Reserve Bank of Australia (RBA) and Australian Bureau of Statistics (ABS), and data for the US is from FRED, the dataset spans from 1990 Q1 to 2023 Q4, comprising 136 observations. To better fit the model, the data has been transformed to 'quarter' to ensure that seasonality effects are removed and logged transformations have been applied to most data except exchange rate and cash rate to reducing outlier effects.

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false
#| results: hide

library(fredr)
library(readrba)
library(readabs)
library(xts)
library(tseries)
library(fUnitRoots) 
library(plot3D)
library(MASS)
library(HDInterval)
library(plotly)       
library(mgcv)
library(mvtnorm)
fredr_set_key("75b470c4883ecfd5a7b4185f30437bd0")
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)

```

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false

#Y variable
# AUD/USD exchange rate quarterly
ex_rate <- read_rba(series_id = "FXRUSD")
ex_rate$date <- as.Date(ex_rate$date)
erate <- xts(ex_rate$value, ex_rate$date)
erate <- apply.quarterly(erate,mean)
index(erate) <- seq(as.Date("1969-09-01"), by = "3 months", length.out = nrow(erate))


#gold price (2006-)
#gpricelink <- "https://query1.finance.yahoo.com/v7/finance/download/GC%3DF?period1=1262304000&period2=1703980800&interval=1mo&filter=history&frequency=1mo&includeAdjustedClose=true"
#gprice  <- read.csv(gpricelink)
#gprice$Date <- as.Date(gprice$Date)
#gprice <- xts(gprice$Adj.Close, gprice$Date)
#gprice <- apply.quarterly(gprice,mean)

# Australia real gdp seasonal adjusted quarterly
#rgdp_au <- read_abs(series_id = "A2304404C")  
rgdp_au <- read_rba(series_id = "GGDPCVGDP")
rgdp_au <- xts::xts(rgdp_au$value, rgdp_au$date)
index(rgdp_au)   <- seq(as.Date("1959-09-01"), by = "3 months", length.out = nrow(rgdp_au))

#cash rate/interest rate of AUS quartly
cashrate<- read_cashrate(type = c("target"))
crate_au<- xts(cashrate$value, cashrate$date)
crate_au<- apply.quarterly(crate_au,mean)
crate_au<- xts(crate_au, seq(as.Date("1990-03-01"), by = "quarter", length.out = length(crate_au)))

#CPI quartly
# cpi_au <- read_rba(series_id = "GCPIAG")
cpi_au <- read_abs(series_id = "A2325846C")  
cpi_au <- xts::xts(cpi_au$value, cpi_au$date)

#unemployment rate quartly
#unemprate <-read_rba(series_id = "GLFSURSA")
unemprate <- read_abs(series_id = "A84423050A") 
unemr_au<- xts(unemprate$value, unemprate$date)
unemr_au<- apply.quarterly(unemr_au,mean)

# International Trade in Goods and Services seasonal adjusted_quartly
exportaus <- read_abs(series_id = "A2718603V")   
expor_au<- xts(exportaus $value, exportaus $date)
expor_au<- abs(expor_au)
expor_au<- apply.quarterly(expor_au,mean)

importaus <- read_abs(series_id = "A2718577A")     
impor_au<- xts(importaus$value, importaus$date)
impor_au<- apply.quarterly(impor_au,mean)

# America data
# us gdp
#rgdpus <- fredr(series_id = "A939RX0Q048SBEA")
rgdpus     <- fredr(series_id = "GDPC1")
rgdp_us     <- to.quarterly(xts(rgdpus$value, rgdpus$date), OHLC = FALSE)
index(rgdp_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(rgdp_us))

#Federal Funds Effective Rate/interest rate quartly
usdratelink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DFF&scale=left&cosd=1954-07-01&coed=2024-03-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Daily%2C%207-Day&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-04-01&revision_date=2024-04-01&nd=1954-07-01"
crate_us <- read.csv(usdratelink)
crate_us$DATE <- as.Date(crate_us$DATE)
crate_us <- xts(crate_us$DFF, order.by = crate_us$DATE)
crate_us <- apply.quarterly(crate_us,mean)
crate_us<- xts(crate_us, seq(as.Date("1954-09-01"), by = "quarter", length.out = length(crate_us)))

# cpi quartly
cpiusd  <- fredr(series_id = "USACPIALLMINMEI")
cpi_us<- xts(cpiusd$value, cpiusd$date)
cpi_us<- apply.quarterly(cpi_us,mean)

# unemployment quartly
unemprate_usd = fredr(series_id = "UNRATE")
unemr_us <- xts(unemprate_usd$value, unemprate_usd$date)
unemr_us<- apply.quarterly(unemr_us,mean)

#export_usd——quartly
usdexportink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=EXPGS&scale=left&cosd=1947-01-01&coed=2023-10-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-03-30&revision_date=2024-03-30&nd=1947-01-01"
expor_us <- read.csv(usdexportink)
expor_us$DATE <- as.Date(expor_us$DATE)
expor_us <- xts::xts(expor_us$EXPGS, order.by = expor_us$DATE)
index(expor_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(expor_us))

#import_usd_quartly
usdimportlink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=IMPGS&scale=left&cosd=1947-01-01&coed=2023-10-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-03-30&revision_date=2024-03-30&nd=1947-01-01"
impor_us <- read.csv(usdimportlink)
impor_us$DATE <- as.Date(impor_us$DATE)
impor_us <- xts::xts(impor_us$IMPGS, order.by = impor_us$DATE)
index(impor_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(impor_us))

```

```{r}
#| echo: false
#| message: false
#| warning: false

# log transformation of data
variables <- c("cpi_au", "cpi_us", "rgdp_au", "rgdp_us", "impor_au", "impor_us", "expor_au", "expor_us")

for(var in variables) {
  assign(var, log(get(var)))
}

#gprice <- log(gprice)
```

```{r}
#| echo: false
#| message: false
#| warning: false
 
# All Variables
merged_data = na.omit(merge(erate, 
                            cpi_au, cpi_us, 
                            crate_au, crate_us, 
                            expor_au, expor_us,  
                            impor_au, impor_us, 
                            rgdp_au, rgdp_us,
                            unemr_au, unemr_us))

# Defining your column name vector:
variable_names <- c("exchange rate", "cpi_au", "cpi_us", 
                    "cashrate_au", "cashrate_us", "export_au", "export_us",
                    "import_au", "import_us", "realgdp_au", "realgdp_us",
                    "unemployemtrate_au", "unemployemtrate_us")


colnames(merged_data)   <- variable_names


```

#### Visualisation of data
Plot the variables to see the patterns of data. It shows from the plots that the exchange rate and cash rate for the US fluctuate over time, the cash rate and unemployment for AU show a downward trend and all other variables have a clear upward trend, with the exports、imports, and GDP for both countries have a clear drop during the COVID-19 period.

```{r all variables plot}
#| echo: false
#| message: false
#| warning: false

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:13) { 
  ts.plot(merged_data[, i], main = colnames(merged_data)[i], 
          ylab = "", xlab = "")
}

```

Since most variables show a non-stationary pattern. To determine whether a unit root is present in a time series dataset, the ADF test will be conducted below.

#### Augmented Dickey-Fuller test for log transformed variables except exchange rates and cash rates.

From the plot we can observe all ACF plots have a high degree of persistence over time, indicating there is significant autocorrelation in the time series data.

```{r}
#| echo: false
#| message: false
#| warning: false

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:13){
acf = acf(merged_data[,i], plot = FALSE)[1:20]
plot(acf, main = "")
title(main = paste(colnames(merged_data)[i]), line = 0.5)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
adf_test <- list()
for (i in 1:13) {
  adf_result = adf.test(merged_data[,i], k = 4)
  adf_test[[i]] <- adf_result
}
```

Below is the p-value of each variable and only the cash rate for AUS has a p-value less than 0.05 which indicates that $crateau_{t}$ is stationary.

```{r}
#| echo: false
#| message: false
#| warning: false
adf_table <- data.frame(p_value = numeric(length(adf_test)))

for (i in 1:length(adf_test)) {adf_table[i, "p_value"] = round(adf_test[[i]]$p.value,3)
}

rownames(adf_table) <- variable_names

colnames(adf_table)<- c("P-value")
print(adf_table)

```

Below is the ADF test result for all non-stationary data taking the first difference. All variables in the first differences are stationary as the null hypothesis of non-stationary can be rejected.

```{r}
#| echo: false
#| message: false
#| warning: false
#take the first difference
nonstationary_merged_data <- subset(merged_data, select = -c(cashrate_au))

dff_merged_data <- na.omit(nonstationary_merged_data - lag(nonstationary_merged_data))
```

```{r}
#| echo: false
#| message: false
#| warning: false
# ADF test
dff_adf_test <- list()
for (i in 1:12) {
  dff_adf_result = adf.test(dff_merged_data[,i], k = 4)
  dff_adf_test[[i]] <- dff_adf_result
}

# View the ADF test results
dff_adf_table <- data.frame(p_value = numeric(length(dff_adf_test)))

# Fill in the data frame with the test results
for (i in 1:length(dff_adf_test)) {
  dff_adf_table[i, "p_value"] = round(dff_adf_test[[i]]$p.value,3)

}
rownames(dff_adf_table) <- variable_names[-4]


colnames(dff_adf_table)<- c("P-value")
print(dff_adf_table)
```

It can be concluded that all variables are integrated at 1 at the 5% significance level of the ADF test, with the conclusion folding for $crate\_au_{t}$ only at 1% significance level. Appropriate prior distributions will be used to accommodate this fact for the VAR model.

# Model and Hypotheses
In this research, the VAR(p) model will be applied to forecast the AUD/USD exchange rate, below is the basic model that is used in this research.

#### The basic VAR(p) model


```{=tex}
\begin{aligned}
 y_{t}&=\mu_{0}+A_{1}y_{t-1}+\cdots+A_{p}y_{t-p}+\epsilon_{t} \\
\epsilon_{t}|Y_{t-1} &\sim iid\mathcal{N}(0_{13},\Sigma)
\end{aligned}
```


For time $t$ = 1,2,.....,$T$：

-   $y_t$ is a $N(13)\times 1$ vector of observations at time $t$
-   $\mu_0$ is a $N(13)\times 1$ vector of constant terms
-   $A_i$ is a $N(13)\times N(13)$ matrix of autoregressive slope parameters
-   $\epsilon_t$ is a $N(13)\times 1$ vector of error terms and follows a multivariate white noise process
-   $Y_{t-1}$ is the information set collecting observations on y up to time $t-1$
-   $\Sigma$ is a $N(13)\times N(13)$ covariance matrix of the error term


```{=tex}
\begin{aligned}
y_{t}=\begin{pmatrix}
erate_{t}\\
crate\_au_{t} \\
rgdp\_au_{t}\\
cpi\_au_{t} \\
unemr\_au_{t}\\
impor\_au_{t} \\
expor\_au_{t} \\
crate\_us_{t} \\
rgdp\_us_{t} \\
cpi\_us_{t} \\
unemr\_us_{t} \\
impor\_us_{t}\\
expor\_us_{t}\\

\end{pmatrix}

\end{aligned}
```


For further research, we may use the predictive density function like 3-year-ahead forecast and forecast with Bayesian VARS.

# Modelling Framework

## The basic model

```{=tex}
\begin{align}
Y &=XA+E\\
E|X&\sim \mathcal{MN}_{T\times N}(0, \Sigma, I_T)\\
Y|X,A,\Sigma&\sim \mathcal{MN}_{T\times N} (XA, \Sigma, I_T)
\end{align}
```


Where $Y$ is a $T\times 13$ Matrix, $X$ is a $T\times(1+p\times13)$, $A$ is a $(1+p\times13)\times13$ matrix that contains $\mu_{0}$ and vectors of the autoregressive slope parameters and $E$ is a $T\times13$ matrix contains vetors of error terms.

The kernel of the likelihood function:


```{=tex}
\begin{align}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}}exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\}
\end{align}
```


The basic model is based on **Natural-conjugate prior distribution**, where the $A$ follows a Matrix-variate Normal distribution and $\Sigma$ follows an Inverse Wishart distribution.

```{=tex}
\begin{align}
p(A,\Sigma) &= p(A|\Sigma)p(\Sigma) \\
A|\Sigma &\sim \mathcal{MN}_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\Sigma &\sim \mathcal{IW}_N(\underline{S},\underline{\nu})
\end{align}
```


The **Minnesota prior** is typically a good choice for specifying priors in BVAR model especially when the model involves many macroeconomic variables. It assumes the variables follow a random walk and it is suitable for unit root non-stationary variables such as in our case, most variables are integrated at 1 at the 5% significance level of the ADF test, where: 


```{=tex}
\begin{align}
\underline{A} &= \begin{bmatrix}0_{N \times 1} & I_{N} & 0_{N \times (p-1)N}\end{bmatrix}'\\

\underline{V} &= diag( \begin{bmatrix} \kappa_2 & \kappa_1(p^{-2}\otimes I_N') \end{bmatrix})

\end{align}
```

The prior mean $\underline{A}$ for the first lag of each variable (the identity matrix portion) is one, while all other coefficients including intercepts, are zeroes. For the column-specific prior covariance $\underline{V}$, two shrinkage hyper-parameters $\kappa_1$ and $\kappa_2$ represent the overall shrinkage level for slopes and constant terms respectively. 

For **posterior distribution**, the kernel of the posterior distribution takes the form of the product of the likelihood and the prior distributions.

```{=tex}
\begin{align*}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma|Y,X)p(A,\Sigma) \\
&= L(A,\Sigma|Y,X)p(A|\Sigma)p(\Sigma)
\end{align*}
```

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\} \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A}) \underline{V}^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\}
\end{align}
```


The kernel can be represent as the normal-inverse Wishart distribution and we can get the following **full conditional joint posterior distribution: **


```{=tex}
\begin{align}
p(A|Y,X,\Sigma) &= \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
p(\Sigma|Y,X) &= \mathcal{IW}_N(\bar{S},\bar{\nu}) \\
\\
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A} \\

\end{align}
```


## The extended model
The extended model will be built based on the the change in distribution of the error to **Laplace distribution** instead of the normally distributed errors assumption. The Laplace distribution is suitable for describing financial anomalies due to its sharp peaks and thick tails and the use of this distribution improves the robustness of the model to anomalies and is particularly suitable for financial time series. As our variables are most financial time series data, a Laplace distribution is more suitable to apply to our error term.

Following [Eltoft,Kim, and Lee 2006b](https://ieeexplore.ieee.org/document/1618702), for covariance with a general Kronecker structure, if each ${\lambda_t}$ has an independent exponential distribution with mean ${\alpha}$, then marginally ${U_t}$ has a multivariate Laplace distribution with mean vector 0 and covariance matrix ${\alpha\Sigma}$.


```{=tex}
\begin{align}
U_t &\sim \text{Laplace}(0, \alpha\Sigma) \\
U_t | \lambda_t &\sim \mathcal{MN}(0, \Sigma, \lambda_t I_T) \\
\lambda_t &\sim \text{Exponential}(\frac{1}{\alpha})
\end{align}
```


The kernel of the likelihood function:


```{=tex}
\begin{align}
L(A,\Sigma,\lambda_t|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\lambda_t I_T)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda_t I_T)^{-1} (Y-XA) ]\}
\end{align}
```


For posteriors distribution, $A$, $\Sigma$ and $\lambda_t$ can then be derived using the likelihood and the prior distributions as follows:


```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma,\lambda_t|Y,X)p(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\lambda_t I_T)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda_t I_T)^{-1} (Y-XA) ]\} \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\} \\
&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\lambda_t I_T)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2} tr[\Sigma^{-1}(Y'(\lambda_t I_T)^{-1}Y - 2A'X'(\lambda_t I_T)^{-1}Y + A'X'(\lambda_t I_T)^{-1}XA \\
&+ A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})]\}

\end{align}
```


The kernel can be rearranged in the form of the **Matrix-variate normal-inverse Wishart distribution**.


```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\sim MNIW(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
&\\
\bar{V} &= (X'(\lambda_t I_T)^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'(\lambda_t I_T)^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= Y'(\lambda_t I_T)^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S} - \bar{A}'\bar{V}^{-1}\bar{A}
\end{align}
```


The kernel of the fully conditional posterior distribution of $\lambda_t$ is then derived as follows:


```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda_t|Y,X)p(\lambda_t) \\
\\
&\propto \det(\lambda_t I_T)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda_t I_T)^{-1} (Y-XA) ]\} \\
&\times \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\

&= \lambda_t^{-\frac{TN}{2}} exp\{-\frac{1}{2}\frac{1}{\lambda_t} tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\}\\
&\times exp\{-\frac{1}{\alpha}\lambda_t \}\\

&= \lambda_t^{-\frac{TN}{2}+1-1} exp\{-\frac{1}{2}[\frac{[tr[\Sigma^{-1}(Y-XA)'(Y-XA)]}{\lambda_t} +\frac{2}{\alpha}\lambda_t]\} 
\end{align}
```


The above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:


```{=tex}
\begin{align}
\lambda_t|Y,A,\Sigma &\sim GIG(a,b,p) \\
\\
a &=\frac{2}{\alpha} \\
b &= tr[\Sigma^{-1}(Y-XA)'(Y-XA)] \\
p &= -\frac{TN}{2}+1
\end{align}
```


## Proof of model validity 
### Proof of basic model validity 
To test the model validity, we simulated 1000 observations from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2 to see how the autoregressive and the covariance matrices and the posterior mean of the constant term behave.

```{r}
#| echo: false
#| message: false
#| warning: false
set.seed(123)
n <- 1000  # Number of observations
mu <- 0    # Mean
sigma <- 1 # Standard deviation

# Simulate two independent random walks
simulation_data <- data.frame(RW1 = cumsum(rnorm(n, mu, sigma)),RW2 = cumsum(rnorm(n, mu, sigma)))

plot(simulation_data$RW1, type = 'l', ylim = range(simulation_data), col = 'red', ylab = 'Value', xlab = 'Time', main = 'Bivariate Random Walk')
lines(simulation_data$RW2,col = 'blue')
legend("topright",legend = c("RW1", "RW2"), col = c("red", "blue"), lty = 1, cex = 0.6)
```

```{r static data setup}
#| echo: false
#| message: false
#| warning: false
## Present data X, Y
y             = ts(merged_data[,1:ncol(merged_data )])
Y             = ts(y[5:nrow(y),], frequency=4)
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])
}
 
## Pre-setup 
N             = ncol(Y)
p             = frequency(Y)
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Minnesota prior 
kappa.1       = 0.02^2                                    
kappa.2       = 100                                   
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N) 
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+1
```

```{r function based on basic model}
#| echo: true
#| message: false
#| warning: false

## Posterior sample draw

    posterior.draws       = function (S, Y, X){
  
    # normal-inverse Wishard posterior parameters
    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))
    V.bar             = solve(V.bar.inv)
    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar            = nrow(Y) + nu.prior
    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
  
    # posterior draws 
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }
 
    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
# simulation data generating process
p=1
N=2

Y_simulation = (simulation_data[(p+1):nrow(simulation_data),c(1,2)]) #contains the obs of the two variables and moves first obs
X_simulation = matrix(1,nrow(Y_simulation),1) #initializes the X matrix with a column of ones(intercept) in the VAR model.                                      
# adds the lagged values of the two variables to the X matrix, in this case, it adds one lagged value for each of the two variables.
for (i in 1:p){
  X_simulation     = cbind(X_simulation, (simulation_data[(p+1):nrow(simulation_data)-i,c(1,2)]))
}

Y_simulation = as.matrix(Y_simulation)
X_simulation = as.matrix(X_simulation)

N           = ncol(Y_simulation)                          
p           = frequency(Y_simulation)
A.hat       = solve(t(X_simulation)%*%X_simulation)%*%t(X_simulation)%*%Y_simulation
Sigma.hat   = t(Y_simulation-X_simulation%*%A.hat)%*%(Y_simulation-X_simulation%*%A.hat)/nrow(Y_simulation)

# Minnesota prior
kappa.1             = 0.02^2                                    
kappa.2             = 100                                  
A.prior             = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior             = diag(diag(Sigma.hat))
nu.prior            = N+1

posterior.sample.draws = posterior.draws(S=1000, Y=Y_simulation, X=X_simulation) 
```

```{r}
Sigma_posterior_mean <- apply(posterior.sample.draws$Sigma.posterior, 1:2, mean)

print("Posterior mean of the covariance matrix Sigma:")
print(Sigma_posterior_mean)
```

```{r}
# Calculate posterior mean of autoregressive coefficients (including the constant term)
A_posterior_means <- apply(posterior.sample.draws$A.posterior, 1:2, mean)

# Print the posterior mean matrix for autoregressive coefficients A
print("Posterior mean of the autoregressive coefficient matrix A:")
print(A_posterior_means)
```

The diagonal entries of the covariance matrix are close to 1, which indicates that each variable has a strong autoregressive relationship with itself and similarly, the diagonal elements of the autoregressive coefficient matrix are close to one, suggesting that each variable is heavily influenced by its past value. Besides, the posterior means for the constant terms is close to 0, the above can indicate that the estimated parameter constant term and means are consistent with what we expect given a Minnesota prior.

### Proof of extended model
The Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:

1. Draw $\Sigma^{(s)}$ from the $IW(\bar{S},\bar{\nu})$ distribution.
2. Draw $A^{(s)}$ from the $MN(\bar{A},\Sigma^{(s)}, \bar{V})$ distribution.
3. Draw $\lambda_t^{(s)}$ from $GIG(a,b,p)$.

Repeat steps 1, step 2 and 3 for $S_1$+$S_2$times.

Discard the first draws that allowed the algorithm to converge to the stationary posterior distribution.

Output is $\left\{ {A^{(s)}, \Sigma^{(s)}}, \lambda_t^{(s)}\right\}^{S_1+S_2}_{s=S_1+1}$.


```{r}
#| echo: true
#| message: false
#| warning: false
# setup 
S1                = 5000                             # will be discard
S2                = 45000                            
total_S           = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
lambda.posterior  = matrix(NA, S1+S2, 1)

# set the initial value of lambda
lambda.posterior[1] = 5                               

# parameter alpha
lambda.priors = list(alpha = 1)
```

```{r}
posterior.draws.extended = function (total_S, Y, X){
for (s in 1:total_S){

    V.bar.inv              = t(X)%*%X + diag(1/diag(lambda.posterior[s]* V.prior)) 
    V.bar                  = solve(V.bar.inv)
    A.bar                  = V.bar%*%(t(X)%*%Y + diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior)
    nu.bar                 = nrow(Y) + nu.prior
    S.bar                  = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv              = solve(S.bar)
  
    Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior.draw   = apply(Sigma.posterior.IW,3,solve)
    Sigma.posterior[,,s]   = Sigma.posterior.draw
    A.posterior[,,s]       = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))
    L                      = t(chol(V.bar))
    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    
    a                             = 2 / lambda.priors$alpha
    deviation_matrix              = A.posterior[,,s] - A.prior
    weighted_deviation_squared    = t(deviation_matrix) %*% solve(V.prior) %*% deviation_matrix
    b                             = sum(diag(solve(Sigma.posterior[,,s] %*% weighted_deviation_squared)))
    p                             = - (T * N) / 2 + 1            
    
    if (s!=total_S){
      lambda.posterior[s+1] = GIGrvg::rgig(n=1, lambda = p, chi = b, psi = a)
    }
}
    output  = list (A.posterior.exten = A.posterior, 
                    Sigma.posterior.exten = Sigma.posterior, 
                    lambda.posterior.exten = lambda.posterior)
    return(output)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
# simulation data generating process
p=1
N=2

Y_simulation = (simulation_data[(p+1):nrow(simulation_data),c(1,2)]) 
X_simulation = matrix(1,nrow(Y_simulation),1)                                             
for (i in 1:p){
  X_simulation     = cbind(X_simulation, (simulation_data[(p+1):nrow(simulation_data)-i,c(1,2)]))
}

Y_simulation = as.matrix(Y_simulation)
X_simulation = as.matrix(X_simulation)


## Test on basic model
N           = ncol(Y_simulation)                          
p           = frequency(Y_simulation)
A.hat       = solve(t(X_simulation)%*%X_simulation)%*%t(X_simulation)%*%Y_simulation
Sigma.hat   = t(Y_simulation-X_simulation%*%A.hat)%*%(Y_simulation-X_simulation%*%A.hat)/nrow(Y_simulation)

# Prior distribution (with Minnesota prior)
kappa.1             = 0.02^2                                     
kappa.2             = 100                                   
A.prior             = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior             = diag(diag(Sigma.hat))
nu.prior            = N+1

# conduct simulation
posterior.extended = posterior.draws.extended(total_S = total_S, Y=Y_simulation, X=X_simulation)
```

```{r}
Sigma_posterior_mean <- apply(posterior.extended$Sigma.posterior, 1:2, mean)

print("Posterior mean of the covariance matrix Sigma:")
print(Sigma_posterior_mean)
```

```{r}
A_posterior_means <- apply(posterior.extended$A.posterior, 1:2, mean)

print("Posterior mean of the autoregressive coefficient matrix A:")
print(A_posterior_means)
```

Similarly, the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and the posterior mean of the constant term is close to zero, so we can conclude that the extended model is also valid.

# Forecasting
The forecast will focus on two models with the sample period from 1990 Q1 to 2023Q4, and forecast how the exchange rate changes for the future up to 2026Q4.
## Forecasting with basic model

```{r forecasting static data}
#| echo: false
#| message: false
#| warning: false
## Present data X, Y
y             = ts(merged_data[,1:ncol(merged_data)])
Y             = ts(y[5:nrow(y),], frequency=4)
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])
}
 
## Pre-setup 
N             = ncol(Y)
p             = frequency(Y)
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution specification - Minnesota prior 
kappa.1       = 0.02^2                                   
kappa.2       = 100                                   
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2,1]  = 1
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+1

h                      = 12
S                      = 50000
Y.h                    = array(NA,c(h,N,S))

## Applying function 
posterior.sample.draws = posterior.draws(S=50000, Y=Y, X=X)
A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior
```

```{r}
#| echo: true
#| message: false
#| warning: false
# sampling predictive density
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,,s]
    x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti               = x.Ti[p:1,]
  for (i in 1:h){
    x.T               = c(1,as.vector(t(x.Ti)))
    Y.f               = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
      x.Ti            = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h[i,,s]         = Y.f[1:N]
  }
}
```

```{r}
#| echo: true
#| message: false
#| warning: false
################################### may not use
# plots of forecasts
ex_rate.point.f    = apply(Y.h[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)
ex_rate.range      = range(y[,1],ex_rate.interval.f)


par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1)
plot(1:(length(y[,1])+h),c(y[,1],ex_rate.point.f), type="l", ylim=ex_rate.range, axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
axis(1,c(1,21,41,61,81,101, nrow(y),nrow(y)+h),c("1992","1997","2002","2007","2012","2017","",""), col="black")
# Assuming ex_rate.range is already set correctly as per your code
axis(2, at=seq(from=ex_rate.range[1], to=ex_rate.range[2], length.out=5), 
     labels=round(seq(from=ex_rate.range[1], to=ex_rate.range[2], length.out=5), 2),
     col="black")

end_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value
abline(h=end_y_value, col="red", lty=2, lwd=1)


text(x=nrow(y), y=9.65, srt=90, "2025-12")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
abline(v=nrow(y)+h, col=mcxs2)


polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)
```

```{r}
ex_rate.point.f    = apply(Y.h[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)
ex_rate.range      = range(y[,1],ex_rate.interval.f)

true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value

# Define the range and labels for the y-axis
y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
y_max <- max(ex_rate.range[2], true_y_value)
num_labels <- 4  # You can adjust the number of labels

# Generate a sequence of y-values for the axis that includes true_y_value
y_values <- pretty(c(y_min, y_max), num_labels)
if(!true_y_value %in% y_values) {
  y_values <- sort(c(y_values))
}

# Generate labels for the y-axis, ensuring true_y_value is included and highlighted
y_labels <- sprintf("%.2f", y_values)
y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")

# Plot adjustments
plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
axis(2, at=y_values, labels=y_labels, col="black")

# Red dashed line for the true y-value
abline(h=true_y_value, col="red", lty=2, lwd=1)


text(x=nrow(y), y=9.65, srt=90, "2025-12")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
abline(v=nrow(y)+h, col=mcxs2)
legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.45)

polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)

```

```{r}
limits.1    = range(Y.h[,1,])
point.f     = apply(Y.h[,1,],1,mean)
interval.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)

x           = seq(from=limits.1[1], to=limits.1[2], length.out=100)
z           = matrix(NA,h,99)
for (i in 1:h){
  z[i,]     = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
}
x           = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
yy          = 1:h
z           = t(z)

theta = 180
phi   = 15.5
f4    = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1, col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
polygon3D(x=c(interval.f[1,],interval.f[2,h:1]), y=c(1:h,h:1), z=rep(0,2*h), col = mcxs1.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE)
for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=0.5, col="black")
}
f4.l1 = trans3d(x=point.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=2, col=mcxs1)
```


### Forecasting on extended model

```{r}
#| echo: true
#| message: false
#| warning: false
# setup 
S1                = 5000                             # will be discard
S2                = 45000                            
total_S           = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
lambda.posterior  = matrix(NA, S1+S2, 1)

# set the initial value of lambda
lambda.posterior[1] = 5                               

# parameter alpha
lambda.priors = list(alpha = 1)

posterior.draws.extended = function (total_S, Y, X){
for (s in 1:total_S){

    V.bar.inv              = t(X)%*%X + diag(1/diag(lambda.posterior[s]* V.prior)) 
    V.bar                  = solve(V.bar.inv)
    A.bar                  = V.bar%*%(t(X)%*%Y + diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior)
    nu.bar                 = nrow(Y) + nu.prior
    S.bar                  = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv              = solve(S.bar)
  
    Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior.draw   = apply(Sigma.posterior.IW,3,solve)
    Sigma.posterior[,,s]   = Sigma.posterior.draw
    A.posterior[,,s]       = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))
    L                      = t(chol(V.bar))
    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    
    a                             = 2 / lambda.priors$alpha
    deviation_matrix              = A.posterior[,,s] - A.prior
    weighted_deviation_squared    = t(deviation_matrix) %*% solve(V.prior) %*% deviation_matrix
    b                             = sum(diag(solve(Sigma.posterior[,,s] %*% weighted_deviation_squared)))
    p                             = - (T * N) / 2 + 1            
    
    if (s!=total_S){
      lambda.posterior[s+1] = GIGrvg::rgig(n=1, lambda = p, chi = b, psi = a)
    }
}
    output  = list (A.posterior.exten = A.posterior, 
                    Sigma.posterior.exten = Sigma.posterior, 
                    lambda.posterior.exten = lambda.posterior)
    return(output)
}

S1                         = 5000                              # determine the
S2                         = 45000                             # number of draws 
total_S                    = S1+S2
posterior.ext              = posterior.draws.extended(total_S = total_S, Y=Y, X=X)
A.posterior.ext.simu       = posterior.ext$A.posterior.exten[,,(S1+1):total_S]
Sigma.posterior.ext.simu   = posterior.ext$Sigma.posterior.exten[,,(S1+1):total_S]

## Three-year ahead forecasting h=12
# set up
h                          = 12
S                          = 45000
Y.h.ext                    = array(NA,c(h,N,S))
```

```{r}
#| echo: true
#| message: false
#| warning: false
# sampling predictive density
for (s in 1:S){
  A.posterior.draw         = A.posterior.ext.simu[,,s]
  Sigma.posterior.draw     = Sigma.posterior.ext.simu[,,s]
    x.Ti                   = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti                   = x.Ti[p:1,]
  for (i in 1:h){
    x.T                    = c(1,as.vector(t(x.Ti)))
    Y.f                    = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
      x.Ti                 = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h.ext[i,,s]          = Y.f[1:N]
  }
}
```

```{r}
##################### may not use
# plots of forecasts
ex_rate.point.f    = apply(Y.h.ext[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
ex_rate.range      = range(y[,1],ex_rate.interval.f)


par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1)
plot(1:(length(y[,1])+h),c(y[,1],ex_rate.point.f), type="l", ylim=ex_rate.range, axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
axis(1,c(1,21,41,61,81,101, nrow(y),nrow(y)+h),c("1992","1997","2002","2007","2012","2017","",""), col="black")
axis(2,c(ex_rate.range[1],mean(ex_rate.range),ex_rate.range[2]),c("","ex_rate",""), col="black")
abline(v=nrow(y), col=mcxs1)

text(x=nrow(y), y=9.65, srt=90, "2025-12")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
abline(v=nrow(y)+h, col=mcxs2)


polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)

```

```{r}
ex_rate.point.f    = apply(Y.h.ext[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
ex_rate.range      = range(y[,1],ex_rate.interval.f)

true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value

# Define the range and labels for the y-axis
y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
y_max <- max(ex_rate.range[2], true_y_value)
num_labels <- 4  # You can adjust the number of labels

# Generate a sequence of y-values for the axis that includes true_y_value
y_values <- pretty(c(y_min, y_max), num_labels)
if(!true_y_value %in% y_values) {
  y_values <- sort(c(y_values))
}

# Generate labels for the y-axis, ensuring true_y_value is included and highlighted
y_labels <- sprintf("%.2f", y_values)
y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")

# Plot adjustments
plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
axis(2, at=y_values, labels=y_labels, col="black")

# Red dashed line for the true y-value
abline(h=true_y_value, col="red", lty=2, lwd=1)

text(x=nrow(y), y=9.65, srt=90, "")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "")
abline(v=nrow(y)+h, col=mcxs2)
legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.45)

polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)

```

```{r}

limits.1    = range(Y.h.ext[,1,])
point.f     = apply(Y.h.ext[,1,],1,mean)
interval.f  = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)

x           = seq(from=limits.1[1], to=limits.1[2], length.out=100)
z           = matrix(NA,h,99)
for (i in 1:h){
  z[i,]     = hist(Y.h.ext[i,1,], breaks=x, plot=FALSE)$density
}
x           = hist(Y.h.ext[i,1,], breaks=x, plot=FALSE)$mids
yy          = 1:h
z           = t(z)

theta = 180
phi   = 15.5
f4    = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1, col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
polygon3D(x=c(interval.f[1,],interval.f[2,h:1]), y=c(1:h,h:1), z=rep(0,2*h), col = mcxs1.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE)
for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=0.5, col="black")
}
f4.l1 = trans3d(x=point.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=2, col=mcxs2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(1,2), mar=c(2,2,1,1))

# Forecasting on Extended model
limits.1        = range(Y.h[,1,])
point.f         = apply(Y.h[,1,],1,mean)
interval.f      = apply(Y.h[,1,],1,hdi,credMass=0.90)
theta = 180
phi   = 15.5

x.erate           = seq(from=limits.1[1], to=limits.1[2], length.out=10)
z.erate           = matrix(NA,h,9)
for (i in 1:h){
  z.erate[i,]     = hist(Y.h[i,1,], breaks=x.erate, plot=FALSE)$density
}
x.erate           = hist(Y.h[i,1,], breaks=x.erate, plot=FALSE)$mids
yy.erate          = 1:h

# plot using plot_ly
par(mfrow=c(1,1))
plot_ly(y = yy.erate, x = x.erate, z=z.erate) %>%
  
  layout(scene=list(xaxis=list(title="AUD/USD"),
                    yaxis=list(title="h-step forecast"),
                    zaxis=list(title="density")),
         title = "AUD/USD forecast densities") %>%
  add_surface(colors = c(mcxs1,mcxs4,mcxs5))


# Forecasting on Extended model
limits.1        = range(Y.h.ext[,1,])
point.f         = apply(Y.h.ext[,1,],1,mean)
interval.f      = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
theta = 180
phi   = 15.5

x.erate           = seq(from=limits.1[1], to=limits.1[2], length.out=10)
z.erate           = matrix(NA,h,9)
for (i in 1:h){
  z.erate[i,]     = hist(Y.h.ext[i,1,], breaks=x.erate, plot=FALSE)$density
}
x.erate           = hist(Y.h.ext[i,1,], breaks=x.erate, plot=FALSE)$mids
yy.erate          = 1:h

# plot using plot_ly
par(mfrow=c(1,1))
plot_ly(y = yy.erate, x = x.erate, z=z.erate) %>%
  
  layout(scene=list(xaxis=list(title="AUD/USD"),
                    yaxis=list(title="h-step forecast"),
                    zaxis=list(title="density")),
         title = "AUD/USD forecast densities") %>%
  add_surface(colors = c(mcxs1,"magenta2",mcxs5))

```

