[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "",
    "text": "Abstract. This research report explores how Bayesian VARs model predict AUD/USD exchange rate. Keywords. Bayesian Vars, Exchange rate, Forecasting, Minnesota Prior, Laplace distribution"
  },
  {
    "objectID": "index.html#the-basic-model",
    "href": "index.html#the-basic-model",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "The basic model",
    "text": "The basic model\n\\[\\begin{align}\nY &=XA+U\\\\\nU|X&\\sim \\mathcal{MN}_{T\\times N}(0, \\Sigma, I_T)\\\\\nY|X,A,\\Sigma&\\sim \\mathcal{MN}_{T\\times N} (XA, \\Sigma, I_T)\n\\end{align}\\]\nWhere \\(Y\\) is a \\(T\\times 13\\) Matrix, \\(X\\) is a \\(T\\times(1+p\\times13)\\), \\(A\\) is a \\((1+p\\times13)\\times13\\) matrix that contains \\(\\mu_{0}\\) and vectors of the autoregressive slope parameters and \\(U\\) is a \\(T\\times13\\) matrix contains vetors of error terms.\nThe kernel of the likelihood function:\n\\[\\begin{align}\nL(A,\\Sigma|Y,X) \\propto det(\\Sigma)^{-\\frac{T}{2}}exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]\\}\n\\end{align}\\]\nThe basic model is based on Natural-conjugate prior distribution, where the \\(A\\) follows a Matrix-variate Normal distribution and \\(\\Sigma\\) follows an Inverse Wishart distribution.\n\\[\\begin{align}\np(A,\\Sigma) &= p(A|\\Sigma)p(\\Sigma) \\\\\nA|\\Sigma &\\sim \\mathcal{MN}_{K \\times N}(\\underline{A},\\Sigma,\\underline{V}) \\\\\n\\Sigma &\\sim \\mathcal{IW}_N(\\underline{S},\\underline{\\nu})\n\\end{align}\\]\nThe Minnesota prior is typically a good choice for specifying priors in BVAR model especially when the model involves many macroeconomic variables. It assumes the variables follow a random walk and it is suitable for unit root non-stationary variables such as in our case, variables are integrated at 1 at the 5% significance level of the ADF test, where:\n\\[\\begin{align}\n\\underline{A} &= \\begin{bmatrix}0_{N \\times 1} & I_{N} & 0_{N \\times (p-1)N}\\end{bmatrix}'\\\\\n\n\\underline{V} &= diag( \\begin{bmatrix} \\kappa_2 & \\kappa_1(p^{-2}\\otimes I_N') \\end{bmatrix})\n\n\\end{align}\\]\nThe prior mean \\(\\underline{A}\\) for the first lag of each variable (the identity matrix portion) is one, while all other coefficients including intercepts, are zeroes. For the column-specific prior covariance \\(\\underline{V}\\), two shrinkage hyper-parameters \\(\\kappa_1\\) and \\(\\kappa_2\\) represent the overall shrinkage level for slopes and constant terms respectively.\nFor posterior distribution, the kernel of the posterior distribution takes the form of the product of the likelihood and the prior distributions.\n\\[\\begin{align*}\np(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma|Y,X)p(A,\\Sigma) \\\\\n&= L(A,\\Sigma|Y,X)p(A|\\Sigma)p(\\Sigma)\n\\end{align*}\\]\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+K+\\underline{v}+1}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A}) \\underline{V}^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\}\n\\end{align}\\]\nThe kernel can be represent as the normal-inverse Wishart distribution and we can get the following full conditional joint posterior distribution:\n\\[\\begin{align}\np(A|Y,X,\\Sigma) &= \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V}) \\\\\np(\\Sigma|Y,X) &= \\mathcal{IW}_N(\\bar{S},\\bar{\\nu}) \\\\\n\\\\\n\\bar{V} &= (X'X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu} \\\\\n\\bar{S} &= \\underline{S} + Y'Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A} \\\\\n\n\\end{align}\\]\n\n## Posterior sample draw\nposterior.draws       = function (S, Y, X, A.prior, V.prior, S.prior, nu.prior){\n  \n    # normal-inverse Wishard posterior parameters\n    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))\n    V.bar             = solve(V.bar.inv)\n    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar            = nrow(Y) + nu.prior\n    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv         = solve(S.bar)\n  \n    # posterior draws \n    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.posterior   = apply(Sigma.posterior,3,solve)\n    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))\n    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))\n    L                 = t(chol(V.bar))\n    for (s in 1:S){\n      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    }\n \n    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)\n    return(output)\n}\n\n\nEstimation Outcomes on Basic Model\n\nhead(round(apply(posterior.sample.draws$A.posterior, 1:2, mean),6))         # posterior draw A\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] -0.000065  4.393479  4.379507  4.322870  3.109329  9.393156  7.134650\n[2,]  0.999953  0.000242  0.000198  0.000952 -0.002938  0.000597  0.000801\n[3,] -0.000018  0.001903  0.001728 -0.012052 -0.009131  0.004610  0.003761\n[4,] -0.000009  0.001763  0.001617 -0.010546 -0.007539  0.004364  0.003498\n[5,] -0.000032 -0.012197 -0.011085  0.165779  0.089862 -0.030050 -0.023661\n[6,] -0.000232 -0.009516 -0.008072  0.097602  0.164954 -0.023272 -0.018085\n          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]\n[1,]  9.350763  7.400268 12.776791  9.592615  6.399594  5.077620\n[2,]  0.000637  0.000655  0.000270  0.000141 -0.001146  0.003952\n[3,]  0.005268  0.003939  0.002342  0.001804 -0.010968 -0.001805\n[4,]  0.004948  0.003739  0.002214  0.001725 -0.010982 -0.002467\n[5,] -0.034266 -0.024120 -0.014915 -0.011523  0.047572  0.026204\n[6,] -0.024277 -0.019804 -0.011412 -0.008103  0.045282 -0.062060\n\nhead(round(apply(posterior.sample.draws$Sigma.posterior, 1:2, mean),6))     # posterior draw sigma\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,]  0.001744 -0.000222 -0.000203  0.003388 -0.003713 -0.001053 -0.001388\n[2,] -0.000222  0.035843  0.032494 -0.221025 -0.173022  0.087174  0.070890\n[3,] -0.000203  0.032494  0.029826 -0.197727 -0.144431  0.080192  0.064603\n[4,]  0.003388 -0.221025 -0.197727  3.156893  1.824037 -0.535259 -0.425632\n[5,] -0.003713 -0.173022 -0.144431  1.824037  3.454357 -0.426593 -0.331317\n[6,] -0.001053  0.087174  0.080192 -0.535259 -0.426593  0.222599  0.177865\n          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]\n[1,] -0.002123 -0.000869 -0.000223 -0.000162 -0.001155  0.011375\n[2,]  0.099222  0.074344  0.044465  0.034232 -0.207662 -0.034843\n[3,]  0.090888  0.068666  0.040791  0.031652 -0.199186 -0.039388\n[4,] -0.623161 -0.423506 -0.268601 -0.205256  0.724093  0.333177\n[5,] -0.441620 -0.362079 -0.209438 -0.148299  0.825929 -1.284683\n[6,]  0.245494  0.192669  0.109971  0.085755 -0.589570 -0.104457\n\n\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\n\nplot.ts(posterior.sample.draws$A.posterior[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate\", col = mcxs1)\nhist(posterior.sample.draws$A.posterior[1,1,], xlab = \"AUD/USD exchange rate\", col = mcxs1, main = '')\n\nplot.ts(posterior.sample.draws$Sigma.posterior[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate sigma\", col = mcxs2)\nhist(posterior.sample.draws$Sigma.posterior[1,1,], xlab = \"AUD/USD exchange rate sigma\", col = mcxs2, main = '')\n\n\n\n\nThe trace plots and the histogram plot of posterior draws of\\(A\\) and \\(\\Sigma\\) is presented above with the trace plots shows significant volatility."
  },
  {
    "objectID": "index.html#the-extended-model",
    "href": "index.html#the-extended-model",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "The Extended Model",
    "text": "The Extended Model\nThe extended model will be built based on the the change in distribution of the error to Laplace distribution instead of the normally distributed errors assumption. The Laplace distribution is suitable for describing financial anomalies due to its sharp peaks and thick tails and the use of this distribution improves the robustness of the model to anomalies and is particularly suitable for financial time series. As our variables are most financial time series data, a Laplace distribution is more suitable to apply to our error term.\nFollowing Eltoft,Kim, and Lee 2006b, for covariance with a general Kronecker structure, if each \\({\\lambda_t}\\) has an independent exponential distribution with mean \\({\\alpha}\\), then marginally \\({U_t}\\) has a multivariate Laplace distribution with mean vector 0 and covariance matrix \\({\\alpha\\Sigma}\\).\n\\[\\begin{align}\n\nvec(U)&\\sim N(0,\\Sigma\\otimes \\Omega)\\\\\n\nU_t &\\sim \\text{Laplace}(0, \\alpha\\Sigma) \\\\\n\nU_t | \\lambda_t &\\sim \\mathcal{MN}(0, \\Sigma, \\Omega) \\\\\n\n\\lambda_t &\\sim \\text{Exponential}(\\frac{1}{\\alpha})\n\\end{align}\\]\nThe model is where \\(\\Omega=diag(\\lambda_1,\\lambda_2,...,\\lambda_t)\\), each lambda independently drawn from an exponential distribution.\nThe kernel of the likelihood function now is :\n\\[\\begin{align}\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1} \\lambda_t^{-\\frac{N}{2}} exp({-\\frac{1}{2}}\\sum^{T}_{t =1}{\\frac{1}{\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t)\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1}( \\lambda_t^{-\\frac{N}{2}} exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t))\n\\end{align}\\]\nThence, for each time t, we have the likelihood like:\n\\[\\begin{align}\n&=\\lambda_t^{-\\frac{N}{2}} exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t)\n\\end{align}\\]\nFor posteriors distribution, \\(A\\), \\(\\Sigma\\) and \\(\\lambda_t\\) can then be derived using the likelihood and the prior distributions as follows:\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma,\\lambda_t|Y,X)p(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\} \\\\\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\Omega)^{-\\frac{N}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2} tr[\\Sigma^{-1}(Y'\\Omega^{-1}Y - 2A'X'\\Omega)^{-1}Y + A'X'\\Omega^{-1}XA \\\\\n&+ A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})]\\}\n\n\\end{align}\\]\nThe kernel can be rearranged in the form of the Matrix-variate normal-inverse Wishart distribution.\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\sim MNIW(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n&\\\\\n\\bar{V} &= (X'\\Omega^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'\\Omega^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= Y'\\Omega^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\\end{align}\\]\nThe prior distribution of \\[\\lambda_t \\sim \\text{Exponential}(\\frac{1}{\\alpha})\\] define as:\n\\[\\begin{align}\np(\\lambda_t|\\alpha) &= \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n\n\\end{align} \\]\nThe kernel of the fully conditional posterior distribution of \\(\\lambda_t\\) is then derived as follows:\n\\[\\begin{align}\np(\\lambda_t|Y,X,A,\\Sigma) &\\propto L(A,\\Sigma,\\lambda_t|Y,X)p(\\lambda_t) \\\\\n\\\\\n&\\propto \\lambda_t^{-\\frac{N}{2}}exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}}\\epsilon_t' \\Sigma^{-1}\\epsilon_t) \\\\\n&\\times \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n\n\n&= \\lambda_t^{-\\frac{N}{2}+1-1} exp\\{-\\frac{1}{2}[\\frac{\n\\epsilon_t' \\Sigma^{-1}\\epsilon_t} {\\lambda_t} +\\frac{2}{\\alpha}\\lambda_t]\\}\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:\n\\[\\begin{align}\n\\lambda_t|Y,A,\\Sigma &\\sim GIG(a,b,p) \\\\\n\\\\\na &=\\frac{2}{\\alpha} \\\\\nb &= \\epsilon_t' \\Sigma^{-1}\\epsilon_t \\\\\np &= -\\frac{N}{2}+1\n\\end{align}\\]\nThe Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:\n\nDraw \\(\\Sigma^{(s)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution.\nDraw \\(A^{(s)}\\) from the \\(MN(\\bar{A},\\Sigma^{(s)}, \\bar{V})\\) distribution.\nDraw \\(\\lambda_t^{(s)}\\) from \\(GIG(a,b,p)\\).\n\nRepeat steps 1, step 2 and 3 for \\(S_1\\)+\\(S_2\\)times.\nDiscard the first draws that allowed the algorithm to converge to the stationary posterior distribution.\nOutput is \\(\\left\\{ {A^{(s)}, \\Sigma^{(s)}}, \\lambda_t^{(s)}\\right\\}^{S_1+S_2}_{s=S_1+1}\\).\n\nposterior.draws.extended &lt;- function(S1,total_S,Y, X, A.prior, V.prior, S.prior, nu.prior, lambda.priors){\n\n  Sigma.posterior.draws = array(NA, c(N,N,total_S))\n  A.posterior.draws = array(NA, c((1+p*N),N,total_S))\n  lambda.posterior.draws = array(NA,c(T,total_S+1))\n\n  for (s in 1:total_S){\n    \n    if (s == 1) {\n      lambda.s = lambda.0\n    } else {\n      lambda.s    = lambda.posterior.draws[,s]\n    }\n    \n    Omega = (diag(lambda.s))\n    Omega.inv = diag(1/lambda.s)\n\n    V.bar.inv.ext   = t(X)%*%Omega.inv%*%X + V.prior.inv\n    V.bar.inv.ext   = 0.5 * (t(V.bar.inv.ext) + V.bar.inv.ext)\n    V.bar.ext       = solve(V.bar.inv.ext)\n    V.bar.ext       = 0.5 * (t(V.bar.ext) + V.bar.ext)\n    A.bar.ext       = V.bar.ext%*%(t(X)%*%Omega.inv%*%Y + V.prior.inv%*%A.prior)\n    nu.bar.ext      = T + nu.prior\n    S.bar.ext       = S.prior + t(Y)%*%Omega.inv%*%Y + t(A.prior)%*%V.prior.inv%*%A.prior - t(A.bar.ext)%*%V.bar.inv.ext%*%A.bar.ext\n    S.bar.ext       = 0.5 * (t(S.bar.ext) + S.bar.ext)\n    S.bar.ext.inv   = solve(S.bar.ext)\n    S.bar.ext.inv   = 0.5 * (t(S.bar.ext.inv) + S.bar.ext.inv)\n    \n    Sigma.inv.draw = rWishart(1, df = nu.bar.ext, Sigma = S.bar.ext.inv)[,,1]\n    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)\n    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)\n    \n    \n    u.t = Y-X%*%A.posterior.draws[,,s]\n    #    ---- loop lambda posterior ----   #\n    c                      = -N/2 + 1          # N=13\n    a                      = 2 / lambda.priors$alpha\n    for (x in 1:T){\n      b                  = t((u.t)[x,])%*% Sigma.inv.draw %*%(u.t)[x,]\n      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)\n    } # END x loop\n  } # END s loop\n  \n  \n  output                 = (list(A.posterior.exten = A.posterior.draws[,,S1:total_S], \n                                 Sigma.posterior.exten = Sigma.posterior.draws[,,S1:total_S], \n                                 lambda.posterior.exten = lambda.posterior.draws[,(S1+1):(total_S+1)]))\n  \n}\n\n\nEstimation Outcomes on Extended Model\n\nhead(round(apply(posterior.ext$A.posterior.exten, 1:2, mean),6))\n\n          [,1]     [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,]  0.001184 0.005815  0.007066  0.006149  0.126937  0.016707  0.013198\n[2,]  0.999970 0.000000  0.000001 -0.000331  0.000130  0.000027  0.000038\n[3,] -0.000003 1.000003 -0.000001  0.000364 -0.000117 -0.000038 -0.000019\n[4,]  0.000002 0.000004  0.999997  0.000502  0.000014 -0.000043 -0.000013\n[5,]  0.000285 0.000037  0.000030  0.995288 -0.001983  0.000246  0.000306\n[6,] -0.000179 0.000018  0.000021  0.001652  0.994441  0.000157 -0.000044\n          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]\n[1,]  0.015776  0.018418  0.007535  0.007276 -0.120121 -0.040701\n[2,]  0.000072  0.000009 -0.000002 -0.000006  0.000324 -0.000233\n[3,] -0.000011 -0.000063 -0.000007 -0.000006  0.000102 -0.000132\n[4,] -0.000028 -0.000051 -0.000007 -0.000004  0.000089 -0.000064\n[5,]  0.000070  0.000344 -0.000012 -0.000009  0.003846  0.004803\n[6,] -0.000033  0.000047  0.000026  0.000058  0.000402  0.003178\n\nhead(round(apply(posterior.ext$Sigma.posterior.exten, 1:2, mean),6))\n\n          [,1]     [,2]     [,3]     [,4]      [,5]      [,6]     [,7]\n[1,]  0.001476 0.000012 0.000028 0.002524 -0.000960 -0.000398 0.000240\n[2,]  0.000012 0.000020 0.000011 0.000511  0.000178  0.000014 0.000035\n[3,]  0.000028 0.000011 0.000055 0.000388  0.001163  0.000050 0.000053\n[4,]  0.002524 0.000511 0.000388 0.129111  0.035261  0.000228 0.002727\n[5,] -0.000960 0.000178 0.001163 0.035261  0.367521  0.001752 0.001545\n[6,] -0.000398 0.000014 0.000050 0.000228  0.001752  0.001858 0.000260\n          [,8]     [,9]     [,10]     [,11]     [,12]     [,13]\n[1,] -0.001080 0.000184  0.000026  0.000031 -0.001382  0.000980\n[2,]  0.000051 0.000030 -0.000001 -0.000002 -0.000146 -0.000038\n[3,]  0.000052 0.000049  0.000001  0.000000 -0.000109 -0.000072\n[4,]  0.000154 0.003373  0.000247  0.000212 -0.021608 -0.018753\n[5,]  0.002409 0.002985 -0.000249  0.000168 -0.017547 -0.035447\n[6,]  0.001176 0.000320  0.000025  0.000027 -0.001182 -0.002020\n\nround(mean(posterior.ext$lambda.posterior.exten),6)\n\n[1] 1.589784\n\n\n\npar(mfrow=c(3,2), mar=c(2,2,2,2))\nplot.ts(posterior.ext$A.posterior.exten[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate.exten\", col = mcxs1)   \nhist(posterior.ext$A.posterior.exten[1,1,], xlab = \"AUD/USD exchange rate.exten\", col = mcxs1, main = '')\nplot.ts(posterior.ext$Sigma.posterior.exten[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate sigma.exten\", col = mcxs2) \nhist(posterior.ext$Sigma.posterior.exten[1,1,], xlab = \"AUD/USD exchange sigma.exten\", col = mcxs2, main = '')\n\nplot.ts(posterior.ext$lambda.posterior.exten[(S1+1):total_S], xlab = \"Simulation times S\", ylab = \"lambda\", col = mcxs3)         \nhist(posterior.ext$lambda.posterior.exten[(S1+1):total_S], xlab = \"lambda\", col = mcxs3, main = '')\n\n\n\n\nCompared to the basic model, by applying laplace distribution into the variance specification of the error term, we introduces more variability to our \\(A\\) and \\(\\Sigma\\). That is, the distribution of the coefficient now more dispersion with the basic model more concentrate 0. And for the distribution of Sigma, it shows the similar shape but central around different value.\nThe lambda mainly concentrates between 0 to 4, with some large outlines throughout the simulation."
  },
  {
    "objectID": "index.html#proof-of-model-validity",
    "href": "index.html#proof-of-model-validity",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "Proof of Model Validity",
    "text": "Proof of Model Validity\n\nProof of Basic Model Validity\nTo test the model validity, we simulated 1000 observations from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2 to see how the autoregressive and the covariance matrices and the posterior mean of the constant term behave.\n\n\n\n\n\n\n\n\nPosterior mean of the covariance matrix Sigma\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nY1-Lag\n0.9796123\n0.0878079\n\n\nY2-Lag\n0.0878079\n1.0176700\n\n\n\n\n\n\n\n\nPosterior mean of the autoregressive coefficient matrix A\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nConstant\n0.0603819\n0.0811782\n\n\nY1-Lag\n0.9894262\n0.0047783\n\n\nY2-Lag\n0.0023282\n0.9956869\n\n\n\n\n\nThe diagonal entries of the covariance matrix are close to 1, which indicates that each variable has a strong autoregressive relationship with itself and similarly, the diagonal elements of the autoregressive coefficient matrix are close to one, suggesting that each variable is heavily influenced by its past value. Besides, the posterior means for the constant terms is close to 0, the above can indicate that the estimated parameter constant term and means are consistent with what we expect given a Minnesota prior.\n\n\nProof of Extended Model\n\n\n\nPosterior mean of the covariance matrix Sigma\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nY1-Lag\n1.2311501\n0.1080764\n\n\nY2-Lag\n0.1080764\n1.3162664\n\n\n\n\n\n\n\n\nPosterior mean of the autoregressive coefficient matrix A\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nConstant\n0.0548787\n0.1056800\n\n\nY1-Lag\n0.9906143\n0.0094673\n\n\nY2-Lag\n0.0026284\n0.9935099\n\n\n\n\n\nSimilarly, the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and the posterior mean of the constant term is close to zero, so we can conclude that the extended model is also valid."
  },
  {
    "objectID": "index.html#forecasting-with-basic-model",
    "href": "index.html#forecasting-with-basic-model",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "Forecasting with basic model",
    "text": "Forecasting with basic model\n\n\n\n\n\n\n90%, 68% and 15% highest density intervals respectively\n\n\n\n\n\nThe above presents the historical and forecasting data the AUD/USD exchange rate for baisc and extended model.\nFor the point forecast, for basic model, the exchange rate shows several peaks and troughs, indicating the volatility in the exchange rate over the years. For the three years forecasting, it shows a clear downward trend and reaching the lowest above 0.6 in 2026. And for extended model, it also shows a clear downward trend for the first half forecasting period and reaching below 0.06 in around 2025 and slightly increase back to 0.61 fo the following period.\nRegard to the 3D with density intervals below for both model forecasting, we could notice that the for each different predictive density at specific horizons, with the back wall we have the one period ahead predictive density and with the front we have the 12 period ahead density which is 3 years. We could see the distribution becomes lower and more dispersed with the increases of the horizon, as the data is more informative about the nearest developments in the future. Hence, one period predictive density is highly concentrated relative to others with smaller variance and taller peak. Similarly, the interval become more wider and dispersed resulting a more uncertainty for the future period forecasting.\n\n\n\n\n\nInteractive versions of the above 3D plots are provided below.\n\n\n\n\n\n\n\n\n\n\nThe 3D plots above allow us to access to alternative vantage points. We could notice that for the first few forecasting periods, the extended model has very sharp and concentrated densities around the point estimated, and it shows clearly heavy tails, which may increase the possibility of capturing extreme values that deviate significantly from the central forecast."
  },
  {
    "objectID": "Test.html",
    "href": "Test.html",
    "title": "test",
    "section": "",
    "text": "# log transformation of data\ncpi_au &lt;- log(cpi_au)\ncpi_us &lt;- log(cpi_us)\nrgdp_au &lt;- log(rgdp_au)\nrgdp_us &lt;- log(rgdp_us)\nimpor_au &lt;- log(impor_au)\nimpor_us&lt;- log(impor_us)\nexpor_au &lt;- log(expor_au)\nexpor_us&lt;- log(expor_us)\ngprice &lt;- log(gprice)\n\n\n\n\n\n\n\n\n\n\nx_varibales &lt;-  na.omit(merge(gprice,\n                            cpi_au, cpi_us, \n                            crate_au, crate_us, \n                            expor_au, expor_us,  \n                            impor_au, impor_us, \n                            rgdp_au, rgdp_us,\n                            unemr_au, unemr_us))\ncolnames(x_varibales)   = c(\"gold price\", \n                         \"cpi_au\",    \"cpi_us\", \n                         \"cashrate_au\",  \"cashrate_us\", \n                         \"export_au\", \"export_us\",\n                         \"import_au\",   \"import_us\",\n                         \"realgdp_au\", \"realgdp_us\",\n                         \"unemployemtrate_au\", \"unemployemtrate_us\")\n\n\n# Compute correlation matrix\ncorrelation_matrix &lt;- cor(merged_data)\n\n# Find the row number corresponding to \"erate\"\nerate_row &lt;- which(rownames(correlation_matrix) == \"erate\")\n\n# Extract correlations between erate and other variables\ncorrelations_erate &lt;- correlation_matrix[erate_row, names(merged_data) != \"erate\"]\n\n# Print correlation values\nprint(correlations_erate)\n\n     exchange rate cpi_au cpi_us cashrate_au cashrate_us export_au export_us\n     import_au import_us realgdp_au realgdp_us unemployemtrate_au\n     unemployemtrate_us\n\n\n\nAugmented Dickey-Fuller test for log transformed variables except exchange rate and cash rate.\n\npar(mfcol = c(3, 2), mar=c(2,2,2,2))\nfor (i in 1:6){\nacf = acf(merged_data[,i], plot = FALSE)[1:20]\nplot(acf, main = \"\")\ntitle(main = paste(colnames(merged_data)[i]), line = 0.5)\n}\n\n\n\npar(mfrow = c(3, 2), mar=c(2,2,2,2))\nfor (i in 7:13){\nacf = acf(merged_data[,i], plot = FALSE)[1:20]\nplot(acf, main = \"\")\ntitle(main = paste(colnames(merged_data)[i]), line = 0.5)\n}\n\n\n\n\n\n\n\n\nadf_test &lt;- list()\nfor (i in 1:13) {\n  adf_result = adf.test(merged_data[,i], k = 4)\n  adf_test[[i]] &lt;- adf_result\n}\n\nprint(adf_test)\n\n[[1]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -2.0604, Lag order = 4, p-value = 0.5507\nalternative hypothesis: stationary\n\n\n[[2]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -1.9709, Lag order = 4, p-value = 0.5871\nalternative hypothesis: stationary\n\n\n[[3]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -1.355, Lag order = 4, p-value = 0.8378\nalternative hypothesis: stationary\n\n\n[[4]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -0.84089, Lag order = 4, p-value = 0.9536\nalternative hypothesis: stationary\n\n\n[[5]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -2.9006, Lag order = 4, p-value = 0.2088\nalternative hypothesis: stationary\n\n\n[[6]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -2.3971, Lag order = 4, p-value = 0.4137\nalternative hypothesis: stationary\n\n\n[[7]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -2.8393, Lag order = 4, p-value = 0.2337\nalternative hypothesis: stationary\n\n\n[[8]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -1.6656, Lag order = 4, p-value = 0.7113\nalternative hypothesis: stationary\n\n\n[[9]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -3.132, Lag order = 4, p-value = 0.1147\nalternative hypothesis: stationary\n\n\n[[10]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -3.0487, Lag order = 4, p-value = 0.1485\nalternative hypothesis: stationary\n\n\n[[11]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -2.7396, Lag order = 4, p-value = 0.2743\nalternative hypothesis: stationary\n\n\n[[12]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -2.1174, Lag order = 4, p-value = 0.5275\nalternative hypothesis: stationary\n\n\n[[13]]\n\n    Augmented Dickey-Fuller Test\n\ndata:  merged_data[, i]\nDickey-Fuller = -2.7746, Lag order = 4, p-value = 0.2601\nalternative hypothesis: stationary\n\n\n\nadf_table &lt;- data.frame(p_value = numeric(length(adf_test)))\n\nfor (i in 1:length(adf_test)) {adf_table[i, \"p_value\"] = round(adf_test[[i]]$p.value,3)\n}\n\nrownames(adf_table)&lt;- c(\"exchange rate\", \n                         \"cpi_au\",    \"cpi_us\", \n                         \"cashrate_au\",  \"cashrate_us\", \n                         \"export_au\", \"export_us\",\n                         \"import_au\",   \"import_us\",\n                         \"realgdp_au\", \"realgdp_us\",\n                         \"unemployemtrate_au\", \"unemployemtrate_us\")\n\ncolnames(adf_table)&lt;- c(\"P-value\")\nprint(adf_table)\n\n                   P-value\nexchange rate        0.551\ncpi_au               0.587\ncpi_us               0.838\ncashrate_au          0.954\ncashrate_us          0.209\nexport_au            0.414\nexport_us            0.234\nimport_au            0.711\nimport_us            0.115\nrealgdp_au           0.149\nrealgdp_us           0.274\nunemployemtrate_au   0.528\nunemployemtrate_us   0.260\n\n#clear that all &gt;0.05 and reject the null that is stationary\n\n\n#take the first difference\ndff_merged_data &lt;- na.omit(merged_data - lag(merged_data))\n\n\n# ADF test\ndff_adf_ &lt;- list()\nfor (i in 1:13) {\n  dff_adf_result = adf.test(dff_merged_data[,i], k = 4)\n  dff_adf_[[i]] &lt;- dff_adf_result\n}\n\nWarning in adf.test(dff_merged_data[, i], k = 4): p-value smaller than printed\np-value\n\nWarning in adf.test(dff_merged_data[, i], k = 4): p-value smaller than printed\np-value\n\n# View the ADF test results\ndff_adf_table &lt;- data.frame(p_value = numeric(length(dff_adf_)))\n\n# Fill in the data frame with the test results\nfor (i in 1:length(dff_adf_)) {\n  dff_adf_table[i, \"p_value\"] = round(dff_adf_[[i]]$p.value,3)\n\n}\nrownames(dff_adf_table)&lt;- c(\"exchange rate\",\n                         \"cpi_au\",    \"cpi_us\", \n                         \"cashrate_au\",  \"cashrate_us\", \n                         \"export_au\", \"export_us\",\n                         \"import_au\",   \"import_us\",\n                         \"realgdp_au\", \"realgdp_us\",\n                         \"unemployemtrate_au\", \"unemployemtrate_us\")\n\ncolnames(dff_adf_table)&lt;- c(\"P-value\")\nprint(dff_adf_table)\n\n                   P-value\nexchange rate        0.035\ncpi_au               0.442\ncpi_us               0.279\ncashrate_au          0.038\ncashrate_us          0.019\nexport_au            0.052\nexport_us            0.024\nimport_au            0.010\nimport_us            0.017\nrealgdp_au           0.034\nrealgdp_us           0.010\nunemployemtrate_au   0.023\nunemployemtrate_us   0.021\n\n\n\n#take the second difference \ndff_dff_data &lt;- subset(dff_merged_data, select = -c(realgdp_us))\n\n# Create a new dataset with the remaining variables\n\ndff_dff_merged_data &lt;- na.omit(dff_dff_data- lag(dff_dff_data)) \n\n\ndff_dff_adf_ &lt;- list()\nfor (i in 1:12) {\n  dff_dff_adf_result = adf.test(dff_dff_merged_data[,i], k = 4)\n  dff_dff_adf_[[i]] &lt;- dff_dff_adf_result\n}\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\nWarning in adf.test(dff_dff_merged_data[, i], k = 4): p-value smaller than\nprinted p-value\n\n# View the ADF test results\ndff_dff_adf_table &lt;- data.frame(p_value = numeric(length(dff_dff_adf_)))\n\n# Fill in the data frame with the test results\nfor (i in 1:length(dff_dff_adf_)) {\n  dff_dff_adf_table[i, \"p_value\"] = round(dff_dff_adf_[[i]]$p.value,3)\n\n}\nrownames(dff_dff_adf_table)&lt;- c(\"exchange rate\", \n                         \"cpi_au\",    \"cpi_us\", \n                         \"cashrate_au\",  \"cashrate_us\", \n                         \"export_au\", \"export_us\",\n                         \"import_au\",   \"import_us\",\n                         \"realgdp_au\",\n                         \"unemployemtrate_au\", \"unemployemtrate_us\")\n\ncolnames(dff_dff_adf_table)&lt;- c(\"P-value\")\nprint(dff_dff_adf_table)\n\n                   P-value\nexchange rate         0.01\ncpi_au                0.01\ncpi_us                0.01\ncashrate_au           0.01\ncashrate_us           0.01\nexport_au             0.01\nexport_us             0.01\nimport_au             0.01\nimport_us             0.01\nrealgdp_au            0.01\nunemployemtrate_au    0.01\nunemployemtrate_us    0.01\n\n\n\\[\\begin{align}\nY =\\begin{pmatrix}\ny_{crateau_,1}&  y_{rgdpau_,1}&  y_{cpiau_,1}&  y_{unemrau_,1}&  y_{imporau_,1}&  y_{exporau_,1}&  y_{crateus_,1}&  y_{rgdpus_,1}&  y_{cpius_,1}&  y_{unemrus_,1}&  y_{imporus_,1}&  y_{exporus_,1}\\\\\ny_{crateau_,2}&  y_{rgdpau_,2}&  y_{cpiau_,2}&  y_{unemrau_,2}&  y_{imporau_,2}&  y_{exporau_,2}&  y_{crateus_,2}&  y_{rgdpus_,2}&  y_{cpius_,2}&  y_{unemrus_,2}&  y_{imporus_,2}&  y_{exporus_,2}\\\\ \\ \\vdots & \\vdots & \\vdots  & \\vdots & \\vdots & \\vdots& \\vdots& \\vdots& \\vdots& \\vdots& \\vdots & \\vdots\\\\\n\ny_{crateau_,T}&  y_{rgdpau_,T}&  y_{cpiau_,T}&  y_{unemrau_,T}&  y_{imporau_,T}&  y_{exporau_,T}&  y_{crateus_,T}&  y_{rgdpus_,T}&  y_{cpius_,T}&  y_{unemrus_,T}&  y_{imporus_,T}&  y_{exporus_,T}\n\\end{pmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#forecasting-with-basic-model-and-extended-model",
    "href": "index.html#forecasting-with-basic-model-and-extended-model",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "Forecasting With Basic Model and Extended Model",
    "text": "Forecasting With Basic Model and Extended Model\n\n\n\n\n\n\n\n\n\n\n\nForecasting on extended model\n\n\n\n\n\n\n\n\n\n\nThe above presents the historical and forecasting data the AUD/USD exchange rate for basic and extended model.\nFor the point forecast, for basic model, the exchange rate shows several peaks and troughs, indicating the volatility in the exchange rate over the years. For the three years forecasting, it shows a slightly downward trend and reaching around 0.67 in 2026. And for extended model, it also shows a clear increasing trend and reaching around 0.751243 fo the following period. For conduct 90% confident interval for both models in which we can notice the extended model has an obvious narrow confident interval for both significant level, which means more informative, and this may be given by the nature of the Laplace distribution applied in the extended model in which the the fatter fail of Laplace distribution will capture more economic activities not covered by the underlying normal distribution, and resulting a more informative estimating of the parameters and then leading to a narrower confident interval as expected.\nRegard to the 3D with density intervals below for both model forecasting, we could notice that the for each different predictive density at specific horizons, with the back wall we have the one period ahead predictive density and with the front we have the 12 period ahead density which is 3 years. We could see the distribution becomes lower and more dispersed with the increases of the horizon, as the data is more informative about the nearest developments in the future. Hence, one period predictive density is highly concentrated relative to others with smaller variance and taller peak. Similarly, the interval become more wider and dispersed resulting a more uncertainty for the future period forecasting."
  },
  {
    "objectID": "index.html#the-extension-model-with-stochastic-volatility-heteroskedasticity",
    "href": "index.html#the-extension-model-with-stochastic-volatility-heteroskedasticity",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "The Extension Model With Stochastic Volatility Heteroskedasticity",
    "text": "The Extension Model With Stochastic Volatility Heteroskedasticity\nWith stochastic volatility heteroskedasticity in our extended model, we have: \\(\\Omega=diag(e^{h_1}\\lambda_1,e^{h_2}\\lambda_2,...,e^{h_T}\\lambda_t)\\), represents as:\n\\[\\\\\\Omega = \\left(\n\\begin{array}{cccc}\ne^{h_1}\\lambda_1 & 0 & \\cdots & 0 \\\\\n0 & e^{h_2}\\lambda_2 & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0 &  e^{h_T}\\lambda_T\\\\\n\\end{array}\n\\right)\\]\nThe likelihood function now change to:\n\\[\\begin{align}\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1} (e^{h_t}\\lambda_t)^{-\\frac{N}{2}} exp({-\\frac{1}{2}}\\sum^{T}_{t =1}{\\frac{1}{e^{h_t}\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t)\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1}( (e^{h_t}\\lambda_t)^{-\\frac{N}{2}} exp({-\\frac{1}{2}}{\\frac{1}{e^{h_t}\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t))\n\\end{align}\\]\nFor posterior distribution,\n\\[\\begin{align}\np(\\lambda_t|Y,X,A,\\Sigma) &\\propto L(A,\\Sigma,\\lambda_t|Y,X)p(\\lambda_t) \\\\\n\\\\\n&\\propto (e^{h_t}\\lambda_t)^{-\\frac{N}{2}} exp({-\\frac{1}{2}}{\\frac{1}{e^{h_t}\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t) \\\\\n&\\times \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n\n\n&= \\lambda_t^{-\\frac{N}{2}+1-1} exp\\{-\\frac{1}{2}[\\frac{\n\\epsilon_t'e^{-h_t}\\Sigma^{-1}\\epsilon_t} {\\lambda_t} +\\frac{2}{\\alpha}\\lambda_t]\\}\n\\end{align}\\]\n\\[\\begin{align}\n\\lambda_t|Y,A,\\Sigma &\\sim GIG(a,b,p) \\\\\n\\\\\na &=\\frac{2}{\\alpha} \\\\\nb &= \\epsilon_t' e^{-h_t}\\Sigma^{-1}\\epsilon_t \\\\\np &= -\\frac{N}{2}+1\n\\end{align}\\]\nGibbs Sampling Routine for SV and Laplace Distributed Errors\nInitialize \\(\\lambda^{(0)}\\) and \\(h_t^{(0)}\\).\nAt each iteration s:\n\nDraw \\(\\Sigma^{(s)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution\nDraw \\(A^{(s)}\\) from the \\(MN(\\bar{A},\\Sigma^{(s)}, \\bar{V})\\) distribution\nDraw \\(\\lambda_t^{(s)}\\) from \\(GIG(a,b,p)\\)\nDraw \\(h^{(s)}\\) from the SV sampling routine described above\n\n\nhead(round(apply(posterior.draws.heter.ext$Sigma.posterior, 1:2, mean),6))\n\n          [,1]      [,2]     [,3]     [,4]     [,5]      [,6]     [,7]\n[1,]  0.001748 -0.000011 0.000054 0.004243 0.000652 -0.000440 0.000177\n[2,] -0.000011  0.000034 0.000018 0.000939 0.000570  0.000027 0.000112\n[3,]  0.000054  0.000018 0.000073 0.001040 0.001930  0.000056 0.000113\n[4,]  0.004243  0.000939 0.001040 0.228606 0.081725  0.002532 0.006125\n[5,]  0.000652  0.000570 0.001930 0.081725 0.487819  0.004447 0.004414\n[6,] -0.000440  0.000027 0.000056 0.002532 0.004447  0.002519 0.000629\n          [,8]     [,9]     [,10]    [,11]     [,12]     [,13]\n[1,] -0.001369 0.000172  0.000010 0.000023 -0.000270  0.002262\n[2,]  0.000089 0.000108  0.000016 0.000018 -0.000722 -0.001693\n[3,]  0.000093 0.000115  0.000007 0.000012 -0.000349 -0.000890\n[4,]  0.001508 0.007324  0.000407 0.000875 -0.042629 -0.058830\n[5,]  0.003225 0.004838 -0.000236 0.000594 -0.027237 -0.056852\n[6,]  0.001708 0.000783  0.000116 0.000108 -0.004753 -0.007247\n\nhead(round(apply(posterior.draws.heter.ext$A.posterior, 1:2, mean),6))\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,]  0.010941 -0.017893 -0.005026 -0.009278 -0.021426  0.000533 -0.002469\n[2,] -0.008914  0.012570  0.013816  0.012148 -0.000773 -0.006951  0.010760\n[3,] -0.016836  0.002213  0.003431  0.013667 -0.003291  0.007056 -0.007025\n[4,]  0.018453 -0.007552 -0.015336 -0.006197  0.008030 -0.003768 -0.016066\n[5,] -0.003943 -0.000569  0.002906 -0.002166  0.004395  0.000259  0.003756\n[6,]  0.008116 -0.006178 -0.008465 -0.003980 -0.007993  0.006197 -0.006969\n          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]\n[1,]  0.003114 -0.002409  0.013222 -0.008543 -0.014699 -0.009912\n[2,] -0.002427 -0.000458 -0.005868 -0.015554 -0.002634  0.007291\n[3,] -0.008042  0.001950 -0.003973  0.001888  0.013728 -0.000739\n[4,] -0.001515 -0.000413 -0.005543 -0.002105 -0.011641 -0.018743\n[5,] -0.000358 -0.009886 -0.015142  0.002379  0.007866  0.008620\n[6,]  0.004981 -0.022774  0.010009 -0.010843 -0.000266 -0.009447\n\n\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\nplot.ts(posterior.draws.heter.ext$A.posterior[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate\", col = mcxs1)\nhist(posterior.draws.heter.ext$A.posterior[1,1,], xlab = \"AUD/USD exchange rate\", col = mcxs1, main = '')\n\nplot.ts(posterior.draws.heter.ext$Sigma.posterior[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate sigma\", col = mcxs2)\nhist(posterior.draws.heter.ext$Sigma.posterior[1,1,], xlab = \"AUD/USD exchange rate sigma\", col = mcxs2, main = '')\n\n\n\n\nFor extended model with heteroskedasticity, the coefficient range now wider a lot, which ranges from -3 to 3."
  },
  {
    "objectID": "testtttt.html",
    "href": "testtttt.html",
    "title": "Untitled",
    "section": "",
    "text": "The second extension to the model is a further augmentation to the form of the errors in order to account for stochastic volatility. We can explicitly account for heteroskedasticity in the errors by applying a model specification in which the variance changes over time according to some stochastic process. This extension can be combined with the previous model including t-distributed errors to achieve an even more robust model for errors.\n\\[ \\begin{align}\nY &= XA + U \\\\\nU|\\lambda &\\sim MN(0, \\Sigma, \\lambda \\text{diag}(\\sigma^2)) \\\\\n\\lambda &\\sim IG2( s_{\\lambda}, \\nu_{\\lambda}) \\\\\n\\\\\n\\sigma^2 &= (\\exp(h_1), ..., exp(h_T)) \\\\\nh_T &- \\text{follows a stochastic volatility process} \\\\\n\\lambda &- \\text{the scale parameter for t errors}\n\\end{align}\\]\nIt is convenient for estimation to assume \\(h_t\\) follows a random walk process. That is,\n\\[\n\\begin{align}\nh_t &= h_{t-1} + \\sigma_v v_t \\\\\n\\\\\nv_t &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma^2_v &- \\text{estimated parameter of the model}\n\\end{align}\n\\]\nEstimation of \\(h\\) is completed via its own Gibbs Sampling routine. The sampler is applied to a log-linearised form of the data which strips out the conditional mean in order to isolate the error term. The sampling routine involves drawing estimated parameters from a combination of Normal, IG2 as well the log Chi-Square distribution. The log Chi-Square distribution in this case is approximated by a mixture of ten normal distributions and sampled from using the inverse transform method. One pass of the sampler draws a sample of all of the estimated parameters, including \\(T\\times1\\) vector \\(h\\), the exponent of which forms \\(\\sigma^2\\).\nWith the overall model specified in this form, the likelihood function is as follows, with the \\(\\sigma^2\\) diagonal matrix entering in place of the previous identity matrix \\(I_T\\).\n\\[\n\\begin{align}\nL(A,\\Sigma,\\Lambda|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda \\times \\text{diag}(\\sigma^2))^{-\\frac{N}{2}} exp(-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda \\times \\text{diag}(\\sigma^2))^{-1} (Y-XA) ])\n\\end{align}\n\\]\nFollowing the same derivations as before, we compute the full conditionals for \\(A\\), \\(\\Sigma\\) and \\(\\lambda\\) as\n\\[\n\\begin{align}\np(A,\\Sigma|Y,X) &= MNIW(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n\\\\\n\\bar{V} &= (X'(\\lambda  \\text{diag}(\\sigma^2))^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'(\\lambda  \\text{diag}(\\sigma^2))^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{S} &= Y'(\\lambda  \\text{diag}(\\sigma^2))^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S} - \\bar{A}'\\bar{V}^{-1}\\bar{A} \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\n\\\\\n\\\\\np(\\lambda|Y,A,\\Sigma) &= IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}}) \\\\\n\\bar{s_{\\lambda}} &= tr[\\Sigma^{-1}(Y-XA)'\\text{diag}(\\sigma^2)^{-1}(Y-XA)] + \\underline{s_{\\lambda}} \\\\\n\\bar{\\nu_{\\lambda}} &= TN + \\underline{\\nu_{\\lambda}}\n\\end{align}\n\\] \\[\n\\begin{align}\np\\left(\\sigma^2_e\\right)&\\propto\\left(\\sigma^2_e\\right)^{-\\frac{\\underline{\\nu}+2}{2}} \\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2_e} \\right\\}\n\\end{align}\n\\]\n\n\nInitialize \\(\\lambda^{(0)}\\) and \\(h_t^{(0)}\\).\nAt each iteration:\n\nDraw \\(\\Sigma^{(i)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution\nDraw \\(A^{(i)}\\) from the \\(MN(\\bar{A},\\Sigma^{(i)}, \\bar{V})\\) distribution\nDraw \\(\\lambda^{(i)}\\) from \\(IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}})\\)\nDraw \\(h^{(i)}\\) from the SV sampling routine described above"
  },
  {
    "objectID": "testtttt.html#model-extension-stochastic-volatility-with-t-distributed-errors",
    "href": "testtttt.html#model-extension-stochastic-volatility-with-t-distributed-errors",
    "title": "Untitled",
    "section": "",
    "text": "The second extension to the model is a further augmentation to the form of the errors in order to account for stochastic volatility. We can explicitly account for heteroskedasticity in the errors by applying a model specification in which the variance changes over time according to some stochastic process. This extension can be combined with the previous model including t-distributed errors to achieve an even more robust model for errors.\n\\[ \\begin{align}\nY &= XA + U \\\\\nU|\\lambda &\\sim MN(0, \\Sigma, \\lambda \\text{diag}(\\sigma^2)) \\\\\n\\lambda &\\sim IG2( s_{\\lambda}, \\nu_{\\lambda}) \\\\\n\\\\\n\\sigma^2 &= (\\exp(h_1), ..., exp(h_T)) \\\\\nh_T &- \\text{follows a stochastic volatility process} \\\\\n\\lambda &- \\text{the scale parameter for t errors}\n\\end{align}\\]\nIt is convenient for estimation to assume \\(h_t\\) follows a random walk process. That is,\n\\[\n\\begin{align}\nh_t &= h_{t-1} + \\sigma_v v_t \\\\\n\\\\\nv_t &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma^2_v &- \\text{estimated parameter of the model}\n\\end{align}\n\\]\nEstimation of \\(h\\) is completed via its own Gibbs Sampling routine. The sampler is applied to a log-linearised form of the data which strips out the conditional mean in order to isolate the error term. The sampling routine involves drawing estimated parameters from a combination of Normal, IG2 as well the log Chi-Square distribution. The log Chi-Square distribution in this case is approximated by a mixture of ten normal distributions and sampled from using the inverse transform method. One pass of the sampler draws a sample of all of the estimated parameters, including \\(T\\times1\\) vector \\(h\\), the exponent of which forms \\(\\sigma^2\\).\nWith the overall model specified in this form, the likelihood function is as follows, with the \\(\\sigma^2\\) diagonal matrix entering in place of the previous identity matrix \\(I_T\\).\n\\[\n\\begin{align}\nL(A,\\Sigma,\\Lambda|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda \\times \\text{diag}(\\sigma^2))^{-\\frac{N}{2}} exp(-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda \\times \\text{diag}(\\sigma^2))^{-1} (Y-XA) ])\n\\end{align}\n\\]\nFollowing the same derivations as before, we compute the full conditionals for \\(A\\), \\(\\Sigma\\) and \\(\\lambda\\) as\n\\[\n\\begin{align}\np(A,\\Sigma|Y,X) &= MNIW(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n\\\\\n\\bar{V} &= (X'(\\lambda  \\text{diag}(\\sigma^2))^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'(\\lambda  \\text{diag}(\\sigma^2))^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{S} &= Y'(\\lambda  \\text{diag}(\\sigma^2))^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S} - \\bar{A}'\\bar{V}^{-1}\\bar{A} \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\n\\\\\n\\\\\np(\\lambda|Y,A,\\Sigma) &= IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}}) \\\\\n\\bar{s_{\\lambda}} &= tr[\\Sigma^{-1}(Y-XA)'\\text{diag}(\\sigma^2)^{-1}(Y-XA)] + \\underline{s_{\\lambda}} \\\\\n\\bar{\\nu_{\\lambda}} &= TN + \\underline{\\nu_{\\lambda}}\n\\end{align}\n\\] \\[\n\\begin{align}\np\\left(\\sigma^2_e\\right)&\\propto\\left(\\sigma^2_e\\right)^{-\\frac{\\underline{\\nu}+2}{2}} \\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2_e} \\right\\}\n\\end{align}\n\\]\n\n\nInitialize \\(\\lambda^{(0)}\\) and \\(h_t^{(0)}\\).\nAt each iteration:\n\nDraw \\(\\Sigma^{(i)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution\nDraw \\(A^{(i)}\\) from the \\(MN(\\bar{A},\\Sigma^{(i)}, \\bar{V})\\) distribution\nDraw \\(\\lambda^{(i)}\\) from \\(IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}})\\)\nDraw \\(h^{(i)}\\) from the SV sampling routine described above"
  },
  {
    "objectID": "index.html#the-basic-model-with-stochastic-volatility-heteroskedasticity",
    "href": "index.html#the-basic-model-with-stochastic-volatility-heteroskedasticity",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "The Basic Model With Stochastic Volatility Heteroskedasticity",
    "text": "The Basic Model With Stochastic Volatility Heteroskedasticity\nThe second modification enhances the model by capturing stochastic volatility in the error terms, which means the variance might vary over time according to a stochastic process. This adjustment allows the model to specifically account for heteroskedasticity, creating a more dynamic error structure. By integrating this feature with the earlier enhancement that introduced laplace errors, the model becomes significantly more robust in handling error variability.\nWith the stochastic volatility in innovation term, the basic model now is:\n\\[\\begin{align}\nY &= X A + U\n\\\\\nU |X &\\sim \\mathcal{MN}_{T\\times 13}(0,\\Sigma,\\mathrm{diag}(\\sigma_{T}^{2}))\n\\end{align} \\]\nWith \\(\\mathrm{diag}(\\sigma_{T}^{2})=diag(e^{h_1},e^{h_2},...,e^{h_T})\\), represent as:\nWhere \\(h_t\\) follows an AR(1) process:\n\\[\\begin{align}\nh_t &= h_{t-1} + \\sigma_v v_t \\\\\nv_t &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma^2_v &\\sim IG2(\\bar{s},\\bar{\\nu})\n\\end{align}\\]\nThe basic heteroskedasticity model is still based on Natural-conjugate prior distribution, where the \\(A\\) follows a Matrix-variate Normal distribution and \\(\\Sigma\\) follows an Inverse Wishart distribution.\nWith the likelihood function now changes to :\n\\[\\begin{align}\nL\\left( {A},{\\Sigma}|Y,X \\right) &\\propto \\text{det}({\\Sigma})^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\text{tr}\\left[ {\\Sigma}^{-1}(Y-X{A})'(Y-X{A}) \\right] \\right\\}\\\\\n&=\\text{det}({\\Sigma})^{-\\frac{T}{2}}\\\\\n&\\quad\\times\\exp\\left\\{ -\\frac{1}{2}\\text{tr}\\left[ {\\Sigma}^{-1}({A}-\\widehat{A})'X' \\ \\mathrm{diag}(\\sigma_{T}^{2})^{-1} \\ X({A}-\\widehat{A}) \\right] \\right\\}\\\\\n&\\quad\\times \\exp\\left\\{ -\\frac{1}{2}\\text{tr}\\left[ {\\Sigma}^{-1}(Y-X\\widehat{A})' \\ \\mathrm{diag}(\\sigma_{T}^{2})^{-1} \\ (Y-X\\widehat{A}) \\right] \\right\\}\n\\end{align} \\]\nFor posterior distribution,\n\\[\\begin{align}\np\\left( {A},{\\Sigma} |Y,X\\right)\n&\\propto  \\text{det}({\\Sigma})^{-\\frac{T}{2}}\\\\\n&\\quad\\times\\exp\\left\\{ -\\frac{1}{2}\\text{tr}\\left[ {\\Sigma}^{-1}({A}-\\widehat{A})'X' \\ \\mathrm{diag}(\\sigma_{T}^{2})^{-1} \\ X({A}-\\widehat{A}) \\right] \\right\\}\\\\\n&\\quad\\times \\exp\\left\\{ -\\frac{1}{2}\\text{tr}\\left[ {\\Sigma}^{-1}(Y-X\\widehat{A})' \\ \\mathrm{diag}(\\sigma_{T}^{2})^{-1} \\ (Y-X\\widehat{A}) \\right] \\right\\}\\\\\n& \\quad\\times\\text{det}({\\Sigma})^{-\\frac{N+K+\\underline{\\nu}+1}{2}}\\\\\n&\\quad\\times\\exp\\left\\{ -\\frac{1}{2}\\text{tr}\\left[ {\\Sigma}^{-1}({A}-\\underline{A})'\\underline{V}^{-1}({A}-\\underline{A}) \\right] \\right\\}\\\\\n&\\quad\\times \\exp\\left\\{ -\\frac{1}{2}\\text{tr}\\left[ {\\Sigma}^{-1}\\underline{S} \\right] \\right\\} \\\\\n\n\\end{align}\\]\nSimilarly, the kernel can be rearranged in the form of the Matrix-variate normal-inverse Wishart distribution.\n\\[\\begin{align}\n\\bar{V} &= (X'\\mathrm{diag}(\\sigma_{T}^{2})^{-1}X + \\underline{V}^{-1})^{-1}\\\\\n\\bar{A} &= \\bar{V}(X'\\mathrm{diag}(\\sigma_{T}^{2})^{-1}Y+\\underline{V}^{-1}\\underline{A})\\\\\n\\bar{\\nu} &= T+\\underline{\\nu}\\\\\n\\bar{S} &= Y'\\mathrm{diag}(\\sigma_{T}^{2})^{-1}Y+\\underline{A}'\\underline{V}^{-1}\\underline{A}-\\bar{A'}\\bar{V}^{-1}\\bar{A}+\\underline{S}\n\\end{align}\\]\n\nEstimation Outcomes on Basic Stochastic Volatility Model\n\nhead(round(apply(A.posterior.heter, 1:2, mean),6))         # posterior draw A\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,]  0.005520  0.005469  0.005497 -0.047023  0.057079  0.010392  0.014101\n[2,]  0.999997  0.000000  0.000000 -0.000032 -0.000030  0.000005  0.000001\n[3,]  0.000004  1.000001 -0.000001  0.000094  0.000044 -0.000006 -0.000002\n[4,]  0.000002  0.000001  0.999999  0.000063  0.000053 -0.000003 -0.000001\n[5,]  0.000000 -0.000004  0.000001  0.999252 -0.000274 -0.000013  0.000014\n[6,] -0.000012 -0.000001 -0.000006 -0.000119  0.999503 -0.000031  0.000000\n          [,8]      [,9]     [,10]    [,11]    [,12]     [,13]\n[1,]  0.015482  0.014708  0.007020 0.007093 0.017148 -0.070498\n[2,]  0.000002 -0.000001  0.000000 0.000000 0.000017 -0.000022\n[3,] -0.000003 -0.000003 -0.000001 0.000000 0.000047 -0.000032\n[4,]  0.000004 -0.000002  0.000000 0.000000 0.000012 -0.000003\n[5,] -0.000021  0.000000 -0.000002 0.000000 0.000370  0.000370\n[6,] -0.000018 -0.000008  0.000002 0.000005 0.000213  0.000334\n\nhead(round(apply(Sigma.posterior.heter, 1:2, mean),6))     # posterior draw sigma\n\n         [,1]     [,2]     [,3]      [,4]      [,5]      [,6]   [,7]      [,8]\n[1,]  6.1e-05  0.0e+00 -1.0e-06  0.000060 -0.000083 -0.000034  1e-06 -0.000062\n[2,]  0.0e+00  1.0e-06  0.0e+00  0.000030  0.000011 -0.000001  0e+00  0.000000\n[3,] -1.0e-06  0.0e+00  2.0e-06 -0.000017  0.000033  0.000001  0e+00 -0.000003\n[4,]  6.0e-05  3.0e-05 -1.7e-05  0.007490 -0.000490 -0.000191 -4e-06 -0.000060\n[5,] -8.3e-05  1.1e-05  3.3e-05 -0.000490  0.021187  0.000152 -6e-05 -0.000141\n[6,] -3.4e-05 -1.0e-06  1.0e-06 -0.000191  0.000152  0.000147  1e-05  0.000054\n         [,9]  [,10]    [,11]     [,12]     [,13]\n[1,] -2.0e-06 -1e-06  0.0e+00 -0.000019  0.000063\n[2,]  1.0e-06  0e+00  0.0e+00 -0.000010 -0.000008\n[3,]  1.0e-06  0e+00  0.0e+00 -0.000013 -0.000003\n[4,] -2.0e-06  2e-06 -6.0e-06 -0.000486 -0.000796\n[5,] -2.8e-05 -5e-05 -1.4e-05 -0.001386 -0.001057\n[6,]  1.6e-05  3e-06  1.0e-06 -0.000216 -0.000032\n\n\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\nplot.ts(A.posterior.heter[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate\", col = mcxs1)\nhist(A.posterior.heter[1,1,], xlab = \"AUD/USD exchange rate\", col = mcxs1, main = '')\n\nplot.ts(Sigma.posterior.heter[1,1,], xlab = \"Simulation times S\", ylab = \"AUD/USD exchange rate sigma\", col = mcxs2)\nhist(Sigma.posterior.heter[1,1,], xlab = \"AUD/USD exchange rate sigma\", col = mcxs2, main = '')\n\n\n\n\nWe can notice that compared to the above two models, the involve of the heteroskedasticity brings out outliers, and these outliers seems all around the zero, with the coefficient also central round 1 and the sigma more central around zero. The change is obvious and significant, which will be demonstrated in the following forecasting plot."
  },
  {
    "objectID": "index.html#heter-plot",
    "href": "index.html#heter-plot",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "heter plot",
    "text": "heter plot\n\n\n\n\n\nInteractive versions of the above 3D plots are provided below.\n\n\n\n\n\n\n\n\n\n\nThe 3D plots above allow us to access to alternative vantage points. We could notice that for the first few forecasting periods, the extended model has very sharp and concentrated densities around the point estimated, and it shows clearly heavy tails, which may increase the possibility of capturing extreme values that deviate significantly from the central forecast."
  },
  {
    "objectID": "index.html#forecasting-with-models",
    "href": "index.html#forecasting-with-models",
    "title": "Exchange Rate Forecasting Using Bayesian VARs Model",
    "section": "Forecasting With Models",
    "text": "Forecasting With Models\n\nForecasting With Basic Model\n\n\n\n\n\nThe above presents the historical and forecasting data the AUD/USD exchange rate for basic and extended model.\nFor the point forecast, for basic model, the exchange rate shows several peaks and troughs, indicating the volatility in the exchange rate over the years. For the three years forecasting, it shows a slightly downward trend and reaching around 0.62 in 2026.\n\n\nForecasting on Extended Model\n\n\n\n\n\nFor extended model, it shows a clear increasing trend and reaching around 0.70 in the end of the forecasting period. For conduct 90% confident interval, we can notice the extended model has an wider confident interval, and this may be given by the nature of the Laplace distribution applied in the extended model in which the the fatter fail of Laplace distribution will capture more economic activities not covered by the underlying normal distribution.\n\n\n\n\n\nRegard to the 3D with density intervals above for both model forecasting, we could notice that the for each different predictive density at specific horizons, with the back wall we have the one period ahead predictive density and with the front we have the 12 period ahead density which is 3 years. We could see the distribution becomes lower and more dispersed with the increases of the horizon, as the data is more informative about the nearest developments in the future. Hence, one period predictive density is highly concentrated relative to others with smaller variance and taller peak. Similarly, the interval become more wider and dispersed resulting a more uncertainty for the future period forecasting.\n\n\nForecasting on Basic Model With Stochastic Volatility Heteroskedasticity\n\n\n\n\n\nBy involving the heteroskedasticity in the error term, the forecasting plot looks similar to the extended model, with the heteroskedasticity extension, it shows an increasing trend but reaching to 0.75 and we could notice that the confident interval is slightly narrower compared to the basic model, indicating a more informative estimating of the parameters as it has a higher certainty from a more concentrated mean value and then leading to a narrower confident interval as expected.\n\n\nInteractive 3D versions\nInteractive versions of the above 3D plots are provided below.\n\n\n\n\n\n\n\n\n\n\nThe 3D plots above allow us to access to alternative vantage points. We could notice that for the first few forecasting periods, the extended model has very sharp and concentrated densities around the point estimated, and it shows clearly heavy tails, which may increase the possibility of capturing extreme values that deviate significantly from the central forecast."
  }
]