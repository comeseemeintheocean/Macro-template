---
title: "Exchange Rate Forecasting Using Bayesian VARs Model"
author: "Qingqing Pang"
execute:
  
  echo: false
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

> **Abstract.** This research report explores how Bayesian VARs model
> predict AUD/USD exchange rate. **Keywords.** Bayesian Vars, Exchange
> rate, Forecasting, Minnesota Prior, Laplace distribution

# Objective and Motivation

The objective of this research is to use the Bayesian Vector
Autoregressions (VARs) method to forecast the exchange rate of the US
dollar exchange rate against the Australian dollar.

The ability to accurately forecast the foreign exchange rate is crucial
for Australia’s global trade and investment. Given the prominence of the
US dollar as the world's primary reserve currency, the monetary policies
of the US Federal Reserve have a worldwide effect on the world economy,
its significant influence on the Australia currency market should be
important for domestic investors and policy maker as it can directly
impact the AUD/USD exchange rate. Besides, America as one of the major
trading partners for Australia, can impact bilateral trade flows and
eventually affect the value of the AUD. Apart from the external from
foreign countries, the domestic economic indicators can also be one of
the determinants of the AUD/USD exchange rate.

The research is aimed to address the question for example, how the
AUD/USD exchange rate will be in 3 months or even longer 1 year?

# Data and Variables

To better forecast the change in the exchange rate, the 13 variables are
selected as follows which contain both domestic and US economic
indicators that affect the exchange rate in different ways.

Real GDP and interest rates have a significant effect on the exchange
rate, a higher realGDP and interest rate in Australia may increase the
demand for AUD, which will lead to an appreciation of AUD and a rise in
the AUD/USD exchange rate. A higher CPI indicates a lower purchasing
power relative to foreign currency which may lead to a depreciation of
the domestic currency. The unemployment rate can in some way represent
business activity and a country with a high unemployment rate will lower
the attractiveness for foreign investors and weaken the domestic
currency competitiveness in the currency market. The balance of trade,
which is the difference between exports and imports, also can influence
the demand for its currency, a trade surplus in AUD may increase the
demand for AUD dollar.

-   $erate_{t}$: AUD/USD exchange rate

-   AUS economic indicators

    -   $crate\_au_{t}$: The Cash Rate Target, Australia
    -   $rgdp\_au_{t}$: The Real Gross Domestic Product, Australia
    -   $cpi\_au_{t}$: The Consumer Price Index, Australia
    -   $unemr\_au_{t}$: The Unemployment rate, Australia
    -   $impor\_au_{t}$: The Imports of Goods and Services, Australia
    -   $expor\_au_{t}$: The Exports of Goods and Services, Australia

-   US economic indicators

    -   $crate\_us_{t}$: The Federal Funds Effective Rate, United States
    -   $rgdp\_us_{t}$: The Real Gross Domestic Product, United States
    -   $cpi\_us_{t}$: The Consumer Price Index, United States
    -   $unemr\_us_{t}$: The Unemployment rate, United States
    -   $impor\_us_{t}$: The Imports of Goods and Services, United
        States
    -   $expor\_us_{t}$: The Exports of Goods and Services, United
        States

### Data Cleaning

For the data cleaning part, most data for AUS is from Reserve Bank of
Australia (RBA) and Australian Bureau of Statistics (ABS), and data for
the US is from FRED, the dataset spans from 1990 Q1 to 2023 Q4,
comprising 136 observations. To better fit the model, the data has been
transformed to 'quarter' to ensure that seasonality effects are removed
and logged transformations have been applied to most data except
exchange rate and cash rate to reducing outlier effects.

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false
#| results: hide

library(fredr)
library(readrba)
library(readabs)
library(xts)
library(tseries)
library(fUnitRoots) 
library(plot3D)
library(MASS)
library(HDInterval)
library(plotly)       
library(mgcv)
library(mvtnorm)
library(Rmpfr)
library(dplyr)
library(psych)
library(ggplot2)
fredr_set_key("75b470c4883ecfd5a7b4185f30437bd0")
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)

```

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false

#Y variable
# AUD/USD exchange rate quarterly
ex_rate <- read_rba(series_id = "FXRUSD")
erate <- xts(ex_rate$value, ex_rate$date)
end_quartly <- endpoints(erate, on = "quarters")
erate <- erate[end_quartly]
index(erate) <- seq(as.Date("1969-09-01"), by = "3 months", length.out = nrow(erate))

# Australia real gdp seasonal adjusted quarterly
#rgdp_au <- read_abs(series_id = "A2304404C")  
rgdp_au <- read_rba(series_id = "GGDPCVGDP")
rgdp_au <- xts::xts(rgdp_au$value, rgdp_au$date)
index(rgdp_au)   <- seq(as.Date("1959-09-01"), by = "3 months", length.out = nrow(rgdp_au))

#cash rate/interest rate of AUS quartly
cashrate<- read_cashrate(type = c("target"))
crate_au<- xts(cashrate$value, cashrate$date)
end_quartly <- endpoints(crate_au, on = "quarters")
crate_au <- crate_au[end_quartly]
index(crate_au) <- seq(as.Date("1990-03-01"), by = "quarter", length.out = length(crate_au))

#CPI quartly
# cpi_au <- read_rba(series_id = "GCPIAG")
cpi_au <- read_abs(series_id = "A2325846C")  
cpi_au <- xts::xts(cpi_au$value, cpi_au$date)

#unemployment rate quartly
#unemprate <-read_rba(series_id = "GLFSURSA")
unemprate <- read_abs(series_id = "A84423050A") 
unemr_au<- xts(unemprate$value, unemprate$date)
end_quartly <- endpoints(unemr_au, on = "quarters")
unemr_au <- unemr_au[end_quartly]

# International Trade in Goods and Services seasonal adjusted_quartly
exportaus <- read_abs(series_id = "A2718603V")   
expor_au<- xts(exportaus $value, exportaus$date)
expor_au<- abs(expor_au)
end_quartly <- endpoints(expor_au, on = "quarters")
expor_au <- expor_au[end_quartly]

importaus <- read_abs(series_id = "A2718577A")     
impor_au<- xts(importaus$value, importaus$date)
end_quartly <- endpoints(impor_au, on = "quarters")
impor_au <- impor_au[end_quartly]

# America data
# us gdp
#rgdpus <- fredr(series_id = "A939RX0Q048SBEA")
rgdpus     <- fredr(series_id = "GDPC1")
rgdp_us     <- to.quarterly(xts(rgdpus$value, rgdpus$date), OHLC = FALSE)
index(rgdp_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(rgdp_us))

#Federal Funds Effective Rate/interest rate quartly
usdratelink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DFF&scale=left&cosd=1954-07-01&coed=2024-03-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Daily%2C%207-Day&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-04-01&revision_date=2024-04-01&nd=1954-07-01"
crate_us <- read.csv(usdratelink)
crate_us$DATE <- as.Date(crate_us$DATE)
crate_us <- xts(crate_us$DFF, order.by = crate_us$DATE)
end_quartly <- endpoints(crate_us, on = "quarters")
crate_us <- crate_us[end_quartly]
index(crate_us) <- seq(as.Date("1954-09-01"), by = "quarter", length.out = length(crate_us))

# cpi quartly
cpiusd  <- fredr(series_id = "USACPIALLMINMEI")
cpi_us<- xts(cpiusd$value, cpiusd$date)
end_quartly <- endpoints(cpi_us, on = "quarters")
cpi_us <- cpi_us[end_quartly]

# unemployment quartly
unemprate_usd = fredr(series_id = "UNRATE")
unemr_us <- xts(unemprate_usd$value, unemprate_usd$date)
end_quartly <- endpoints(unemr_us, on = "quarters")
unemr_us <- unemr_us[end_quartly]

#export_usd——quartly
usdexportink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=EXPGS&scale=left&cosd=1947-01-01&coed=2023-10-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-03-30&revision_date=2024-03-30&nd=1947-01-01"
expor_us <- read.csv(usdexportink)
expor_us$DATE <- as.Date(expor_us$DATE)
expor_us <- xts::xts(expor_us$EXPGS, order.by = expor_us$DATE)
index(expor_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(expor_us))

#import_usd_quartly
usdimportlink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=IMPGS&scale=left&cosd=1947-01-01&coed=2023-10-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-03-30&revision_date=2024-03-30&nd=1947-01-01"
impor_us <- read.csv(usdimportlink)
impor_us$DATE <- as.Date(impor_us$DATE)
impor_us <- xts::xts(impor_us$IMPGS, order.by = impor_us$DATE)
index(impor_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(impor_us))

```

```{r}
#| echo: false
#| message: false
#| warning: false
# log transformation of data
variables <- c("cpi_au", "cpi_us", "rgdp_au", "rgdp_us", "impor_au", "impor_us", "expor_au", "expor_us")

for(var in variables) {
  assign(var, log(get(var)))
}

```

```{r}
#| echo: false
#| message: false
#| warning: false
 
# All Variables
merged_data = na.omit(merge(erate, 
                            cpi_au, cpi_us, 
                            crate_au, crate_us, 
                            expor_au, expor_us,  
                            impor_au, impor_us, 
                            rgdp_au, rgdp_us,
                            unemr_au, unemr_us))

# Defining your column name vector:
variable_names <- c("exchange rate", "cpi_au", "cpi_us", 
                    "cashrate_au", "cashrate_us", "export_au", "export_us",
                    "import_au", "import_us", "realgdp_au", "realgdp_us",
                    "unemployemtrate_au", "unemployemtrate_us")


colnames(merged_data)   <- variable_names
```

#### Visualisation of data

Plot the variables to see the patterns of data. It shows from the plots
that the exchange rate and cash rate for the US fluctuate over time, the
cash rate and unemployment for AU show a downward trend and all other
variables have a clear upward trend, with the exports、imports, and GDP
for both countries have a clear drop during the COVID-19 period.

```{r all variables plot}
#| echo: false
#| message: false
#| warning: false

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:13) { 
  ts.plot(merged_data[, i], main = colnames(merged_data)[i], 
          ylab = "", xlab = "")
}

```

Since most variables show a non-stationary pattern. To determine whether
a unit root is present in a time series dataset, the ADF test will be
conducted below.

#### Augmented Dickey-Fuller test for log transformed variables except exchange rates and cash rates.

From the plot we can observe all ACF plots have a high degree of
persistence over time, indicating there is significant autocorrelation
in the time series data.

```{r}
#| echo: false
#| message: false
#| warning: false

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:13){
acf = acf(merged_data[,i], plot = FALSE)[1:20]
plot(acf, main = "")
title(main = paste(colnames(merged_data)[i]), line = 0.5)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
adf_test <- list()
for (i in 1:13) {
  adf_result = adf.test(merged_data[,i], k = 4)
  adf_test[[i]] <- adf_result
}
```

Below is the p-value of each variable and only the cash rate for AUS has
a p-value less than 0.05 which indicates that $crateau_{t}$ is
stationary.

```{r}
#| echo: false
#| message: false
#| warning: false
adf_table <- data.frame(p_value = numeric(length(adf_test)))

for (i in 1:length(adf_test)) {adf_table[i, "p_value"] = round(adf_test[[i]]$p.value,3)
}

rownames(adf_table) <- variable_names

colnames(adf_table)<- c("P-value")
knitr::kable(adf_table)
```

Below is the ADF test result for all non-stationary data taking the
first difference. All variables in the first differences are stationary
as the null hypothesis of non-stationary can be rejected.

```{r}
#| echo: false
#| message: false
#| warning: false
#take the first difference
nonstationary_merged_data <- subset(merged_data, select = -c(cashrate_au))

dff_merged_data <- na.omit(nonstationary_merged_data - lag(nonstationary_merged_data))
```

```{r}
#| echo: false
#| message: false
#| warning: false
# ADF test
dff_adf_test <- list()
for (i in 1:12) {
  dff_adf_result = adf.test(dff_merged_data[,i], k = 4)
  dff_adf_test[[i]] <- dff_adf_result
}

# View the ADF test results
dff_adf_table <- data.frame(p_value = numeric(length(dff_adf_test)))

# Fill in the data frame with the test results
for (i in 1:length(dff_adf_test)) {
  dff_adf_table[i, "p_value"] = round(dff_adf_test[[i]]$p.value,3)

}
rownames(dff_adf_table) <- variable_names[-4]


colnames(dff_adf_table)<- c("P-value")
knitr::kable(dff_adf_table)
```

It can be concluded that all variables are integrated at 1 at the 5% significance level of the ADF test, with the conclusion folding for $crate\_au_{t}$ only at 1% significance level. Appropriate prior distributions will be used to accommodate this fact for the VAR model.

# Model and Hypotheses

In this research, the VAR(p) model will be applied to forecast the AUD/USD exchange rate, below is the basic model that is used in this research.

#### The basic VAR(p) model

```{=tex}
\begin{aligned}
 y_{t}=\mu_{0}+A_{1}y_{t-1}+\cdots+A_{p}y_{t-p}+\epsilon_{t} \\
\epsilon_{t}|Y_{t-1} \sim iid\mathcal{N}(0_{13},\Sigma)
\end{aligned}
```
For time $t$ = 1,2,.....,$T$：

-   $y_t$ is a $N(13)\times 1$ vector of observations at time $t$
-   $\mu_0$ is a $N(13)\times 1$ vector of constant terms
-   $A_i$ is a $N(13)\times N(13)$ matrix of autoregressive slope parameters
-   $\epsilon_t$ is a $N(13)\times 1$ vector of error terms and follows a multivariate white noise process
-   $Y_{t-1}$ is the information set collecting observations on y up to time $t-1$
-   $\Sigma$ is a $N(13)\times N(13)$ covariance matrix of the error term

```{=tex}
\begin{aligned}
y_{t}=\begin{pmatrix}
erate_{t}\\
crate\_au_{t} \\
rgdp\_au_{t}\\
cpi\_au_{t} \\
unemr\_au_{t}\\
impor\_au_{t} \\
expor\_au_{t} \\
crate\_us_{t} \\
rgdp\_us_{t} \\
cpi\_us_{t} \\
unemr\_us_{t} \\
impor\_us_{t}\\
expor\_us_{t}\\

\end{pmatrix}

\end{aligned}
```
For further research, we may use the predictive density function like 3-year-ahead forecast and forecast with Bayesian VARS.

# Modelling Framework

## The basic model

```{=tex}
\begin{align}
Y &=XA+U\\
U|X&\sim \mathcal{MN}_{T\times N}(0, \Sigma, I_T)\\
Y|X,A,\Sigma&\sim \mathcal{MN}_{T\times N} (XA, \Sigma, I_T)
\end{align}
```
Where $Y$ is a $T\times 13$ Matrix, $X$ is a $T\times(1+p\times13)$, $A$ is a $(1+p\times13)\times13$ matrix that contains $\mu_{0}$ and vectors of the autoregressive slope parameters and $U$ is a $T\times13$ matrix contains vetors of error terms.

The kernel of the likelihood function:

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}}exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\}
\end{align}
```
The basic model is based on **Natural-conjugate prior distribution**, where the $A$ follows a Matrix-variate Normal distribution and $\Sigma$ follows an Inverse Wishart distribution.

```{=tex}
\begin{align}
p(A,\Sigma) &= p(A|\Sigma)p(\Sigma) \\
A|\Sigma &\sim \mathcal{MN}_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\Sigma &\sim \mathcal{IW}_N(\underline{S},\underline{\nu})
\end{align}
```
The **Minnesota prior** is typically a good choice for specifying priors in BVAR model especially when the model involves many macroeconomic variables. It assumes the variables follow a random walk and it is suitable for unit root non-stationary variables such as in our case, variables are integrated at 1 at the 5% significance level of the ADF test, where:

```{=tex}
\begin{align}
\underline{A} &= \begin{bmatrix}0_{N \times 1} & I_{N} & 0_{N \times (p-1)N}\end{bmatrix}'\\

\underline{V} &= diag( \begin{bmatrix} \kappa_2 & \kappa_1(p^{-2}\otimes I_N') \end{bmatrix})

\end{align}
```
The prior mean $\underline{A}$ for the first lag of each variable (the identity matrix portion) is one, while all other coefficients including intercepts, are zeroes. For the column-specific prior covariance
$\underline{V}$, two shrinkage hyper-parameters $\kappa_1$ and $\kappa_2$ represent the overall shrinkage level for slopes and constant terms respectively.

For **posterior distribution**, the kernel of the posterior distribution takes the form of the product of the likelihood and the prior distributions.

```{=tex}
\begin{align*}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma|Y,X)p(A,\Sigma) \\
&= L(A,\Sigma|Y,X)p(A|\Sigma)p(\Sigma)
\end{align*}
```
```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\} \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A}) \underline{V}^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\}
\end{align}
```
The kernel can be represent as the normal-inverse Wishart distribution and we can get the following **full conditional joint posterior distribution:**

```{=tex}
\begin{align}
p(A|Y,X,\Sigma) &= \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
p(\Sigma|Y,X) &= \mathcal{IW}_N(\bar{S},\bar{\nu}) \\
\\
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A} \\

\end{align}
```

```{r function based on basic model}
#| echo: true
#| message: false
#| warning: false
## Posterior sample draw
posterior.draws       = function (S, Y, X, A.prior, V.prior, S.prior, nu.prior){
  
    # normal-inverse Wishard posterior parameters
    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))
    V.bar             = solve(V.bar.inv)
    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar            = nrow(Y) + nu.prior
    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
  
    # posterior draws 
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }
 
    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}
```

### Estimation Outcomes on Basic Model
```{r}
#| echo: false
#| message: false
#| warning: false
############ basic model parameter display

## Present data X, Y
y             = ts(merged_data[,1:ncol(merged_data)])
Y             = ts(y[5:nrow(y),], frequency=4)
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])
}

## Pre-setup 
N             = ncol(Y)
p             = frequency(Y)
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution specification - Minnesota prior 
kappa.1       = 0.02^2                                   
kappa.2       = 100                                   
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2,1]  = 1
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+1

S=10000
```

```{r}
#| echo: false
#| message: false
#| warning: false
posterior.sample.draws = posterior.draws(S=S, Y=Y, X=X,A.prior, V.prior, S.prior, nu.prior)
```

```{r}
#| echo: true
#| message: false
#| warning: false
head(round(apply(posterior.sample.draws$A.posterior, 1:2, mean),6))         # posterior draw A
head(round(apply(posterior.sample.draws$Sigma.posterior, 1:2, mean),6))     # posterior draw sigma
```

```{r}
#| echo: true
#| message: false
#| warning: false
par(mfrow=c(2,2), mar=c(4,4,2,2))

plot.ts(posterior.sample.draws$A.posterior[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate", col = mcxs1)
hist(posterior.sample.draws$A.posterior[1,1,], xlab = "AUD/USD exchange rate", col = mcxs1, main = '')

plot.ts(posterior.sample.draws$Sigma.posterior[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate sigma", col = mcxs2)
hist(posterior.sample.draws$Sigma.posterior[1,1,], xlab = "AUD/USD exchange rate sigma", col = mcxs2, main = '')
```
The trace plots and the histogram plots of posterior draws of$A$ and $\Sigma$ is presented above with the trace plots shows significant volatility.

## The Extended Model

The extended model will be built based on the the change in distribution of the error to **Laplace distribution** instead of the normally distributed errors assumption. The Laplace distribution is suitable for describing financial anomalies due to its sharp peaks and thick tails and the use of this distribution improves the robustness of the model to anomalies and is particularly suitable for financial time series. As our variables are most financial time series data, a Laplace distribution is more suitable to apply to our error term. The following plot gives a general idea of how laplace distribution behaves different with the normal distribution under the same mean and variance.
```{r}
#| echo: true
#| message: false
#| warning: false
x <- seq(-10, 10, length.out = 1000)
mean <- 0 

normal_pdf <- dnorm(x, mean = mean, sd = 2)

b_equal_variance <- sqrt(2)
laplace_pdf_equal_variance <- dexp(abs(x - mean) / b_equal_variance) / (2 * b_equal_variance)

df <- data.frame(x, Normal = normal_pdf, Laplace = laplace_pdf_equal_variance)

plot <- ggplot(df, aes(x)) +
  geom_line(aes(y = Normal), color = mcxs1, linewidth = 1) +
  geom_line(aes(y = Laplace), color = mcxs2, linewidth = 1) +
  labs(title = "Comparison of Normal and Laplace Distributions with Same Mean and Variance",
       x = "Value", y = "Probability Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

print(plot)
```

Following [Eltoft,Kim, and Lee 2006b](https://ieeexplore.ieee.org/document/1618702), for covariance with a general Kronecker structure, if each ${\lambda_t}$ has an independent exponential distribution with mean ${\alpha}$, then marginally ${U_t}$ has a multivariate Laplace distribution with mean vector 0 and covariance matrix ${\alpha\Sigma}$.

```{=tex}
\begin{align}

vec(U)&\sim N(0,\Sigma\otimes \Omega)\\

U_t &\sim \text{Laplace}(0, \alpha\Sigma) \\

U_t | \lambda_t &\sim \mathcal{MN}(0, \Sigma, \Omega) \\

\lambda_t &\sim \text{Exponential}(\frac{1}{\alpha})
\end{align}
```

The model is where $\Omega=diag(\lambda_1,\lambda_2,...,\lambda_t)$, each lambda independently drawn from an exponential distribution.

The kernel of the likelihood function now is :

```{=tex}
\begin{align}
L(A,\Sigma,\Omega |Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1} \lambda_t^{-\frac{N}{2}} exp({-\frac{1}{2}}\sum^{T}_{t =1}{\frac{1}{\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t)\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1}( \lambda_t^{-\frac{N}{2}} exp({-\frac{1}{2}}{\frac{1}{\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t))
\end{align}
```

Thence, for each time t, we have the likelihood like: 

```{=tex}
\begin{align}
&=\lambda_t^{-\frac{N}{2}} exp({-\frac{1}{2}}{\frac{1}{\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t)
\end{align}
```

For posteriors distribution, $A$, $\Sigma$ and $\lambda_t$ can then be derived using the likelihood and the prior distributions as follows:

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma,\lambda_t|Y,X)p(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\} \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\} \\
&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2} tr[\Sigma^{-1}(Y'\Omega^{-1}Y - 2A'X'\Omega)^{-1}Y + A'X'\Omega^{-1}XA \\
&+ A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})]\}

\end{align}
```

The kernel can be rearranged in the form of the **Matrix-variate normal-inverse Wishart distribution**. 

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\sim MNIW(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
&\\
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S} - \bar{A}'\bar{V}^{-1}\bar{A}
\end{align}
```

The prior distribution of
$$\lambda_t \sim \text{Exponential}(\frac{1}{\alpha})$$ define as:

```{=tex}
\begin{align}
p(\lambda_t|\alpha) &= \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\

\end{align} 
```

The kernel of the fully conditional posterior distribution of $\lambda_t$ is then derived as follows:

```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda_t|Y,X)p(\lambda_t) \\
\\
&\propto \lambda_t^{-\frac{N}{2}}exp({-\frac{1}{2}}{\frac{1}{\lambda_t}}\epsilon_t' \Sigma^{-1}\epsilon_t) \\
&\times \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\


&= \lambda_t^{-\frac{N}{2}+1-1} exp\{-\frac{1}{2}[\frac{
\epsilon_t' \Sigma^{-1}\epsilon_t} {\lambda_t} +\frac{2}{\alpha}\lambda_t]\} 
\end{align}
```

The above expression can be rearranged in the form of a Generalized
inverse Gaussian distribution kernel as follows:

```{=tex}
\begin{align}
\lambda_t|Y,A,\Sigma &\sim GIG(a,b,p) \\
\\
a &=\frac{2}{\alpha} \\
b &= \epsilon_t' \Sigma^{-1}\epsilon_t \\
p &= -\frac{N}{2}+1
\end{align}
```

The Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:
  
1. Draw $\Sigma^{(s)}$ from the $IW(\bar{S},\bar{\nu})$ distribution.
2. Draw $A^{(s)}$ from the $MN(\bar{A},\Sigma^{(s)}, \bar{V})$ distribution.
3. Draw $\lambda_t^{(s)}$ from $GIG(a,b,p)$.

Repeat steps 1, step 2 and 3 for $S_1$+$S_2$times.

Discard the first draws that allowed the algorithm to converge to the stationary posterior distribution.

Output is $\left\{ {A^{(s)}, \Sigma^{(s)}}, \lambda_t^{(s)}\right\}^{S_1+S_2}_{s=S_1+1}$.

```{r}
#| echo: true
#| message: false
#| warning: false
posterior.draws.extended <- function(S1,total_S,Y, X, A.prior, V.prior, S.prior, nu.prior, lambda.priors){

  Sigma.posterior.draws = array(NA, c(N,N,total_S))
  A.posterior.draws = array(NA, c((1+p*N),N,total_S))
  lambda.posterior.draws = array(NA,c(T,total_S+1))

  for (s in 1:total_S){
    
    if (s == 1) {
      lambda.s = lambda.0
    } else {
      lambda.s    = lambda.posterior.draws[,s]
    }
    
    Omega.inv = diag(1/lambda.s)

    V.bar.inv.ext   = t(X)%*%Omega.inv%*%X + V.prior.inv
    V.bar.inv.ext   = 0.5 * (t(V.bar.inv.ext) + V.bar.inv.ext)
    V.bar.ext       = solve(V.bar.inv.ext)
    V.bar.ext       = 0.5 * (t(V.bar.ext) + V.bar.ext)
    A.bar.ext       = V.bar.ext%*%(t(X)%*%Omega.inv%*%Y + V.prior.inv%*%A.prior)
    nu.bar.ext      = T + nu.prior
    S.bar.ext       = S.prior + t(Y)%*%Omega.inv%*%Y + t(A.prior)%*%V.prior.inv%*%A.prior - t(A.bar.ext)%*%V.bar.inv.ext%*%A.bar.ext
    S.bar.ext       = 0.5 * (t(S.bar.ext) + S.bar.ext)
    S.bar.ext.inv   = solve(S.bar.ext)
    S.bar.ext.inv   = 0.5 * (t(S.bar.ext.inv) + S.bar.ext.inv)
    
    Sigma.inv.draw = rWishart(1, df = nu.bar.ext, Sigma = S.bar.ext.inv)[,,1]
    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
    
    
    u.t = Y-X%*%A.posterior.draws[,,s]
    #    ---- loop lambda posterior ----   #
    c                      = -N/2 + 1          # N=13
    a                      = 2 / lambda.priors$alpha
    for (x in 1:T){
      b                  = t((u.t)[x,])%*% Sigma.inv.draw %*%(u.t)[x,]
      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)
    } # END x loop
  } # END s loop
  
  
  output                 = (list(A.posterior.exten = A.posterior.draws[,,S1:total_S], 
                                 Sigma.posterior.exten = Sigma.posterior.draws[,,S1:total_S], 
                                 lambda.posterior.exten = lambda.posterior.draws[,(S1+1):(total_S+1)]))
  
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
## Present data X, Y
  y             = ts(merged_data[,1:ncol(merged_data)])
  Y             = ts(y[5:nrow(y),], frequency=4)
  X             = matrix(1,nrow(Y),1)
  for (i in 1:frequency(Y)){
    X           = cbind(X,y[5:nrow(y)-i,])
  }

  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  N = ncol(Y)
  K = ncol(X)
  T = nrow(Y)
  p = (K - 1) / N
  
  kappa.1   = 0.02^2
  kappa.2   = 100
  
  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  V.prior.inv = diag(1/diag(V.prior))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  alpha <- 1
  lambda.0 <- rexp(T, rate = 1/alpha)
  lambda.priors = list(alpha = alpha)
 ################### test  need to delete
  S1= 500
  S2= 9500
  total_S = S1+S2
```

### Estimation Outcomes on Extended Model
```{r}
#| echo: false
#| message: false
#| warning: false
## Applying function
posterior.ext = posterior.draws.extended(S1=S1,total_S = total_S, Y=Y, X=X, A.prior, V.prior, S.prior, nu.prior, lambda.priors)
```

```{r}
#| echo: true
#| message: false
#| warning: false
head(round(apply(posterior.ext$A.posterior.exten, 1:2, mean),6))
head(round(apply(posterior.ext$Sigma.posterior.exten, 1:2, mean),6))
round(mean(posterior.ext$lambda.posterior.exten),6)
```

```{r}
#| echo: true
#| message: false
#| warning: false
par(mfrow=c(3,2), mar=c(2,2,2,2))
plot.ts(posterior.ext$A.posterior.exten[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate.exten", col = mcxs1)   
hist(posterior.ext$A.posterior.exten[1,1,], xlab = "AUD/USD exchange rate.exten", col = mcxs1, main = '')
plot.ts(posterior.ext$Sigma.posterior.exten[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate sigma.exten", col = mcxs2) 
hist(posterior.ext$Sigma.posterior.exten[1,1,], xlab = "AUD/USD exchange sigma.exten", col = mcxs2, main = '')

plot.ts(posterior.ext$lambda.posterior.exten[(S1+1):total_S], xlab = "Simulation times S", ylab = "lambda", col = mcxs3)         
hist(posterior.ext$lambda.posterior.exten[(S1+1):total_S], xlab = "lambda", col = mcxs3, main = '')
```
Compared to the basic model, by applying laplace distribution into the variance specification of the error term, we introduces more variability to our $A$ and $\Sigma$. That is, the distribution of the coefficient now more dispersion with the basic model more concentrate at 0. And for the distribution of Sigma, it shows the similar shape but central around different value.

The lambda mainly concentrates between 0 to 4, with some large outlines throughout the simulation.

## The Basic Model With Stochastic Volatility Heteroskedasticity
The second modification enhances the model by capturing stochastic volatility in the error terms, which means the variance might vary over time according to a stochastic process. This adjustment allows the model to specifically account for heteroskedasticity, creating a more dynamic error structure. By integrating this feature with the earlier enhancement that introduced laplace errors, the model becomes significantly more robust in handling error variability.

With the stochastic volatility in innovation term, the basic model now is:
```{=tex}
\begin{align}
Y &= X A + U
\\
U |X &\sim \mathcal{MN}_{T\times 13}(0,\Sigma,\mathrm{diag}(\sigma_{T}^{2}))
\end{align} 
```

With $\mathrm{diag}(\sigma_{T}^{2})=diag(e^{h_1},e^{h_2},...,e^{h_T})$, represent as:

```{=tex}
\left(
\begin{array}{cccc}
e^{h_1} & 0 & \cdots & 0 \\
0 & e^{h_2} & \cdots & \vdots \\
0 & \cdots & \ddots & \vdots \\
0 & \cdots & 0 &  e^{h_T}\\
\end{array}
\right)
```

Where $h_t$ follows an AR(1) process:
```{=tex}
\begin{align}
h_t &= h_{t-1} + \sigma_v v_t \\
v_t &\sim \mathcal{N}(0,1) \\
\sigma^2_v &\sim IG2(\bar{s},\bar{\nu})
\end{align}
```

The basic heteroskedasticity model is still based on Natural-conjugate prior distribution, where the $A$ follows a Matrix-variate Normal distribution and $\Sigma$ follows an Inverse Wishart distribution.

With the likelihood function now changes to :
```{=tex}
\begin{align} 
L\left( {A},{\Sigma}|Y,X \right) &\propto \text{det}({\Sigma})^{-\frac{T}{2}}\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}(Y-X{A})'(Y-X{A}) \right] \right\}\\
&=\text{det}({\Sigma})^{-\frac{T}{2}}\\
&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}({A}-\widehat{A})'X' \ \mathrm{diag}(\sigma_{T}^{2})^{-1} \ X({A}-\widehat{A}) \right] \right\}\\
&\quad\times \exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}(Y-X\widehat{A})' \ \mathrm{diag}(\sigma_{T}^{2})^{-1} \ (Y-X\widehat{A}) \right] \right\}
\end{align} 
```

For posterior distribution,

```{=tex}
\begin{align} 
p\left( {A},{\Sigma} |Y,X\right) 
&\propto  \text{det}({\Sigma})^{-\frac{T}{2}}\\
&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}({A}-\widehat{A})'X' \ \mathrm{diag}(\sigma_{T}^{2})^{-1} \ X({A}-\widehat{A}) \right] \right\}\\
&\quad\times \exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}(Y-X\widehat{A})' \ \mathrm{diag}(\sigma_{T}^{2})^{-1} \ (Y-X\widehat{A}) \right] \right\}\\
& \quad\times\text{det}({\Sigma})^{-\frac{N+K+\underline{\nu}+1}{2}}\\
&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}({A}-\underline{A})'\underline{V}^{-1}({A}-\underline{A}) \right] \right\}\\
&\quad\times \exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}\underline{S} \right] \right\} \\

\end{align}
```

Similarly, the kernel can be rearranged in the form of the Matrix-variate normal-inverse Wishart distribution.

```{=tex}
\begin{align}
\bar{V} &= (X'\mathrm{diag}(\sigma_{T}^{2})^{-1}X + \underline{V}^{-1})^{-1}\\
\bar{A} &= \bar{V}(X'\mathrm{diag}(\sigma_{T}^{2})^{-1}Y+\underline{V}^{-1}\underline{A})\\
\bar{\nu} &= T+\underline{\nu}\\
\bar{S} &= Y'\mathrm{diag}(\sigma_{T}^{2})^{-1}Y+\underline{A}'\underline{V}^{-1}\underline{A}-\bar{A'}\bar{V}^{-1}\bar{A}+\underline{S}
\end{align}
```

### Estimation Outcomes on Basic Stochastic Volatility Model
```{r}
posterior.draws.hetero = function (S, Y, X, A.prior, V.prior, S.prior, nu.prior){
  # aux is a list containing:
  #   Y - a TxN matrix
  #   X - a TxK matrix
  #   H - a Tx1 matrix
  #   h0 - a scalar
  #   sigma.v2 - a scalar
  #   s - a Tx1 matrix
  #   A - a KxN matrix
  #   Sigma - an NxN matrix
  #   sigma2 - a Tx1 matrix
  #
  # priors is a list containing:
  #   h0.v - a positive scalar
  #   h0.m - a scalar
  #   sigmav.s - a positive scalar
  #   sigmav.nu - a positive scalar
  #   HH - a TxT matrix
  
  T             = dim(Y)[1]
  N             = dim(Y)[2]
  K             = dim(X)[2] 
  p             = frequency(Y)

  H             = diag(T)
  sdiag(H,-1)   = -1
  HH            = 2*diag(T)
  sdiag(HH,-1)  = -1
  sdiag(HH,1)   = -1
  
  HH                = HH
  h0.m              = 0
  h0.v              = 1
  sigmav.s          = 1
  sigmav.nu         = 1

  posteriors    = list(                        
    H           = matrix(NA,T,S),
    sigma2      = matrix(NA,T,S),
    s           = matrix(NA,T,S),
    h0          = rep(NA,S),
    sigma.v2    = rep(NA,S),
    A           = array(NA, c((1 + N * p), N, S)),
    Sigma       = array(NA, c(N, N, S))
  )
  
  aux        = list(
    Y        = Y,
    X        = X,
    H        = matrix(1, T, 1),
    h0       = 0,
    sigma.v2 = 1,
    s        = matrix(1, T, 1),
    A        = matrix(0, K, N),
    Sigma    = matrix(0, N, N),             
    sigma2   = matrix(1, T, 1)
  )
  
  for (s in 1:S){
      # normal-inverse Wishard posterior 
      V.bar.inv              = t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$X + diag(1/ diag(V.prior))
      V.bar                  = solve(V.bar.inv)
      A.bar                  = V.bar%*%(t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y + diag(1/diag( V.prior))%*%A.prior) 
      nu.bar                 = T + nu.prior
      S.bar                  = S.prior + t(aux$Y)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar 
      S.bar.inv              = solve(S.bar)
      S.bar.inv              = 0.5 * (t(S.bar.inv) + S.bar.inv) # positive-definite
    
      # posterior draws for A and Sigma
      Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
      Sigma.posterior.draw   = apply(Sigma.posterior.IW, 3 ,solve)
      aux$Sigma              = array(Sigma.posterior.draw,c(N,N,1))
      A.norm                 = array(rnorm(prod(c(K,N,1))),c(K,N,1))
      L                      = t(chol(V.bar))
      aux$A                  = A.bar + L%*%A.norm[,,1]%*%chol(aux$Sigma[,,1])
  
      # posterior draw for sigma2   
      N             = dim(aux$Y)[2] # 10
      alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
      sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
      pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
      
      Lambda        = solve(chol(aux$Sigma[,,1]))
      Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
      Y.tilde       = as.vector(log((Z + 0.0000001)^2))
      Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
        
      # sampling initial condition
      ############################################################
      V.h0.bar      = 1/((1 / h0.v) + (1 / aux$sigma.v2))
      m.h0.bar      = V.h0.bar*((h0.m / h0.v) + (aux$H[1] / aux$sigma.v2))
      h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
      aux$h0        = h0.draw
      
      # sampling sigma.v2
      ############################################################
      sigma.v2.s    = sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
      sigma.v2.draw = sigma.v2.s / rchisq(1, sigmav.nu + T)
      aux$sigma.v2  = sigma.v2.draw
      
      # sampling auxiliary states
      ############################################################
      Pr.tmp        = simplify2array(lapply(1:10,function(x){
        dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
      }))
      Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
      s.cum         = t(apply(Pr, 1, cumsum))
      r             = matrix(rep(runif(T), 10), ncol = 10)
      ss            = apply(s.cum < r, 1, sum) + 1
      aux$s         = as.matrix(ss)
      
      # sampling log-volatilities using functions for tridiagonal precision matrix
      ############################################################
      Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
      D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * HH
      b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
      lead.diag     = diag(D.inv)
      sub.diag      = mgcv::sdiag(D.inv, -1)
      D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
      D.L           = diag(D.chol$ld)
      mgcv::sdiag(D.L,-1) = D.chol$sd
      x             = as.matrix(rnorm(T))
      a             = forwardsolve(D.L, b)
      draw          = backsolve(t(D.L), a + x)
      aux$H         = as.matrix(draw)
      aux$sigma2    = as.matrix(exp(draw))
        
      # output list
      posteriors$H[,s]             = aux$H
      posteriors$sigma2[,s]        = aux$sigma2
      posteriors$s[,s]             = aux$s
      posteriors$h0[s]             = aux$h0
      posteriors$sigma.v2[s]       = aux$sigma.v2
      posteriors$A[,,s]            = aux$A
      posteriors$Sigma[,,s]        = aux$Sigma
      
  }
  return(posteriors)
}
```


```{r}
#| echo: false
#| message: false
#| warning: false
  y             = ts(merged_data[,1:ncol(merged_data)])
  Y             = ts(y[5:nrow(y),], frequency=4)
  X             = matrix(1,nrow(Y),1)
  for (i in 1:frequency(Y)){
    X           = cbind(X,y[5:nrow(y)-i,])
  }

  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  N = ncol(Y)
  K = ncol(X)
  T = nrow(Y)
  p = (K - 1) / N

  kappa.1   = 0.02^2
  kappa.2   = 100
  
  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  V.prior.inv = diag(1/diag(V.prior))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+2

  S = 10000
  
posterior.heter = posterior.draws.hetero(S = S, Y = Y, X = X, A.prior, V.prior, S.prior, nu.prior)
```

```{r}
#| echo: false
#| message: false
#| warning: false
A.posterior.heter       = posterior.heter[["A"]]
Sigma.posterior.heter   = posterior.heter[["Sigma"]]
```

```{r}
#| echo: true
#| message: false
#| warning: false
head(round(apply(A.posterior.heter, 1:2, mean),6))         # posterior draw A
head(round(apply(Sigma.posterior.heter, 1:2, mean),6))     # posterior draw sigma
```

```{r}
#| echo: true
#| message: false
#| warning: false
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior.heter[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate", col = mcxs1)
hist(A.posterior.heter[1,1,], xlab = "AUD/USD exchange rate", col = mcxs1, main = '')

plot.ts(Sigma.posterior.heter[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate sigma", col = mcxs2)
hist(Sigma.posterior.heter[1,1,], xlab = "AUD/USD exchange rate sigma", col = mcxs2, main = '')
```
We can notice that compared to the above two models, the involve of the heteroskedasticity brings out outliers, and these outliers seems all around the zero, with the coefficient also central round 1 and the sigma more central around zero. The change is obvious and significant, which will be demonstrated in the following forecasting plot.

## The Extension Model With Stochastic Volatility Heteroskedasticity

With stochastic volatility heteroskedasticity in our extended model, we have:
$\Omega=diag(e^{h_1}\lambda_1,e^{h_2}\lambda_2,...,e^{h_T}\lambda_t)$, represents as:

$$\\\Omega = \left(
\begin{array}{cccc}
e^{h_1}\lambda_1 & 0 & \cdots & 0 \\
0 & e^{h_2}\lambda_2 & \cdots & \vdots \\
0 & \cdots & \ddots & \vdots \\
0 & \cdots & 0 &  e^{h_T}\lambda_T\\
\end{array}
\right)$$

The likelihood function now change to:

```{=tex}
\begin{align}
L(A,\Sigma,\Omega |Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1} (e^{h_t}\lambda_t)^{-\frac{N}{2}} exp({-\frac{1}{2}}\sum^{T}_{t =1}{\frac{1}{e^{h_t}\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t)\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1}( (e^{h_t}\lambda_t)^{-\frac{N}{2}} exp({-\frac{1}{2}}{\frac{1}{e^{h_t}\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t))
\end{align}
```

For posterior distribution,

```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda_t|Y,X)p(\lambda_t) \\
\\
&\propto (e^{h_t}\lambda_t)^{-\frac{N}{2}} exp({-\frac{1}{2}}{\frac{1}{e^{h_t}\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t) \\
&\times \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\


&= \lambda_t^{-\frac{N}{2}+1-1} exp\{-\frac{1}{2}[\frac{
\epsilon_t'e^{-h_t}\Sigma^{-1}\epsilon_t} {\lambda_t} +\frac{2}{\alpha}\lambda_t]\} 
\end{align}
```

```{=tex}
\begin{align}
\lambda_t|Y,A,\Sigma &\sim GIG(a,b,p) \\
\\
a &=\frac{2}{\alpha} \\
b &= \epsilon_t' e^{-h_t}\Sigma^{-1}\epsilon_t \\
p &= -\frac{N}{2}+1
\end{align}
```

Gibbs Sampling Routine for SV and Laplace Distributed Errors

Initialize $\lambda^{(0)}$ and $h_t^{(0)}$.

At each iteration s:

1. Draw $\Sigma^{(s)}$ from the $IW(\bar{S},\bar{\nu})$ distribution
2. Draw $A^{(s)}$ from the $MN(\bar{A},\Sigma^{(s)}, \bar{V})$ distribution
3. Draw $\lambda_t^{(s)}$ from $GIG(a,b,p)$
4. Draw $h^{(s)}$ from the SV sampling routine described above

```{r}
#| echo: false
#| message: false
#| warning: false
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

S = 10000
N = ncol(Y)
T = nrow(Y)
K = 1 + (p*N)
N = ncol(Y)
kappa.1   = 0.02^2
kappa.2   = 100
K = 1 + (p*N)

A.prior     = matrix(0, K , N)
A.prior[2:(N+1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

alpha <- 1
lambda.0 <- rexp(T, rate = 1/alpha)
lambda.priors = list(alpha = alpha)

H                 = diag(T)
sdiag(H,-1)       = -1
HH                = 2*diag(T)
sdiag(HH,-1)      = -1
sdiag(HH,1)       = -1

priors = list(
A.prior     = A.prior,
V.prior     = V.prior,
S.prior     = S.prior,
nu.prior    = nu.prior,

h0.v        = 1,
h0.m        = 0,
sigmav.s    = 1,
sigmav.nu   = 1, 
HH          = HH 
)
```

```{r}
#| echo: false
#| message: false
#| warning: false
SVcommon.Gibbs.iteration = function(aux, priors){
  T             = dim(aux$Y)[1]
  N             = dim(aux$Y)[2]
  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
  
  Lambda        = solve(chol(aux$Sigma))
  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
  Y.tilde       = as.vector(log((Z + 0.0000001)^2))
  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
  
  # sampling initial condition
  ############################################################
  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))
  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))
  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
  aux$h0        = h0.draw
  
  # sampling sigma.v2
  ############################################################
  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)
  aux$sigma.v2  = sigma.v2.draw
  
  # sampling auxiliary states
  ############################################################
  Pr.tmp        = simplify2array(lapply(1:10,function(x){
    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
  }))
  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
  s.cum         = t(apply(Pr, 1, cumsum))
  r             = matrix(rep(runif(T), 10), ncol = 10)
  ss            = apply(s.cum < r, 1, sum) + 1
  aux$s         = as.matrix(ss)
  
  # sampling log-volatilities using functions for tridiagonal precision matrix
  ############################################################
  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH
  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
  lead.diag     = diag(D.inv)
  sub.diag      = mgcv::sdiag(D.inv, -1)
  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
  D.L           = diag(D.chol$ld)
  mgcv::sdiag(D.L,-1) = D.chol$sd
  x             = as.matrix(rnorm(T))
  a             = forwardsolve(D.L, b)
  draw          = backsolve(t(D.L), a + x)
  aux$H         = as.matrix(draw)
  aux$sigma2    = as.matrix(exp(draw))
  
  return(aux)
}
```

```{r}
posterior.draws.hetero.ext = function(Y,X,priors,S){
  alpha <- 1
  lambda.0 <- rexp(T, rate = 1/alpha)
  lambda.priors = list(alpha = alpha)
  
  Omega.inv = diag(1/lambda.0)

  aux <- list(
    Y = sqrt(Omega.inv)%*%Y, 
    X = sqrt(Omega.inv)%*%X,  
    H = matrix(1,T,1), 
    h0 = 0, 
    sigma.v2 = 1,
    s = matrix(1,T,1),
    A = matrix(0, K, N), 
    Sigma = diag(diag(matrix(1, N, N))),
    sigma2 = matrix(1, T, 1) 
  )
  
  A.posterior        = array(NA, dim = c(K,N,S))
  Sigma.posterior    = array(NA,dim=c(N,N,S))
  sigma2.posterior    = matrix(NA, nrow(Y), (S)) 
  lambda.posterior.draws = array(NA,c(T,S))
  
  for (s in 1:S){
    if (s == 1) {
      lambda.s = lambda.0
    } else {
      lambda.s    = lambda.posterior.draws[,s-1]
      
      Omega.inv = diag(1/lambda.s)
      
      aux <- list(
      Y = sqrt(Omega.inv)%*%Y, 
      X = sqrt(Omega.inv)%*%X,  
      H = matrix(1,T,1), 
      h0 = 0, 
      sigma.v2 = 1,
      s = matrix(1,T,1),
      A = matrix(0, K, N), 
      Sigma = diag(diag(matrix(1, N, N))),
      sigma2 = matrix(1, T, 1) 
  )
    }
    
    Omega.inv = diag(1/lambda.s)
    
    # normal-inverse Wishart posterior parameters
    V.bar.inv   = t(X)%*%diag(1/as.vector(aux$sigma2))%*%X + diag(1/diag(priors$V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%diag(1/as.vector(aux$sigma2))%*%Y + diag(1/diag(priors$V.prior))%*%priors$A.prior)
    nu.bar      = nrow(Y) + priors$nu.prior
    S.bar       = priors$S.prior + t(Y)%*%diag(1/as.vector(aux$sigma2))%*%Y + t(priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    
    #posterior draws
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    
    # Draw using stochastic volatility Gibbs common sampler
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    aux = SVcommon.Gibbs.iteration(aux, priors)
    sigma2.posterior[,s]  = aux$sigma2
    

     # posterior draws for lambda
      u.t = Y-X%*%A.posterior[,,s]
      c                      = -N/2 + 1       
      a                      = 2 / lambda.priors$alpha
    for (x in 1:T){
      b                      = t((u.t)[x,])%*%Sigma.posterior[,,s]%*%(u.t)[x,]
      
      lambda.posterior.draws[x,s] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)
    } 
    
    
    
    
  }
  
  posterior.sv = list(
    Sigma.posterior   = Sigma.posterior,
    A.posterior       = A.posterior
  )
  return(posterior.sv)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
posterior.draws.heter.ext = posterior.draws.hetero.ext(Y=Y, X=X, priors=priors, S=S)
```

```{r}
#| echo: true
#| message: false
#| warning: false
head(round(apply(posterior.draws.heter.ext$Sigma.posterior, 1:2, mean),6))
head(round(apply(posterior.draws.heter.ext$A.posterior, 1:2, mean),6))
```

```{r}
#| echo: true
#| message: false
#| warning: false
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(posterior.draws.heter.ext$A.posterior[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate", col = mcxs1)
hist(posterior.draws.heter.ext$A.posterior[1,1,], xlab = "AUD/USD exchange rate", col = mcxs1, main = '')

plot.ts(posterior.draws.heter.ext$Sigma.posterior[1,1,], xlab = "Simulation times S", ylab = "AUD/USD exchange rate sigma", col = mcxs2)
hist(posterior.draws.heter.ext$Sigma.posterior[1,1,], xlab = "AUD/USD exchange rate sigma", col = mcxs2, main = '')
```
For extended model with heteroskedasticity, the coefficient range now wider a lot, which ranges from -3 to 3.

## Proof of Model Validity

### Proof of Basic Model Validity

To test the model validity, we simulated 1000 observations from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2 to see how the autoregressive and the covariance matrices and the posterior mean of the constant term behave.

```{r}
#| echo: false
#| message: false
#| warning: false
set.seed(123)
n <- 1000  # Number of observations
mu <- 0    # Mean
sigma <- 1 # Standard deviation

# Simulate two independent random walks
simulation_data <- data.frame(RW1 = cumsum(rnorm(n, mu, sigma)),RW2 = cumsum(rnorm(n, mu, sigma)))

plot(simulation_data$RW1, type = 'l', ylim = range(simulation_data), col = "#e74c3c", ylab = 'Value', xlab = 'Time', main = 'Bivariate Random Walk')
lines(simulation_data$RW2,col = "#2ecc71")
legend("topright",legend = c("RW1", "RW2"), col = c("#e74c3c", "#2ecc71"), lty = 1, cex = 0.6)
```

```{r}
#| echo: false
#| message: false
#| warning: false

#################################  basic model prof

# simulation data generating process
p=1

Y_simulation = (simulation_data[(p+1):nrow(simulation_data),]) 
X_simulation = matrix(1,nrow(Y_simulation),1)                                   

for (i in 1:p){
  X_simulation     = cbind(X_simulation, (simulation_data[(p+1):nrow(simulation_data)-i,]))
}

Y_simulation = as.matrix(Y_simulation)
X_simulation = as.matrix(X_simulation)

N           = ncol(Y_simulation)                          
p           = frequency(Y_simulation)
A.hat       = solve(t(X_simulation)%*%X_simulation)%*%t(X_simulation)%*%Y_simulation
Sigma.hat   = t(Y_simulation-X_simulation%*%A.hat)%*%(Y_simulation-X_simulation%*%A.hat)/nrow(Y_simulation)

# Minnesota prior
kappa.1             = 0.02^2                                    
kappa.2             = 100                                  
A.prior             = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior             = diag(diag(Sigma.hat))
nu.prior            = N+1
S = 10000

posterior.sample.draws = posterior.draws(S=S, Y=Y_simulation, X=X_simulation, A.prior, V.prior, S.prior, nu.prior) 
```

```{r}
Sigma_posterior_mean <- apply(posterior.sample.draws$Sigma.posterior, 1:2, mean)

Sigma_df <- as.data.frame(Sigma_posterior_mean)
colnames(Sigma_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(Sigma_df) <- c("Y1-Lag", "Y2-Lag")
knitr::kable(Sigma_df, caption = "Posterior mean of the covariance matrix Sigma")
```

```{r}
# Calculate posterior mean of autoregressive coefficients (including the constant term)
A_posterior_means <- apply(posterior.sample.draws$A.posterior, 1:2, mean)

A_df <- as.data.frame(A_posterior_means)
colnames(A_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(A_df) <- c("Constant", "Y1-Lag", "Y2-Lag")
knitr::kable(A_df, caption = "Posterior mean of the autoregressive coefficient matrix A")
```

The diagonal entries of the covariance matrix are close to 1, which indicates that each variable has a strong autoregressive relationship with itself and similarly, the diagonal elements of the autoregressive
coefficient matrix are close to one, suggesting that each variable is heavily influenced by its past value. Besides, the posterior means for the constant terms is close to 0, the above can indicate that the
estimated parameter constant term and means are consistent with what we expect given a Minnesota prior.

### Proof of Extended Model

```{r}
#| echo: false
#| message: false
#| warning: false

S1= 500
S2= 9500
total_S = S1+S2

  A.hat       = solve(t(X_simulation)%*%X_simulation)%*%t(X_simulation)%*%Y_simulation
  Sigma.hat   = t(Y_simulation-X_simulation%*%A.hat)%*%(Y_simulation-X_simulation%*%A.hat)/nrow(Y_simulation)
  
  N = ncol(Y_simulation)
  K = ncol(X_simulation)
  T = nrow(Y_simulation)
  p = (K - 1) / N
  
  kappa.1   = 0.02^2
  kappa.2   = 100
  
  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  V.prior.inv = diag(1/diag(V.prior))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  alpha <- 1
  lambda.0 <- rexp(T, rate = 1/alpha)
  lambda.priors = list(alpha = alpha)

posterior.extended = posterior.draws.extended(S1 = S1, total_S = total_S , Y=Y_simulation, X=X_simulation,  A.prior, V.prior, S.prior, nu.prior, lambda.priors)
```

```{r}
Sigma_posterior_mean <- apply(posterior.extended$Sigma.posterior.exten, 1:2, mean)

Sigma_df <- as.data.frame(Sigma_posterior_mean)
colnames(Sigma_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(Sigma_df) <- c("Y1-Lag", "Y2-Lag")
knitr::kable(Sigma_df, caption = "Posterior mean of the covariance matrix Sigma")
```

```{r}
A_posterior_means <- apply(posterior.extended$A.posterior.exten, 1:2, mean)

A_df <- as.data.frame(A_posterior_means)
colnames(A_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(A_df) <- c("Constant", "Y1-Lag", "Y2-Lag")
knitr::kable(A_df, caption = "Posterior mean of the autoregressive coefficient matrix A")
```
Similarly, the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and the posterior mean of the constant term is close to zero, so we can conclude that the extended model is also valid.

# Forecasting

The forecast will focus on two models with the sample period from 1990 Q1 to 2023Q4, and forecast how the exchange rate changes for the future up to 2026Q4.

## Forecasting With Models

### Forecasting With Basic Model
```{r}
#| echo: false
#| message: false
#| warning: false
############ basic model parameter display

## Present data X, Y
y             = ts(merged_data[,1:ncol(merged_data)])
Y             = ts(y[5:nrow(y),], frequency=4)
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])
}

## Pre-setup 
N             = ncol(Y)
p             = frequency(Y)
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution specification - Minnesota prior 
kappa.1       = 0.02^2                                   
kappa.2       = 100                                   
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2,1]  = 1
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+1

S=10000
```

```{r}
#| echo: false
#| message: false
#| warning: false
posterior.sample.draws = posterior.draws(S=S, Y=Y, X=X,A.prior, V.prior, S.prior, nu.prior)
```

```{r}
#| echo: false
#| message: false
#| warning: false
h                      = 12
S                      = 10000
Y.h                    = array(NA,c(h,N,S))

# ## Applying function 
# posterior.sample.draws = posterior.draws(S=S, Y=Y, X=X,A.prior, V.prior, S.prior, nu.prior)
A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior
```

```{r}
#| echo: false
#| message: false
#| warning: false
# sampling predictive density
sampling.predictive.density = function(A.posterior.simu, Sigma.posterior.simu, S, h) {
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,,s]
  x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
  x.Ti               = x.Ti[p:1,]
  for (i in 1:h){
    x.T               = c(1,as.vector(t(x.Ti)))
    Y.f               = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
    x.Ti            = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h[i,,s]         = Y.f[1:N]
    }
  }
   return(Y.h)
}
```


```{r}
#| echo: false
#| message: false
#| warning: false
Y.h       = sampling.predictive.density(A.posterior.simu = A.posterior.simu,
                                            Sigma.posterior.simu = Sigma.posterior.simu,
                                            S = S, h = h)
```

```{r}
#| echo: false
#| message: false
#| warning: false
basic.plot.2d.fun = function(Y.h){
limits.1    = range(Y.h[,1,])
point.f     = apply(Y.h[,1,],1,mean)
interval.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)

x           = seq(from=limits.1[1], to=limits.1[2], length.out=100)
z           = matrix(NA,h,99)
for (i in 1:h){
  z[i,]     = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
}
x           = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
yy          = 1:h
z           = t(z)

theta = 180
phi   = 15.5
f4    = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1, col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
polygon3D(x=c(interval.f[1,],interval.f[2,h:1]), y=c(1:h,h:1), z=rep(0,2*h), col = mcxs1.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE)
for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=0.5, col="black")
}
f4.l1 = trans3d(x=point.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=2, col=mcxs1)
}
#basic.plot.2d = basic.plot.2d.fun(Y.h)
```

```{r}
#| echo: false
#| message: false
#| warning: false
basic.plot.fun = function(Y.h){
  
ex_rate.point.f    = apply(Y.h[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)
ex_rate.range      = range(y[,1],ex_rate.interval.f)

true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value

# Define the range and labels for the y-axis
y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
y_max <- max(ex_rate.range[2], true_y_value)
num_labels <- 4  # You can adjust the number of labels

# Generate a sequence of y-values for the axis that includes true_y_value
y_values <- pretty(c(y_min, y_max), num_labels)
if(!true_y_value %in% y_values) {
  y_values <- sort(c(y_values))
}

# Generate labels for the y-axis, ensuring true_y_value is included and highlighted
y_labels <- sprintf("%.2f", y_values)
y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")

# Plot adjustments
plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
axis(2, at=y_values, labels=y_labels, col="black",las=1)

# Red dashed line for the true y-value
abline(h=true_y_value, col="red", lty=2, lwd=1)


text(x=nrow(y), y=9.65, srt=90, "2025-12")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
abline(v=nrow(y)+h, col=mcxs2)
legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.35)

polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)
}
basic.plot.fun.plot = basic.plot.fun(Y.h)
  
```
The above presents the historical and forecasting data the AUD/USD exchange rate for basic and extended model. 

For the point forecast, for basic model, the exchange rate shows several peaks and troughs, indicating the volatility in the exchange rate over the years. For the three years forecasting, it shows a slightly downward trend and reaching around 0.62 in 2026.

```{r}
#| echo: false
#| message: false
#| warning: false
basic.plot.3d.fun = function(Y.h){
limits.1    = range(Y.h[,1,])
point.f     = apply(Y.h[,1,],1,mean)
interval.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)

x           = seq(from=limits.1[1], to=limits.1[2], length.out=100)
z           = matrix(NA,h,99)
for (i in 1:h){
  z[i,]     = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
}
x           = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
yy          = 1:h
z           = t(z)

theta = 180
phi   = 15.5
f4    = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1, col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
polygon3D(x=c(interval.f[1,],interval.f[2,h:1]), y=c(1:h,h:1), z=rep(0,2*h), col = mcxs1.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE)
for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=0.5, col="black")
}
f4.l1 = trans3d(x=point.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=2, col=mcxs1)
}

#basic.plot.3d = basic.plot.3d.fun(Y.h)
```

### Forecasting on Extended Model
```{r}
#| echo: false
#| message: false
#| warning: false
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

  N = ncol(Y)
  K = ncol(X)
  T = nrow(Y)
  p = (K - 1) / N

  kappa.1   = 0.02^2
  kappa.2   = 100

  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  V.prior.inv = diag(1/diag(V.prior))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1

  alpha <- 1
  lambda.0 <- rexp(T, rate = 1/alpha)
  lambda.priors = list(alpha = alpha)

  h                          = 12
  S                          = 9500
  Y.h.ext                    = array(NA,c(h,N,S))

  A.posterior.ext.simu       = posterior.ext$A.posterior.exten
  Sigma.posterior.ext.simu   = posterior.ext$Sigma.posterior.exten
```

```{r}
#| echo: false
#| message: false
#| warning: false
  Y.h.ext       = sampling.predictive.density(A.posterior.simu = A.posterior.ext.simu ,
                                            Sigma.posterior.simu = Sigma.posterior.ext.simu ,
                                            S = S, h = h)
```

```{r}
#| echo: false
#| message: false
#| warning: false
#basic plot for extended
basic.plot.fun.extend = basic.plot.fun(Y.h.ext)
```
For extended model, it shows a clear increasing trend and reaching around 0.70 in the end of the forecasting period. For conduct 90% confident interval, we can notice the extended model has an wider confident interval, and this may be given by the nature of the Laplace distribution applied in the extended model in which the the fatter fail of Laplace distribution will capture more economic activities not covered by the underlying normal distribution.
```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(1,2), mar=c(1,2,1,1))
basic.plot.3d = basic.plot.3d.fun(Y.h)
basic.plot.3d.ext = basic.plot.3d.fun(Y.h.ext)
```
Regard to the 3D with density intervals above for both model forecasting, we could notice that the for each different predictive density at specific horizons, with the back wall we have the one period ahead predictive density and with the front we have the 12 period ahead density which is 3 years. We could see the distribution becomes lower and more dispersed with the increases of the horizon, as the data is more informative about the nearest developments in the future. Hence, one period predictive density is highly concentrated relative to others with smaller variance and taller peak. Similarly, the interval become more wider and dispersed resulting a more uncertainty for the future period forecasting.

### Forecasting on Basic Model With Stochastic Volatility Heteroskedasticity
```{r}
#| echo: false
#| message: false
#| warning: false
A.posterior.heter.simu       = posterior.heter[["A"]]
Sigma.posterior.heter.simu   = posterior.heter[["Sigma"]]
```

```{r}
#| echo: false
#| message: false
#| warning: false
# set up for heter-basic model
kappa.1   = 0.02^2
kappa.2   = 100
h         = 12
N         = ncol(Y)
S         = 9500
p         = frequency(Y)

Y.h.heter      = array(NA,c(h,N,S))

Y.h.heter       = sampling.predictive.density(A.posterior.simu = A.posterior.heter.simu ,
                                                Sigma.posterior.simu = Sigma.posterior.heter.simu,
                                                S = S, h = h) 
```

```{r}
#| echo: false
#| message: false
#| warning: false
#basic plot on basic heter
basic.plot.fun.heter = basic.plot.fun(Y.h.heter)
```
By involving the heteroskedasticity in the error term, the forecasting plot looks similar to the extended model, with the heteroskedasticity extension, it shows an increasing trend and reaching to 0.75, we could notice that the confident interval is slightly narrower compared to the basic model, indicating a more informative estimating of the parameters as it has a higher certainty from a more concentrated mean value and then leading to a narrower confident interval as the model can better accommodate shifts in variance that might otherwise skew or widen the distribution of observed values.

```{r}
#| echo: false
#| message: false
#| warning: false
# ### Forecasting on Extended Model With Stochastic Volatility Heteroskedasticity
# 
# A.posterior.heter.ext.simu       = posterior.draws.heter.ext$A.posterior
# Sigma.posterior.heter.ext.simu   = posterior.draws.heter.ext$Sigma.posterior
# h                          = 12
# S                          = 1000
# 
# Y.h.heter.ext      = array(NA,c(h,N,S))
# Y.h.heter.ext      = sampling.predictive.density(A.posterior.simu = A.posterior.heter.ext.simu ,
#                      Sigma.posterior.simu = Sigma.posterior.heter.ext.simu,S = S, h = h)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# basic.plot.fun.heter.ext = basic.plot.fun(Y.h.heter.ext)
```


### Interactive 3D versions
Interactive versions of the above 3D plots are provided below.
```{r}
#| echo: false
#| message: false
#| warning: false

  par(mfrow=c(1,2), mar=c(2,2,1,1))
  
  # Forecasting on Extended model
  limits.1        = range(Y.h[,1,])
  point.f         = apply(Y.h[,1,],1,mean)
  interval.f      = apply(Y.h[,1,],1,hdi,credMass=0.90)
  theta = 180
  phi   = 15.5
  
  x.erate           = seq(from=limits.1[1], to=limits.1[2], length.out=10)
  z.erate           = matrix(NA,h,9)
  for (i in 1:h){
    z.erate[i,]     = hist(Y.h[i,1,], breaks=x.erate, plot=FALSE)$density
  }
  x.erate           = hist(Y.h[i,1,], breaks=x.erate, plot=FALSE)$mids
  yy.erate          = 1:h

  # plot using plot_ly
  par(mfrow=c(1,1))
  plot_ly(y = yy.erate, x = x.erate, z=z.erate) %>%
    
    layout(scene=list(xaxis=list(title="AUD/USD"),
                      yaxis=list(title="h-step forecast"),
                      zaxis=list(title="density")),
           title = "AUD/USD forecast densities for basic model") %>%
    add_surface(colors = c(mcxs1,mcxs4,mcxs5))



# Forecasting on Extended model
limits.1        = range(Y.h.ext[,1,])
point.f         = apply(Y.h.ext[,1,],1,mean)
interval.f      = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
theta = 180
phi   = 15.5

x.erate           = seq(from=limits.1[1], to=limits.1[2], length.out=10)
z.erate           = matrix(NA,h,9)
for (i in 1:h){
  z.erate[i,]     = hist(Y.h.ext[i,1,], breaks=x.erate, plot=FALSE)$density
}
x.erate           = hist(Y.h.ext[i,1,], breaks=x.erate, plot=FALSE)$mids
yy.erate          = 1:h

# plot using plot_ly
par(mfrow=c(1,1))
plot_ly(y = yy.erate, x = x.erate, z=z.erate) %>%

  layout(scene=list(xaxis=list(title="AUD/USD"),
                    yaxis=list(title="h-step forecast"),
                    zaxis=list(title="density")),
         title = "AUD/USD forecast densities for extended model") %>%
  add_surface(colors = c(mcxs1,"magenta2",mcxs5))

```


# Conclusion 
In this research project, we use one basic and two extended Bayesian models to forecast the AUD/USD exchange rate, with the extension model with heteroskedasticity as a remaining topic to do further forecasting. For the basic model, we use Minnesota prior distribution before fitting our non-stationary data property. Then we applied the Laplace distribution to the error term to capture the leptokurtotic and anomies of the financial data. We also introduced the stochastic volatility model to further capture the volatility of the error term.

We do the forecasting in h=12 steps which is 3 years in our case and for the point estimate of the forecasting, among different models, the basic model forecasting shows a slightly decreasing trend, while the extended and the basic model with heteroskedasticity has an opposite trend and shows an increasing trend among forecasting periods.

In conclusion, compared to the basic model, both extension models increase the estimation of the forecasting in different ways and provide more informative forecasting of the AUD/USD exchange rate.

# References {.unnumbered}
Woźniak, T. (2016), Bayesian Vector Autoregressions. Australian Economic Review, 49: 365-380. https://doi.org/10.1111/1467-8462.12179

Eltoft, T., Kim, T., & Lee, T.-W. (2006). On the multivariate Laplace distribution. IEEE Signal Processing Letters, 13(5), 300–303. https://doi.org/10.1109/LSP.2006.870353
