---
title: "Exchange Rate Forecasting Using Bayesian VARs Model"
author: "Qingqing Pang"
execute:
  
  echo: false
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

> **Abstract.** This research report explores how Bayesian VARs model
> predict AUD/USD exchange rate. **Keywords.** Bayesian Vars, Exchange
> rate, Forecasting, Minnesota Prior, Laplace distribution

# Objective and Motivation

The objective of this research is to use the Bayesian Vector
Autoregressions (VARs) method to forecast the exchange rate of the US
dollar exchange rate against the Australian dollar.

The ability to accurately forecast the foreign exchange rate is crucial
for Australia’s global trade and investment. Given the prominence of the
US dollar as the world's primary reserve currency, the monetary policies
of the US Federal Reserve have a worldwide effect on the world economy,
its significant influence on the Australia currency market should be
important for domestic investors and policy maker as it can directly
impact the AUD/USD exchange rate. Besides, America as one of the major
trading partners for Australia, can impact bilateral trade flows and
eventually affect the value of the AUD. Apart from the external from
foreign countries, the domestic economic indicators can also be one of
the determinants of the AUD/USD exchange rate.

The research is aimed to address the question for example, how the
AUD/USD exchange rate will be in 3 months or even longer 1 year?

# Data and Variables

To better forecast the change in the exchange rate, the 13 variables are
selected as follows which contain both domestic and US economic
indicators that affect the exchange rate in different ways.

Real GDP and interest rates have a significant effect on the exchange
rate, a higher realGDP and interest rate in Australia may increase the
demand for AUD, which will lead to an appreciation of AUD and a rise in
the AUD/USD exchange rate. A higher CPI indicates a lower purchasing
power relative to foreign currency which may lead to a depreciation of
the domestic currency. The unemployment rate can in some way represent
business activity and a country with a high unemployment rate will lower
the attractiveness for foreign investors and weaken the domestic
currency competitiveness in the currency market. The balance of trade,
which is the difference between exports and imports, also can influence
the demand for its currency, a trade surplus in AUD may increase the
demand for AUD dollar.

-   $erate_{t}$: AUD/USD exchange rate

-   AUS economic indicators

    -   $crate\_au_{t}$: The Cash Rate Target, Australia
    -   $rgdp\_au_{t}$: The Real Gross Domestic Product, Australia
    -   $cpi\_au_{t}$: The Consumer Price Index, Australia
    -   $unemr\_au_{t}$: The Unemployment rate, Australia
    -   $impor\_au_{t}$: The Imports of Goods and Services, Australia
    -   $expor\_au_{t}$: The Exports of Goods and Services, Australia

-   US economic indicators

    -   $crate\_us_{t}$: The Federal Funds Effective Rate, United States
    -   $rgdp\_us_{t}$: The Real Gross Domestic Product, United States
    -   $cpi\_us_{t}$: The Consumer Price Index, United States
    -   $unemr\_us_{t}$: The Unemployment rate, United States
    -   $impor\_us_{t}$: The Imports of Goods and Services, United
        States
    -   $expor\_us_{t}$: The Exports of Goods and Services, United
        States

### Data Cleaning

For the data cleaning part, most data for AUS is from Reserve Bank of
Australia (RBA) and Australian Bureau of Statistics (ABS), and data for
the US is from FRED, the dataset spans from 1990 Q1 to 2023 Q4,
comprising 136 observations. To better fit the model, the data has been
transformed to 'quarter' to ensure that seasonality effects are removed
and logged transformations have been applied to most data except
exchange rate and cash rate to reducing outlier effects.

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false
#| results: hide

library(fredr)
library(readrba)
library(readabs)
library(xts)
library(tseries)
library(fUnitRoots) 
library(plot3D)
library(MASS)
library(HDInterval)
library(plotly)       
library(mgcv)
library(mvtnorm)
library(Rmpfr)
library(dplyr)
library(psych)
fredr_set_key("75b470c4883ecfd5a7b4185f30437bd0")
```

```{r global options}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)

```

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false

#Y variable
# AUD/USD exchange rate quarterly
ex_rate <- read_rba(series_id = "FXRUSD")
ex_rate$date <- as.Date(ex_rate$date)
erate <- xts(ex_rate$value, ex_rate$date)
erate <- apply.quarterly(erate,mean)
index(erate) <- seq(as.Date("1969-09-01"), by = "3 months", length.out = nrow(erate))


#gold price (2006-)
#gpricelink <- "https://query1.finance.yahoo.com/v7/finance/download/GC%3DF?period1=1262304000&period2=1703980800&interval=1mo&filter=history&frequency=1mo&includeAdjustedClose=true"
#gprice  <- read.csv(gpricelink)
#gprice$Date <- as.Date(gprice$Date)
#gprice <- xts(gprice$Adj.Close, gprice$Date)
#gprice <- apply.quarterly(gprice,mean)

# Australia real gdp seasonal adjusted quarterly
#rgdp_au <- read_abs(series_id = "A2304404C")  
rgdp_au <- read_rba(series_id = "GGDPCVGDP")
rgdp_au <- xts::xts(rgdp_au$value, rgdp_au$date)
index(rgdp_au)   <- seq(as.Date("1959-09-01"), by = "3 months", length.out = nrow(rgdp_au))

#cash rate/interest rate of AUS quartly
cashrate<- read_cashrate(type = c("target"))
crate_au<- xts(cashrate$value, cashrate$date)
crate_au<- apply.quarterly(crate_au,mean)
crate_au<- xts(crate_au, seq(as.Date("1990-03-01"), by = "quarter", length.out = length(crate_au)))

#CPI quartly
# cpi_au <- read_rba(series_id = "GCPIAG")
cpi_au <- read_abs(series_id = "A2325846C")  
cpi_au <- xts::xts(cpi_au$value, cpi_au$date)

#unemployment rate quartly
#unemprate <-read_rba(series_id = "GLFSURSA")
unemprate <- read_abs(series_id = "A84423050A") 
unemr_au<- xts(unemprate$value, unemprate$date)
unemr_au<- apply.quarterly(unemr_au,mean)

# International Trade in Goods and Services seasonal adjusted_quartly
exportaus <- read_abs(series_id = "A2718603V")   
expor_au<- xts(exportaus $value, exportaus $date)
expor_au<- abs(expor_au)
expor_au<- apply.quarterly(expor_au,mean)

importaus <- read_abs(series_id = "A2718577A")     
impor_au<- xts(importaus$value, importaus$date)
impor_au<- apply.quarterly(impor_au,mean)

# America data
# us gdp
#rgdpus <- fredr(series_id = "A939RX0Q048SBEA")
rgdpus     <- fredr(series_id = "GDPC1")
rgdp_us     <- to.quarterly(xts(rgdpus$value, rgdpus$date), OHLC = FALSE)
index(rgdp_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(rgdp_us))

#Federal Funds Effective Rate/interest rate quartly
usdratelink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DFF&scale=left&cosd=1954-07-01&coed=2024-03-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Daily%2C%207-Day&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-04-01&revision_date=2024-04-01&nd=1954-07-01"
crate_us <- read.csv(usdratelink)
crate_us$DATE <- as.Date(crate_us$DATE)
crate_us <- xts(crate_us$DFF, order.by = crate_us$DATE)
crate_us <- apply.quarterly(crate_us,mean)
crate_us<- xts(crate_us, seq(as.Date("1954-09-01"), by = "quarter", length.out = length(crate_us)))

# cpi quartly
cpiusd  <- fredr(series_id = "USACPIALLMINMEI")
cpi_us<- xts(cpiusd$value, cpiusd$date)
cpi_us<- apply.quarterly(cpi_us,mean)

# unemployment quartly
unemprate_usd = fredr(series_id = "UNRATE")
unemr_us <- xts(unemprate_usd$value, unemprate_usd$date)
unemr_us<- apply.quarterly(unemr_us,mean)

#export_usd——quartly
usdexportink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=EXPGS&scale=left&cosd=1947-01-01&coed=2023-10-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-03-30&revision_date=2024-03-30&nd=1947-01-01"
expor_us <- read.csv(usdexportink)
expor_us$DATE <- as.Date(expor_us$DATE)
expor_us <- xts::xts(expor_us$EXPGS, order.by = expor_us$DATE)
index(expor_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(expor_us))

#import_usd_quartly
usdimportlink = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=IMPGS&scale=left&cosd=1947-01-01&coed=2023-10-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-03-30&revision_date=2024-03-30&nd=1947-01-01"
impor_us <- read.csv(usdimportlink)
impor_us$DATE <- as.Date(impor_us$DATE)
impor_us <- xts::xts(impor_us$IMPGS, order.by = impor_us$DATE)
index(impor_us) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(impor_us))

```

```{r}
#| echo: false
#| message: false
#| warning: false

# log transformation of data
variables <- c("cpi_au", "cpi_us", "rgdp_au", "rgdp_us", "impor_au", "impor_us", "expor_au", "expor_us")

for(var in variables) {
  assign(var, log(get(var)))
}

#gprice <- log(gprice)
```

```{r}
#| echo: false
#| message: false
#| warning: false
 
# All Variables
merged_data = na.omit(merge(erate, 
                            cpi_au, cpi_us, 
                            crate_au, crate_us, 
                            expor_au, expor_us,  
                            impor_au, impor_us, 
                            rgdp_au, rgdp_us,
                            unemr_au, unemr_us))

# Defining your column name vector:
variable_names <- c("exchange rate", "cpi_au", "cpi_us", 
                    "cashrate_au", "cashrate_us", "export_au", "export_us",
                    "import_au", "import_us", "realgdp_au", "realgdp_us",
                    "unemployemtrate_au", "unemployemtrate_us")


colnames(merged_data)   <- variable_names
```

#### Visualisation of data

Plot the variables to see the patterns of data. It shows from the plots
that the exchange rate and cash rate for the US fluctuate over time, the
cash rate and unemployment for AU show a downward trend and all other
variables have a clear upward trend, with the exports、imports, and GDP
for both countries have a clear drop during the COVID-19 period.

```{r all variables plot}
#| echo: false
#| message: false
#| warning: false

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:13) { 
  ts.plot(merged_data[, i], main = colnames(merged_data)[i], 
          ylab = "", xlab = "")
}

```

Since most variables show a non-stationary pattern. To determine whether
a unit root is present in a time series dataset, the ADF test will be
conducted below.

#### Augmented Dickey-Fuller test for log transformed variables except exchange rates and cash rates.

From the plot we can observe all ACF plots have a high degree of
persistence over time, indicating there is significant autocorrelation
in the time series data.

```{r}
#| echo: false
#| message: false
#| warning: false

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:13){
acf = acf(merged_data[,i], plot = FALSE)[1:20]
plot(acf, main = "")
title(main = paste(colnames(merged_data)[i]), line = 0.5)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
adf_test <- list()
for (i in 1:13) {
  adf_result = adf.test(merged_data[,i], k = 4)
  adf_test[[i]] <- adf_result
}
```

Below is the p-value of each variable and only the cash rate for AUS has
a p-value less than 0.05 which indicates that $crateau_{t}$ is
stationary.

```{r}
#| echo: false
#| message: false
#| warning: false
adf_table <- data.frame(p_value = numeric(length(adf_test)))

for (i in 1:length(adf_test)) {adf_table[i, "p_value"] = round(adf_test[[i]]$p.value,3)
}

rownames(adf_table) <- variable_names

colnames(adf_table)<- c("P-value")
knitr::kable(adf_table)
```

Below is the ADF test result for all non-stationary data taking the
first difference. All variables in the first differences are stationary
as the null hypothesis of non-stationary can be rejected.

```{r}
#| echo: false
#| message: false
#| warning: false
#take the first difference
nonstationary_merged_data <- subset(merged_data, select = -c(cashrate_au))

dff_merged_data <- na.omit(nonstationary_merged_data - lag(nonstationary_merged_data))
```

```{r}
#| echo: false
#| message: false
#| warning: false
# ADF test
dff_adf_test <- list()
for (i in 1:12) {
  dff_adf_result = adf.test(dff_merged_data[,i], k = 4)
  dff_adf_test[[i]] <- dff_adf_result
}

# View the ADF test results
dff_adf_table <- data.frame(p_value = numeric(length(dff_adf_test)))

# Fill in the data frame with the test results
for (i in 1:length(dff_adf_test)) {
  dff_adf_table[i, "p_value"] = round(dff_adf_test[[i]]$p.value,3)

}
rownames(dff_adf_table) <- variable_names[-4]


colnames(dff_adf_table)<- c("P-value")
knitr::kable(dff_adf_table)
```

It can be concluded that all variables are integrated at 1 at the 5% significance level of the ADF test, with the conclusion folding for $crate\_au_{t}$ only at 1% significance level. Appropriate prior distributions will be used to accommodate this fact for the VAR model.

# Model and Hypotheses

In this research, the VAR(p) model will be applied to forecast the AUD/USD exchange rate, below is the basic model that is used in this research.

#### The basic VAR(p) model

```{=tex}
\begin{aligned}
 y_{t}&=\mu_{0}+A_{1}y_{t-1}+\cdots+A_{p}y_{t-p}+\epsilon_{t} \\
\epsilon_{t}|Y_{t-1} &\sim iid\mathcal{N}(0_{13},\Sigma)
\end{aligned}
```
For time $t$ = 1,2,.....,$T$：

-   $y_t$ is a $N(13)\times 1$ vector of observations at time $t$
-   $\mu_0$ is a $N(13)\times 1$ vector of constant terms
-   $A_i$ is a $N(13)\times N(13)$ matrix of autoregressive slope parameters
-   $\epsilon_t$ is a $N(13)\times 1$ vector of error terms and follows a multivariate white noise process
-   $Y_{t-1}$ is the information set collecting observations on y up to time $t-1$
-   $\Sigma$ is a $N(13)\times N(13)$ covariance matrix of the error term

```{=tex}
\begin{aligned}
y_{t}=\begin{pmatrix}
erate_{t}\\
crate\_au_{t} \\
rgdp\_au_{t}\\
cpi\_au_{t} \\
unemr\_au_{t}\\
impor\_au_{t} \\
expor\_au_{t} \\
crate\_us_{t} \\
rgdp\_us_{t} \\
cpi\_us_{t} \\
unemr\_us_{t} \\
impor\_us_{t}\\
expor\_us_{t}\\

\end{pmatrix}

\end{aligned}
```
For further research, we may use the predictive density function like 3-year-ahead forecast and forecast with Bayesian VARS.

# Modelling Framework

## The basic model

```{=tex}
\begin{align}
Y &=XA+U\\
U|X&\sim \mathcal{MN}_{T\times N}(0, \Sigma, I_T)\\
Y|X,A,\Sigma&\sim \mathcal{MN}_{T\times N} (XA, \Sigma, I_T)
\end{align}
```
Where $Y$ is a $T\times 13$ Matrix, $X$ is a $T\times(1+p\times13)$, $A$ is a $(1+p\times13)\times13$ matrix that contains $\mu_{0}$ and vectors of the autoregressive slope parameters and $U$ is a $T\times13$ matrix contains vetors of error terms.

The kernel of the likelihood function:

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}}exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\}
\end{align}
```
The basic model is based on **Natural-conjugate prior distribution**, where the $A$ follows a Matrix-variate Normal distribution and $\Sigma$ follows an Inverse Wishart distribution.

```{=tex}
\begin{align}
p(A,\Sigma) &= p(A|\Sigma)p(\Sigma) \\
A|\Sigma &\sim \mathcal{MN}_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\Sigma &\sim \mathcal{IW}_N(\underline{S},\underline{\nu})
\end{align}
```
The **Minnesota prior** is typically a good choice for specifying priors in BVAR model especially when the model involves many macroeconomic variables. It assumes the variables follow a random walk and it is suitable for unit root non-stationary variables such as in our case, variables are integrated at 1 at the 5% significance level of the ADF test, where:

```{=tex}
\begin{align}
\underline{A} &= \begin{bmatrix}0_{N \times 1} & I_{N} & 0_{N \times (p-1)N}\end{bmatrix}'\\

\underline{V} &= diag( \begin{bmatrix} \kappa_2 & \kappa_1(p^{-2}\otimes I_N') \end{bmatrix})

\end{align}
```
The prior mean $\underline{A}$ for the first lag of each variable (the identity matrix portion) is one, while all other coefficients including intercepts, are zeroes. For the column-specific prior covariance
$\underline{V}$, two shrinkage hyper-parameters $\kappa_1$ and $\kappa_2$ represent the overall shrinkage level for slopes and constant terms respectively.

For **posterior distribution**, the kernel of the posterior distribution takes the form of the product of the likelihood and the prior distributions.

```{=tex}
\begin{align*}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma|Y,X)p(A,\Sigma) \\
&= L(A,\Sigma|Y,X)p(A|\Sigma)p(\Sigma)
\end{align*}
```
```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\} \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A}) \underline{V}^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\}
\end{align}
```
The kernel can be represent as the normal-inverse Wishart distribution and we can get the following **full conditional joint posterior distribution:**

```{=tex}
\begin{align}
p(A|Y,X,\Sigma) &= \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
p(\Sigma|Y,X) &= \mathcal{IW}_N(\bar{S},\bar{\nu}) \\
\\
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A} \\

\end{align}
```
## The extended model

The extended model will be built based on the the change in distribution of the error to **Laplace distribution** instead of the normally distributed errors assumption. The Laplace distribution is suitable for describing financial anomalies due to its sharp peaks and thick tails and the use of this distribution improves the robustness of the model to anomalies and is particularly suitable for financial time series. As our variables are most financial time series data, a Laplace distribution is more suitable to apply to our error term.

Following [Eltoft,Kim, and Lee 2006b](https://ieeexplore.ieee.org/document/1618702), for covariance with a general Kronecker structure, if each ${\lambda_t}$ has an independent exponential distribution with mean ${\alpha}$, then marginally ${U_t}$ has a multivariate Laplace distribution with mean vector 0 and covariance matrix ${\alpha\Sigma}$.

```{=tex}
\begin{align}

vec(U)&\sim N(0,\Sigma\otimes \Omega)\\

U_t &\sim \text{Laplace}(0, \alpha\Sigma) \\

U_t | \lambda_t &\sim \mathcal{MN}(0, \Sigma, \Omega) \\

\lambda_t &\sim \text{Exponential}(\frac{1}{\alpha})
\end{align}
```

The model is where $$\Omega=diag(\lambda_1,\lambda_2,...,\lambda_t)$$, each lambda independently drawn from an exponential distribution.

The kernel of the likelihood function now is :

```{=tex}
\begin{align}
L(A,\Sigma,\Omega |Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1} \lambda_t^{-\frac{N}{2}} exp({-\frac{1}{2}}\sum^{T}_{t =1}{\frac{1}{\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t)\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1}( \lambda_t^{-\frac{N}{2}} exp({-\frac{1}{2}}{\frac{1}{\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t))
\end{align}
```

Thence, for each time t, we have the likelihood like: 

```{=tex}
\begin{align}
&=\lambda_t^{-\frac{N}{2}} exp({-\frac{1}{2}}{\frac{1}{\lambda_t}} \epsilon_t' \Sigma^{-1}\epsilon_t)
\end{align}
```

For posteriors distribution, $A$, $\Sigma$ and $\lambda_t$ can then be derived using the likelihood and the prior distributions as follows:

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma,\lambda_t|Y,X)p(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\} \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\} \\
&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2} tr[\Sigma^{-1}(Y'\Omega^{-1}Y - 2A'X'\Omega)^{-1}Y + A'X'\Omega^{-1}XA \\
&+ A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})]\}

\end{align}
```

The kernel can be rearranged in the form of the **Matrix-variate normal-inverse Wishart distribution**. 

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\sim MNIW(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
&\\
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S} - \bar{A}'\bar{V}^{-1}\bar{A}
\end{align}
```

The prior distribution of
$$\lambda_t \sim \text{Exponential}(\frac{1}{\alpha})$$ define as:

```{=tex}
\begin{align}
p(\lambda_t|\alpha) &= \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\

\end{align} 
```

The kernel of the fully conditional posterior distribution of $\lambda_t$ is then derived as follows:

```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda_t|Y,X)p(\lambda_t) \\
\\
&\propto \lambda_t^{-\frac{N}{2}}exp({-\frac{1}{2}}{\frac{1}{\lambda_t}}\epsilon_t' \Sigma^{-1}\epsilon_t) \\
&\times \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\


&= \lambda_t^{-\frac{N}{2}+1-1} exp\{-\frac{1}{2}[\frac{
\epsilon_t' \Sigma^{-1}\epsilon_t} {\lambda_t} +\frac{2}{\alpha}\lambda_t]\} 
\end{align}
```

The above expression can be rearranged in the form of a Generalized
inverse Gaussian distribution kernel as follows:

```{=tex}
\begin{align}
\lambda_t|Y,A,\Sigma &\sim GIG(a,b,p) \\
\\
a &=\frac{2}{\alpha} \\
b &= \epsilon_t' \Sigma^{-1}\epsilon_t \\
p &= -\frac{N}{2}+1
\end{align}
```

## Proof of model validity

### Proof of basic model validity

To test the model validity, we simulated 1000 observations from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2 to see how the autoregressive and the covariance matrices and the posterior mean of the constant term behave.

```{r}
#| echo: false
#| message: false
#| warning: false
set.seed(123)
n <- 1000  # Number of observations
mu <- 0    # Mean
sigma <- 1 # Standard deviation

# Simulate two independent random walks
simulation_data <- data.frame(RW1 = cumsum(rnorm(n, mu, sigma)),RW2 = cumsum(rnorm(n, mu, sigma)))

plot(simulation_data$RW1, type = 'l', ylim = range(simulation_data), col = 'red', ylab = 'Value', xlab = 'Time', main = 'Bivariate Random Walk')
lines(simulation_data$RW2,col = 'blue')
legend("topright",legend = c("RW1", "RW2"), col = c("red", "blue"), lty = 1, cex = 0.6)
```

```{r static data setup}
#| echo: false
#| message: false
#| warning: false
## Present data X, Y
y             = ts(merged_data[,1:ncol(merged_data )])
Y             = ts(y[5:nrow(y),], frequency=4)
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])
}
 
## Pre-setup 
N             = ncol(Y)
p             = frequency(Y)
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Minnesota prior 
kappa.1       = 0.02^2                                    
kappa.2       = 100                                   
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N) 
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+1
I.matrix            = diag(1,nrow(Y),nrow(Y))
```

```{r function based on basic model}
#| echo: false
#| message: false
#| warning: false

## Posterior sample draw

    posterior.draws       = function (S, Y, X){
  
    # normal-inverse Wishard posterior parameters
    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))
    V.bar             = solve(V.bar.inv)
    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar            = nrow(Y) + nu.prior
    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
  
    # posterior draws 
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }
 
    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
# simulation data generating process
p=1
N=2

Y_simulation = (simulation_data[(p+1):nrow(simulation_data),]) #contains the obs of the two variables and moves first obs
X_simulation = matrix(1,nrow(Y_simulation),1) #initializes the X matrix with a column of ones(intercept) in the VAR model.                                      
# adds the lagged values of the two variables to the X matrix, in this case, it adds one lagged value for each of the two variables.
for (i in 1:p){
  X_simulation     = cbind(X_simulation, (simulation_data[(p+1):nrow(simulation_data)-i,]))
}

Y_simulation = as.matrix(Y_simulation)
X_simulation = as.matrix(X_simulation)

N           = ncol(Y_simulation)                          
p           = frequency(Y_simulation)
A.hat       = solve(t(X_simulation)%*%X_simulation)%*%t(X_simulation)%*%Y_simulation
Sigma.hat   = t(Y_simulation-X_simulation%*%A.hat)%*%(Y_simulation-X_simulation%*%A.hat)/nrow(Y_simulation)

# Minnesota prior
kappa.1             = 0.02^2                                    
kappa.2             = 100                                  
A.prior             = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior             = diag(diag(Sigma.hat))
nu.prior            = N+1

posterior.sample.draws = posterior.draws(S=1000, Y=Y_simulation, X=X_simulation) 
```

```{r}
Sigma_posterior_mean <- apply(posterior.sample.draws$Sigma.posterior, 1:2, mean)

Sigma_df <- as.data.frame(Sigma_posterior_mean)
colnames(Sigma_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(Sigma_df) <- c("Y1-Lag", "Y2-Lag")
knitr::kable(Sigma_df, caption = "Posterior mean of the covariance matrix Sigma")
```

```{r}
# Calculate posterior mean of autoregressive coefficients (including the constant term)
A_posterior_means <- apply(posterior.sample.draws$A.posterior, 1:2, mean)

A_df <- as.data.frame(A_posterior_means)
colnames(A_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(A_df) <- c("Constant", "Y1-Lag", "Y2-Lag")
knitr::kable(A_df, caption = "Posterior mean of the autoregressive coefficient matrix A")
```

The diagonal entries of the covariance matrix are close to 1, which indicates that each variable has a strong autoregressive relationship with itself and similarly, the diagonal elements of the autoregressive
coefficient matrix are close to one, suggesting that each variable is heavily influenced by its past value. Besides, the posterior means for the constant terms is close to 0, the above can indicate that the
estimated parameter constant term and means are consistent with what we expect given a Minnesota prior.

### Proof of extended model
The Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:
  
  1. Draw $\Sigma^{(s)}$ from the $IW(\bar{S},\bar{\nu})$ distribution.
2. Draw $A^{(s)}$ from the $MN(\bar{A},\Sigma^{(s)}, \bar{V})$ distribution.
3. Draw $\lambda_t^{(s)}$ from $GIG(a,b,p)$.

Repeat steps 1, step 2 and 3 for $S_1$+$S_2$times.

Discard the first draws that allowed the algorithm to converge to the stationary posterior distribution.

Output is $\left\{ {A^{(s)}, \Sigma^{(s)}}, \lambda_t^{(s)}\right\}^{S_1+S_2}_{s=S_1+1}$.
```{r}
S1= 500
S2= 9500
total_S= S1+S2
S=total_S
```

```{r}
posterior.draws.extended <- function(total_S,Y, X){
  
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  N = ncol(Y)
  T <- nrow(Y)
  kappa.1   = 0.02^2
  kappa.2   = 100
  K = 1 + (p*N)
  
  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  alpha <- 2
  lambda.0 <- rexp(T, rate = 1/alpha)
  lambda.priors = list(alpha = 2)
  
  
  # Initialize arrays to store posterior draws
  Sigma.posterior.draws = array(NA, c(N,N,S))
  A.posterior.draws = array(NA, c((1+p*N),N,S))
  
  lambda.posterior.draws = array(NA,c(T,S+1))
  b = array(NA,c(T,S))
  
  #lambda.posterior.draws <- array(NA,c(T,S+1))
  
  for (s in 1:S){
    
    if (s == 1) {
      lambda.s = lambda.0
    } else {
      lambda.s    = lambda.posterior.draws[,s]
    }
    
    Omega = (diag(lambda.s))
    Omega.inv = diag(1/lambda.s)
    # Omega.inv = sqrt(diag(1/lambda.s))
    # Omega.inv = sqrt(Omega.inv)
    
    V.bar.inv.ext   = t(X)%*%Omega.inv%*%X + solve(V.prior)
    V.bar.ext       = solve(V.bar.inv.ext)
    A.bar.ext       = V.bar.ext%*%(t(X)%*%Omega.inv%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar.ext      = T + nu.prior
    S.bar.ext       = S.prior + t(Y)%*%Omega.inv%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar.ext)%*%V.bar.inv.ext%*%A.bar.ext
    S.bar.ext.inv   = solve(S.bar.ext)
    
    
    Sigma.inv.draw = rWishart(1, df = nu.bar.ext, Sigma = S.bar.ext.inv)[,,1]
    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
    
    
    u.t = Y-X%*%A.posterior.draws[,,s]
    #    ---- loop lambda posterior ----   #
    c                      = -N/2 + 1          # N=13
    a                      = 2 / lambda.priors$alpha
    for (x in 1:T){
      b                  = t((u.t)[x,])%*%Sigma.posterior.draws[,,s]%*%(u.t)[x,]
      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)
    } # END x loop
  } # END s loop
  
  
  #}
  
  output                 = (list(A.posterior.exten = A.posterior.draws[,,S1:total_S], 
                                Sigma.posterior.exten = Sigma.posterior.draws[,,S1:total_S], 
                                lambda.posterior.exten = lambda.posterior.draws[,(S1+1):(S+1)]))
  
}

# conduct simulation
posterior.extended = posterior.draws.extended(total_S = total_S, Y=Y_simulation, X=X_simulation)
```


```{r}
Sigma_posterior_mean <- apply(posterior.extended$Sigma.posterior.exten, 1:2, mean)

Sigma_df <- as.data.frame(Sigma_posterior_mean)
colnames(Sigma_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(Sigma_df) <- c("Y1-Lag", "Y2-Lag")
knitr::kable(Sigma_df, caption = "Posterior mean of the covariance matrix Sigma")
```

```{r}
A_posterior_means <- apply(posterior.extended$A.posterior.exten, 1:2, mean)

A_df <- as.data.frame(A_posterior_means)
colnames(A_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(A_df) <- c("Constant", "Y1-Lag", "Y2-Lag")
knitr::kable(A_df, caption = "Posterior mean of the autoregressive coefficient matrix A")
```

# Forecasting

The forecast will focus on two models with the sample period from 1990 Q1 to 2023Q4, and forecast how the exchange rate changes for the future up to 2026Q4.

## Forecasting with basic model and extended model

```{r forecasting static data}
#| echo: false
#| message: false
#| warning: false
## Present data X, Y
y             = ts(merged_data[,1:ncol(merged_data)])
Y             = ts(y[5:nrow(y),], frequency=4)
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])
}

## Pre-setup 
N             = ncol(Y)
p             = frequency(Y)
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution specification - Minnesota prior 
kappa.1       = 0.02^2                                   
kappa.2       = 100                                   
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2,1]  = 1
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+1

h                      = 12
S                      = 10000
Y.h                    = array(NA,c(h,N,S))

## Applying function 
posterior.sample.draws = posterior.draws(S=10000, Y=Y, X=X)
A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior
```

```{r}
#| echo: false
#| message: false
#| warning: false
# sampling predictive density
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,,s]
  x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
  x.Ti               = x.Ti[p:1,]
  for (i in 1:h){
    x.T               = c(1,as.vector(t(x.Ti)))
    Y.f               = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
    x.Ti            = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h[i,,s]         = Y.f[1:N]
  }
}
```

```{r}
# ex_rate.point.f    = apply(Y.h[,1,],1,mean) 
# ex_rate.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.15)
# ex_rate.range      = range(y[,1],ex_rate.interval.f)
# 
# true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value
# 
# # Define the range and labels for the y-axis
# y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
# y_max <- max(ex_rate.range[2], true_y_value)
# num_labels <- 4  # You can adjust the number of labels
# 
# # Generate a sequence of y-values for the axis that includes true_y_value
# y_values <- pretty(c(y_min, y_max), num_labels)
# if(!true_y_value %in% y_values) {
#   y_values <- sort(c(y_values))
# }
# 
# # Generate labels for the y-axis, ensuring true_y_value is included and highlighted
# y_labels <- sprintf("%.2f", y_values)
# y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")
# 
# # Plot adjustments
# plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
# axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
# axis(2, at=y_values, labels=y_labels, col="black")
# 
# # Red dashed line for the true y-value
# abline(h=true_y_value, col="red", lty=2, lwd=1)
# 
# 
# text(x=nrow(y), y=9.65, srt=90, "2025-12")
# abline(v=nrow(y), col=mcxs4)
# text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
# abline(v=nrow(y)+h, col=mcxs2)
# legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.45)
# 
# polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
#         c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
#         col=mcxs1.shade1, border=mcxs1.shade1)

```

```{r}
### Forecasting on extended model
S1                = 100                            # will be discard
S2                = 900                            
total_S           = S1+S2
S=total_S

y             = ts(merged_data[,1:ncol(merged_data)])
Y             = ts(y[5:nrow(y),], frequency=4)

X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])
}

p             = frequency(Y)

posterior.draws.extended <- function(total_S,Y, X){
  
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  N = ncol(Y)
  T <- nrow(Y)
  kappa.1   = 0.02^2
  kappa.2   = 100
  K = 1 + (p*N)
  
  
  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  alpha <- 2
  lambda.0 <- rexp(T, rate = 1/alpha)
  lambda.priors = list(alpha = 2)
  
  
  # Initialize arrays to store posterior draws
  Sigma.posterior.draws = array(NA, c(N,N,S))
  A.posterior.draws = array(NA, c((1+p*N),N,S))
  
  lambda.posterior.draws = array(NA,c(T,S+1))
  
 # Omega.inv.test = diag(1/lambda.0)
  
  #lambda.posterior.draws <- array(NA,c(T,S+1))
  
  for (s in 1:S){
    
    if (s == 1) {
      lambda.s = lambda.0
    } else {
      lambda.s    = lambda.posterior.draws[,s]
    }
    
    Omega = (diag(lambda.s))
    Omega.inv = diag(1/lambda.s)
    Omega.inv = sqrt(diag(1/lambda.s))
    # Omega.inv = sqrt(Omega.inv)
    
    V.bar.inv.ext   = t(X)%*%Omega.inv%*%X + solve(V.prior)
    V.bar.ext       = solve(V.bar.inv.ext)
    A.bar.ext       = V.bar.ext%*%(t(X)%*%Omega.inv%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar.ext      = T + nu.prior
    S.bar.ext       = S.prior + t(Y)%*%Omega.inv%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar.ext)%*%V.bar.inv.ext%*%A.bar.ext
    S.bar.ext.inv   = solve(S.bar.ext)
    
    
    Sigma.inv.draw = rWishart(1, df = nu.bar.ext, Sigma = S.bar.ext.inv)[,,1]
    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
    
    
    u.t = Y-X%*%A.posterior.draws[,,s]
    #    ---- loop lambda posterior ----   #
    c                      = -N/2 + 1          # N=13
    a                      = 2 / lambda.priors$alpha
    for (x in 1:T){
      b                  = t((u.t)[x,])%*%Sigma.posterior.draws[,,s]%*%(u.t)[x,]
      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)
    } # END x loop
  } # END s loop
  
  
  #}
  
  output                 = (list(A.posterior.exten = A.posterior.draws[,,S1:total_S], 
                                 Sigma.posterior.exten = Sigma.posterior.draws[,,S1:total_S], 
                                 lambda.posterior.exten = lambda.posterior.draws[,(S1+1):(S+1)]))
  }


posterior.ext              = posterior.draws.extended(total_S = total_S, Y=Y, X=X)
A.posterior.ext.simu       = posterior.ext$A.posterior.exten
Sigma.posterior.ext.simu   = posterior.ext$Sigma.posterior.exten

#posterior.extend.draws     = posterior.draws.extended(total_S = total_S, Y=Y, X=X)
#A.posterior.ext.simu       = posterior.extend.draws[["A.posterior.exten"]]
#Sigma.posterior.ext.simu   = posterior.extend.draws[["Sigma.posterior.exten"]]

## Three-year ahead forecasting h=12
# set up
h                          = 12
S                          = 900
Y.h.ext                    = array(NA,c(h,N,S))
```

```{r}
# sampling predictive density
for (s in 1:S){
  A.posterior.draw         = A.posterior.ext.simu[,,s]
  Sigma.posterior.draw     = Sigma.posterior.ext.simu[,,s]
  x.Ti                   = Y[(nrow(Y)-p+1):nrow(Y),]
  x.Ti                   = x.Ti[p:1,]
  for (i in 1:h){
    x.T                    = c(1,as.vector(t(x.Ti)))
    Y.f                    = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
    x.Ti                 = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h.ext[i,,s]          = Y.f[1:N]
  }
}
```


#### 90% and 68% highest density intervals respectively
```{r}
par(mfrow = c(3, 2), mar = c(2, 3, 1, 1), oma = c(0, 0, 4, 0))

ex_rate.point.f    = apply(Y.h[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)
ex_rate.range      = range(y[,1],ex_rate.interval.f)

true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value

# Define the range and labels for the y-axis
y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
y_max <- max(ex_rate.range[2], true_y_value)
num_labels <- 4  # You can adjust the number of labels

# Generate a sequence of y-values for the axis that includes true_y_value
y_values <- pretty(c(y_min, y_max), num_labels)
if(!true_y_value %in% y_values) {
  y_values <- sort(c(y_values))
}

# Generate labels for the y-axis, ensuring true_y_value is included and highlighted
y_labels <- sprintf("%.2f", y_values)
y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")

# Plot adjustments
plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
axis(2, at=y_values, labels=y_labels, col="black",las=1)

# Red dashed line for the true y-value
abline(h=true_y_value, col="red", lty=2, lwd=1)


text(x=nrow(y), y=9.65, srt=90, "2025-12")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
abline(v=nrow(y)+h, col=mcxs2)
legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.35)

polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)


ex_rate.point.f    = apply(Y.h.ext[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
ex_rate.range      = range(y[,1],ex_rate.interval.f)         


true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value

# Define the range and labels for the y-axis
y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
y_max <- max(ex_rate.range[2], true_y_value)
num_labels <- 4  # You can adjust the number of labels

# Generate a sequence of y-values for the axis that includes true_y_value
y_values <- pretty(c(y_min, y_max), num_labels)
if(!true_y_value %in% y_values) {
  y_values <- sort(c(y_values))
}

# Generate labels for the y-axis, ensuring true_y_value is included and highlighted
y_labels <- sprintf("%.2f", y_values)
y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")

# Plot adjustments
plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), xlab="", ylab="", lwd=2, col=mcxs1, axes=FALSE)
axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
axis(2, at=y_values, labels=y_labels, col="black",las=1)

# Red dashed line for the true y-value
abline(h=true_y_value, col="red", lty=2, lwd=1)

text(x=nrow(y), y=9.65, srt=90, "")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "")
abline(v=nrow(y)+h, col=mcxs2)
legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.35)

polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)

#### 68% highest density intervals

#par(mfrow = c(1, 2), mar = c(4, 4, 1, 1), oma = c(0, 0, 6, 0))

ex_rate.point.f    = apply(Y.h[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.68)
ex_rate.range      = range(y[,1],ex_rate.interval.f)

true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value

# Define the range and labels for the y-axis
y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
y_max <- max(ex_rate.range[2], true_y_value)
num_labels <- 4  # You can adjust the number of labels

# Generate a sequence of y-values for the axis that includes true_y_value
y_values <- pretty(c(y_min, y_max), num_labels)
if(!true_y_value %in% y_values) {
  y_values <- sort(c(y_values))
}

# Generate labels for the y-axis, ensuring true_y_value is included and highlighted
y_labels <- sprintf("%.2f", y_values)
y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")

# Plot adjustments
plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
axis(2, at=y_values, labels=y_labels, col="black",las=1)

# Red dashed line for the true y-value
abline(h=true_y_value, col="red", lty=2, lwd=1)


text(x=nrow(y), y=9.65, srt=90, "2025-12")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
abline(v=nrow(y)+h, col=mcxs2)
#legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.35)

polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)


ex_rate.point.f    = apply(Y.h.ext[,1,],1,mean) 
ex_rate.interval.f = apply(Y.h.ext[,1,],1,hdi,credMass=0.68)
ex_rate.range      = range(y[,1],ex_rate.interval.f)         


true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value

# Define the range and labels for the y-axis
y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
y_max <- max(ex_rate.range[2], true_y_value)
num_labels <- 4  # You can adjust the number of labels

# Generate a sequence of y-values for the axis that includes true_y_value
y_values <- pretty(c(y_min, y_max), num_labels)
if(!true_y_value %in% y_values) {
  y_values <- sort(c(y_values))
}

# Generate labels for the y-axis, ensuring true_y_value is included and highlighted
y_labels <- sprintf("%.2f", y_values)
y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")

# Plot adjustments
plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), xlab="", ylab="", lwd=2, col=mcxs1, axes=FALSE)
axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
axis(2, at=y_values, labels=y_labels, col="black",las=1)

# Red dashed line for the true y-value
abline(h=true_y_value, col="red", lty=2, lwd=1)

text(x=nrow(y), y=9.65, srt=90, "")
abline(v=nrow(y), col=mcxs4)
text(x=nrow(y)+h, y=9.65, srt=90, "")
abline(v=nrow(y)+h, col=mcxs2)
#legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.35)

polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
        c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
        col=mcxs1.shade1, border=mcxs1.shade1)
```

```{r}
# #### 15% highest density intervals
# 
# ex_rate.point.f    = apply(Y.h[,1,],1,mean) 
# ex_rate.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.15)
# ex_rate.range      = range(y[,1],ex_rate.interval.f)
# 
# true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value
# 
# # Define the range and labels for the y-axis
# y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
# y_max <- max(ex_rate.range[2], true_y_value)
# num_labels <- 4  # You can adjust the number of labels
# 
# # Generate a sequence of y-values for the axis that includes true_y_value
# y_values <- pretty(c(y_min, y_max), num_labels)
# if(!true_y_value %in% y_values) {
#   y_values <- sort(c(y_values))
# }
# 
# # Generate labels for the y-axis, ensuring true_y_value is included and highlighted
# y_labels <- sprintf("%.2f", y_values)
# y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")
# 
# # Plot adjustments
# plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), axes=FALSE, xlab="", ylab="", lwd=2, col=mcxs1)
# axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
# axis(2, at=y_values, labels=y_labels, col="black",las=1)
# 
# # Red dashed line for the true y-value
# abline(h=true_y_value, col="red", lty=2, lwd=1)
# 
# 
# text(x=nrow(y), y=9.65, srt=90, "2025-12")
# abline(v=nrow(y), col=mcxs4)
# text(x=nrow(y)+h, y=9.65, srt=90, "2026-12")
# abline(v=nrow(y)+h, col=mcxs2)
# #legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.35)
# 
# polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
#         c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
#         col=mcxs1.shade1, border=mcxs1.shade1)
# 
# 
# ex_rate.point.f    = apply(Y.h.ext[,1,],1,mean) 
# ex_rate.interval.f = apply(Y.h.ext[,1,],1,hdi,credMass=0.15)
# ex_rate.range      = range(y[,1],ex_rate.interval.f)         
# 
# 
# true_y_value <- ex_rate.point.f[12]  # Set this to your actual true y-value
# 
# # Define the range and labels for the y-axis
# y_min <- min(ex_rate.range[1], true_y_value)  # Adjust as necessary to include true_y_value
# y_max <- max(ex_rate.range[2], true_y_value)
# num_labels <- 4  # You can adjust the number of labels
# 
# # Generate a sequence of y-values for the axis that includes true_y_value
# y_values <- pretty(c(y_min, y_max), num_labels)
# if(!true_y_value %in% y_values) {
#   y_values <- sort(c(y_values))
# }
# 
# # Generate labels for the y-axis, ensuring true_y_value is included and highlighted
# y_labels <- sprintf("%.2f", y_values)
# y_labels[y_values == true_y_value] <- paste(y_labels[y_values == true_y_value], "(True Value)", sep=" ")
# 
# # Plot adjustments
# plot(1:(length(y[,1])+h), c(y[,1], ex_rate.point.f), type="l", ylim=c(min(y_values), max(y_values)), xlab="", ylab="", lwd=2, col=mcxs1, axes=FALSE)
# axis(1, c(1, 21, 41, 61, 81, 101, nrow(y), nrow(y)+h), c("1992", "1997", "2002", "2007", "2012", "2017", "", ""), col="black")
# axis(2, at=y_values, labels=y_labels, col="black",las=1)
# 
# # Red dashed line for the true y-value
# abline(h=true_y_value, col="red", lty=2, lwd=1)
# 
# text(x=nrow(y), y=9.65, srt=90, "")
# abline(v=nrow(y), col=mcxs4)
# text(x=nrow(y)+h, y=9.65, srt=90, "")
# abline(v=nrow(y)+h, col=mcxs2)
# #legend(136, 1.2, legend=c("Y2023Q4", "Y2026Q4"), col=c(mcxs4, mcxs2), lty=1, cex=0.35)
# 
# polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[13:1]),
#         c(y[136,1],ex_rate.interval.f[1,],ex_rate.interval.f[2,12:1],y[136,1]),
#         col=mcxs1.shade1, border=mcxs1.shade1)
```
The above presents the historical and forecasting data the AUD/USD exchange rate for basic and extended model. 

For the point forecast, for basic model, the exchange rate shows several peaks and troughs, indicating the volatility in the exchange rate over the years. For the three years forecasting, it shows a clear downward trend and reaching the lowest above 0.6 in 2026. And for extended model, it also shows a clear downward trend for the first half forecasting period and reaching below 0.06 in around 2025 and slightly increase back to 0.61 fo the following period. For conduct 90% and 68% confident interval in the extended model in which we can notice the extended model has an obvious wider confident interval for both significant level, it probably given by the nature of the Laplace distribution applied in the extended model in which the the fatter fail of Laplace distribution will capture more outliers which resulting a larger variance and then leading to a wider confident interval.

Regard to the 3D with density intervals below for both model forecasting, we could notice that the for each different predictive density at specific horizons, with the back wall we have the one period ahead predictive density and with the front we have the 12 period ahead density which is 3 years. We could see the distribution becomes lower and more dispersed with the increases of the horizon, as the data is more informative about the nearest developments in the future. Hence, one period predictive density is highly concentrated relative to others with smaller variance and taller peak. Similarly, the interval become more wider and dispersed resulting a more uncertainty for the future period forecasting.

```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(1,2), mar=c(2,2,1,1))
limits.1    = range(Y.h[,1,])
point.f     = apply(Y.h[,1,],1,mean)
interval.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)

x           = seq(from=limits.1[1], to=limits.1[2], length.out=100)
z           = matrix(NA,h,99)
for (i in 1:h){
  z[i,]     = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
}
x           = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
yy          = 1:h
z           = t(z)

theta = 180
phi   = 15.5
f4    = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1, col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
polygon3D(x=c(interval.f[1,],interval.f[2,h:1]), y=c(1:h,h:1), z=rep(0,2*h), col = mcxs1.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE)
for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=0.5, col="black")
}
f4.l1 = trans3d(x=point.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=2, col=mcxs1)


limits.1    = range(Y.h.ext[,1,])
point.f     = apply(Y.h.ext[,1,],1,mean)
interval.f  = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)

x           = seq(from=limits.1[1], to=limits.1[2], length.out=100)
z           = matrix(NA,h,99)
for (i in 1:h){
  z[i,]     = hist(Y.h.ext[i,1,], breaks=x, plot=FALSE)$density
}
x           = hist(Y.h.ext[i,1,], breaks=x, plot=FALSE)$mids
yy          = 1:h
z           = t(z)

theta = 180
phi   = 15.5
f4    = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1, col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, xlab="\nerate[t+h|t]", ylab="h", zlab="\npredictive densities of erate", ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
polygon3D(x=c(interval.f[1,],interval.f[2,h:1]), y=c(1:h,h:1), z=rep(0,2*h), col = mcxs1.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE)
for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=0.5, col="black")
}
f4.l1 = trans3d(x=point.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=2, col=mcxs2)
```
Interactive versions of the above 3D plots are provided below.
```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(1,2), mar=c(2,2,1,1))

# Forecasting on Extended model
limits.1        = range(Y.h[,1,])
point.f         = apply(Y.h[,1,],1,mean)
interval.f      = apply(Y.h[,1,],1,hdi,credMass=0.90)
theta = 180
phi   = 15.5

x.erate           = seq(from=limits.1[1], to=limits.1[2], length.out=10)
z.erate           = matrix(NA,h,9)
for (i in 1:h){
  z.erate[i,]     = hist(Y.h[i,1,], breaks=x.erate, plot=FALSE)$density
}
x.erate           = hist(Y.h[i,1,], breaks=x.erate, plot=FALSE)$mids
yy.erate          = 1:h

# plot using plot_ly
par(mfrow=c(1,1))
plot_ly(y = yy.erate, x = x.erate, z=z.erate) %>%
  
  layout(scene=list(xaxis=list(title="AUD/USD"),
                    yaxis=list(title="h-step forecast"),
                    zaxis=list(title="density")),
         title = "AUD/USD forecast densities for basic model") %>%
  add_surface(colors = c(mcxs1,mcxs4,mcxs5))


# Forecasting on Extended model
limits.1        = range(Y.h.ext[,1,])
point.f         = apply(Y.h.ext[,1,],1,mean)
interval.f      = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
theta = 180
phi   = 15.5

x.erate           = seq(from=limits.1[1], to=limits.1[2], length.out=10)
z.erate           = matrix(NA,h,9)
for (i in 1:h){
  z.erate[i,]     = hist(Y.h.ext[i,1,], breaks=x.erate, plot=FALSE)$density
}
x.erate           = hist(Y.h.ext[i,1,], breaks=x.erate, plot=FALSE)$mids
yy.erate          = 1:h

# plot using plot_ly
par(mfrow=c(1,1))
plot_ly(y = yy.erate, x = x.erate, z=z.erate) %>%
  
  layout(scene=list(xaxis=list(title="AUD/USD"),
                    yaxis=list(title="h-step forecast"),
                    zaxis=list(title="density")),
         title = "AUD/USD forecast densities for extended model") %>%
  add_surface(colors = c(mcxs1,"magenta2",mcxs5))

#[which indicates the extended model can be more informative in forecasting as it has a higher certainty from a more concentrated distribution.]
```
The 3D plots above allow us to access to alternative vantage points. We could notice that for the first few forecasting periods, the extended model has very sharp and concentrated densities around the point estimated, and it shows clearly heavy tails, which may increase the possibility of capturing extreme values that deviate significantly from the central forecast.